
## Monitoring, troubleshooting and disaster recovery

We talked about deploying to production in the previous chapter, but just _being_ in production is only half the job. 
The other half is ensuring that your systems are up, running and answering queries faster than your SLA thresholds. 

In this chapter, we'll cover how to tell that your RavenDB cluster is healthy, how to monitor its 
state over time and what to do if any issues pop up.

Even before getting the production deployment ready, you need to plan how you'll monitor what's going on in 
RavenDB. The first thing you'll see when you go to the RavenDB Studio in a browser is the dashboard view, giving
you the most-important details about what's going on in this specific RavenDB instance. This dashboard shows you crucial details, such as the number of requests, CPU load, memory utilization, indexing rate, etc. 

The dashboard view was designed so you can just throw it on a monitor somewhere and take a peek every so often to measure
the health of your system. If you have nothing else to guide you, just looking at the dashboard should still give you
some idea about what's happening on a particular node.

The problem with relying on _only_ the RavenDB dashboard is that it's limited to what's going on only right now, and only on one 
node. This view isn't so useful if you want to capture aggregated statistics across your cluster, for example, including historical
information and an analysis of the patterns over time. For that, you need a dedicated monitoring system. 

RavenDB doesn't attempt to build a full-blown monitoring solution in the dashboard, only to show you the most-pertinent information. The expectation is that you'll plug RavenDB into your existing monitoring solutions. Let's 
now explore what kind of hooks are provided for you to do just that.

### Ongoing monitoring

The most obvious monitoring option is to use SNMP.^[Simple Network Management Protocol]
To configure SNMP in RavenDB you'll need to set it up in the way shown in Listing 16.1.

```{caption="Setting up SNMP using the settings.json configuration file" .json}
{
    "Monitoring.Snmp.Enabled": true,
    "Monitoring.Snmp.Port": 161,
    "Monitoring.Snmp.Community": "password"
}
```

Once you have the configuration options from Listing 16.1 in the `settings.json` file, restart the node. You can 
now start querying RavenDB's internal state using SNMP. For example, the command in Listing 16.2 fetches the server update from the public RavenDB test instance. 

```{caption="Getting the live instance server update via SNMP" .bash}
$ snmpget -v 2c -c ravendb live-test.ravendb.net 1.3.6.1.4.1.45751.1.1.1.3

iso.3.6.1.4.1.45751.1.1.1.3 = Timeticks: (84790246) 9 days, 19:31:42.46
```

The key parts of these commands are:

* `-v 2c` - the protocol version (RavenDB uses RFCs 1901-1908)
* `-c ravendb` - the community string, which in this case is `-c password`, etc. (in the case of the public instance, the default `ravendb` is uesed)
* `live-test.ravendb.net` - the host name, which in this case is the live instance (plug your own 
  instance URL here, of course)
* `1.3.6.1.4.1.45751.1.1.1.3` - the OID to query, which in this case is the server uptime

Delete this sentence (redundant/confusing)?: The output is the server uptime, and we told RavenDB that we wanted this value using the OID. 
OID stands for Object Identifier, which is a way to globally name a particular value. The 
[`1.3.6.1.4.1.45751`](http://oidref.com/1.3.6.1.4.1.45751) prefix belongs to RavenDB, and anything nested under it denotes a particular value that can be queried. Don't worry, you don't have to memorize all these OIDs. Instead, you can simply
ask RavenDB for a list of any that are supported for a particular instance, by calling the `/monitoring/snmp/oids` endpoint on your server. For the live test server, that would be [http://live-test.ravendb.net/monitoring/snmp/oids](http://live-test.ravendb.net/monitoring/snmp/oids).

The live test server can be very helpful because it also gives you the OID for specific values in specific databases. For 
example, the `1.3.6.1.4.1.45751.1.1.5.2.2.2.1` OID can be used to tell us the size (in megabytes) of a particular database.
This can make it very easy to plot all sorts of interesting values over time. You can also use this process to monitor 
the index rate of a specific index in a specific database. The amount of details and control you have is extensive.

Of course, you won't usually querying SNMP OIDs using `snmpget`. You'll instead plug in directly a monitoring solution like Zabbix, Nagios, SCOM, OpenView, etc. The online documentation has a few walkthroughs for setting up common scenarios, but I'll assume you're familiar with your own monitoring systems and skip any further hand-holding at this point.

> **RavenDB isn't the only source of monitoring data.**
>
> RavenDB exposes a lot of information about its internal state to make it easier to figure out what's going on.
> Yet we try not to expose the _machine_ state via SNMP. RavenDB assumes that you can get that information directly
> from the machine using standard monitoring metrics (the `1.3.6.1.2.1.25.3.3` OID to get per-core CPU load, for example).
>
> When setting up monitoring, be sure to include metrics for anything that is useful; CPU load, memory, network and I/O 
> are among the most-common values that should be tracked. In most systems, these are part of the standard templates
> and require very little effort to set up. I mention this explicitly because tracking only the values that RavenDB
> provides will paint only part of the picture. 

Beyond using SNMP, RavenDB also exposes a lot of the state in REST endpoints, outputting JSON that you can use. 
These are meant both for humans to look at during troubleshooting and for automated systems to scrape and store over time. Most monitoring solutions have a way for you to provide a URL and a path in the response to gather interesting values to keep. 

You can get the full list of the debug endpoints from your server's `/debug/routes` endpoint. (For the live test server
this URL would be [http://live-test.ravendb.net/debug/routes](http://live-test.ravendb.net/debug/routes).) Just like any
other endpoint in RavenDB, when running in a secure mode, you'll need to provide authenticatation using a client certificate. 
Each monitoring solution has a different way of specifying this authentication, so check your user's manual for more details.

#### What should be monitored?

Now that we know how to _get_ the values to monitor, the obvious question is: What _should_ be monitored? I'm afraid that this is a hard question to answer, as it depends on your actual usage and needs. CPU load, memory, network and 
disk I/O are the most obvious things that you want to pay attention to. The numbers of requests and writes are also very interesting in most cases. 

> **Why not just monitor _everything_?**
>
> You can most certainly capture and store all the values that RavenDB and the machines expose. The problem there 
> is that most of this information isn't relevant for most scenarios. So, you might end up with a needle-in-a-haystack situation when you're looking for something specific. It would be easy overlook the pertinent information if you're drowning in so many other details.

At the very least, you definitely want to monitor values like the number of alerts for each of your databases, for example. But seeing the 
time since the last query for each index probably isn't worth it. 

The general recommendation is to start with the basic template (see the online documentation for more 
 on that) and add more details as you see fit. A lot of these details depend on whether you're 
seeing any signs of trouble and are hoping to head them off early on.

This is vague advice, I realize. But the problem is that good advice is hard to give when speaking in generalities. One
scenario may call for a B2B system with a fairly constant load, so that seeing CPU percentage go too low or too high would be a 
a strong indicator of an issue. But for another scenario, such as a user-facing system with hardly any activity lunchtime, a very clean spike in CPU usage at 9 AM every single workday would be normal. 

There is simply no substitution for knowing your environment and what's expected. Once you know that, you can set
set thresholds for alerting the operations team when the monitored values go out of their expected range. Some
monitoring tools can even analyze the data on their own and detect and report when there are irregularities for you. 

#### Performance hints and alerts

RavenDB itself is constantly monitoring its own health. This includes measuring I/O latencies, conenctivity
to other nodes in the cluster, the amount of memory and disk available, etc. For some of these values, RavenDB
will take action on its own. For example, if the memory is low, RavenDB will switch gears and activtely try to reduce 
its memory usage to avoid running out of space completely. 

Most of the time, however, there isn't much that RavenDB _can_ do. For example, if the backup folder is out of free space or if RavenDB is experiencing slow I/O, all RavenDB can do is alert you to these troublesome details. Such issues appear as performance hints and alerts, as shown in red in Figure 16.1.

![Alerts and performance hints give your operations team ideas about what to look into.](./Ch16/img01.png)

Alerts are issues that RavenDB encounters that require some sort of manual intervention. These may be something simple,
such as running out of disk space or network issues with talking to a particular database. Sometimes the errors are 
transient, which is common in network scenarios. Alerts are also generated for your perusal even if the actual 
error was already fixed (a connection resumed between the nodes, for example) because investigating what happened
after the fact can still be useful.

> **Alerts are node-local.**
>
> An alert isn't a cluster-wide notification. Indeed, in many cases, you'll be alerted that a node is unable to 
> talk to the rest of the cluster. As such, you need to monitoring each node in the cluster and check its state
> whenever an alert appears. 

Performance hints, on the other hand, are different (see the blue in Figure 16.1). While they don't denote errors, 
they do very likely warrant your attention. RavenDB generate such hints for various reasons, such as the slow I/O example
in Figure 16.1. Other reasons include queries that return very large numbers of results without using streaming, 
slow requests, high fanout during indexing and a few other common issues.

These issues aren't typically too severe, but they're still better to address early on. Hints are generated 
whenever RavenDB detects a use pattern that is known to be problematic. For example, if your queries have a
very large page size or your documents are unusually large, then RavenDB identifies a potential for issues 
and is proactive in bringing these to your attention. 

In general, alerts are designed as an acknowledgement that no one reads the logs until it's too late. So, instead
of hiding errors in a text file that no one reads, RavenDB ensures that your operations teams knows about any big
stuff as they need to.

#### The debug log

Alerts are high-level items that needs operator attention. By design, very few things rise to the level of alerts, to
avoid spamming your operations team with too many cries for attention. That's where the debug log comes in.

RavenDB has just two log levels: `Operations` and `Information`. By default, RavenDB will log only messages 
with the `Operations` label. Operational log messages are things that the operations team needs to pay attention to
or be aware of, such as updating certificates, a failure to process commands, background tasks errors, etc. 

At the `Operations` level, the log output is meant to be readable and fairly shallow. You aren't informed about anything that
goes on inside RavenDB, only things that matter to the operations team. These are mostly errors and issues, even if they were transient or already worked around by RavenDB.

> **The audit log**
>
> In addition to the `Operations` level, RavenDB also supports an explict audit mode, where RavenDB records
> all database connections, their sources, their certificates used and their level of access granted.
>
> Certain special actions, such as the creation of a database or index, are also written to the aduit log. To enable the
> audit log, you can set a path to `Security.AuditLog.FolderPath`. For more details, see Chapter 13. 

The `Information` level, on the other hand, is much more detailed and will include anything that
goes in the database. That can be helpful when tracking down a specific issue, but generally that results in a _lot_
of data that goes to the log. On a busy server, you can easily see a log file rollover (which happens every 256MB)
in 5 - 10 minutes.

By default, RavenDB retain log files for 72 hours. After that, it will delete the old log files. In many cases, 
you'll have a logs agent monitor the directory and send the logs to a central location, which means that you can 
delete the logs file as soon as RavenDB starts a new one.

The logs location is controlled by `Logs.Path` configuration option and it generally recommended to have RavenDB
write the logs to a separate hard disk. This is primarily to avoid the case of setting the `Logs.Mode` to the 
`Information` level and have the logs fill up the RavenDB data disk completely. 

Changing the logs path or the logs mode requires a server restart. But that isn't the only way you can use to get
information from RavenDB.

### Finding out your server is doing

If you run into issues with RavenDB, it might be too late to setup logging and monitoring. Recognizing this reality
RavenDB has a lot of knobs to tweak and places to peek into the internals of the engine. This does _not_ mean that you
can skip setting up proper monitoring regime. The techniques outlined in the section are very useful when trying to 
understand the behavior of RavenDB.

In many cases, they can help you pinpoint what is going on very quickly. Monitoring, on the other hand, gives you 
insight into the behavior of the system over time and alert you if something is out of whack. It is expected that
any production cluster will have a proper monitoring strategy.

The RavenDB Studio provide explicit support to figure out what is currently going on. The first place to look at is
inside `Manage Server` and the `Debug` section, shown in Figure 16.2.

![The `Debug` section is your first stop to figure out what is going on the inside](./Ch16/img02.png)

These options are typically only available to the operations team (requiring `Operator` or `Cluster Admin` privileges).
This is because they contain data for the entire server, not for a particular database. A standard user will not be
able to use these features. 

Let's explore each one of these options in turn.

#### The admin logs view

The easiest way to figure out what is going on inside RavenDB is to use the `Admin Logs` feature. This allow you to 
hook into the logs stream that is generated by RavenDB and view that on the browser. It is a very convient option 
because it doesn't require you to do anything to the server.

Even if the server is configured to not log anything, going to the `Admin Logs` page will start streaming the log
entries to you at `Information` level. This means that you can enable the logs and watch them only during interesting
periods, instead of having them on all the time (or having to restart the server to change the logs mode).

> **`rvn logstream`**
>
> From the command line, if you are running on the same machine as RavenDB, you can use the `rvn logstream` command
> to have the same effect, but only for the pre-configured logging. The `rvn admin-channel` command have an option
> to enable logging on a live server on a temporary basis.

Errors will be highlighted in red, but do note that there are many errors that are actually expected and handled by
RavenDB, so seeing some errors isn't something that you should be frightened of. 

#### The traffic watch view

Looking at the logs can be fairly tedious, given how much information is being generated. In some cases, it is easier
to figure out what is going on by just looking at the requests that are being made. RavenDB contains a dedicated
view for that, as you can see in Figure 16.3. 

![Watching live requests on a production database can be helpful in many cases](./Ch16/img03.png)

In Figure 16.3 you can see each individual request, how much time it took as well as some statistics about all the 
requests that were captured during a specific session (number, min, avg, max duration). That can be very helpful if 
you want to figure out why something is slow or result in an error. You can also export the traffic capture for 
later analysis. 

#### The debug info package

By far the most valuable tool to inspect the state of RavenDB is the `Gather Debug Info` page. You can see how this
looks like in Figure 16.4. 

![The `Debug Info` pacakge gives us access to _all_ the RavenDB debug endpoints](./Ch16/img04.png)

What this page does is to give us the ability to generate, at the click of a single button, a snapshot of the current
state of the RavenDB instance or even the whole cluster. The result is a single `zip` file that you can send along 
with your support ticket or do offline analysis at a later date.

I mentioned a few times before that RavenDB goes to great lengths to externalize its state. Even with this chapter, 
dedicated to talking about how you can figure out what RavenDB is doing, I'll not have enough time and space to go
over all the details that are available. Some of them are extremely useful, for a single specific case. Most of the 
time, you don't really need to go over each and every one of them.

The ability to capture all the state at once, easily and with no fuss, means that if you are in a bad state, you can
get the current status of the system and then take potentially invasive operations to fix this. The classic example
is rebooting the system, which will "fix" many issues. The problem is that if you do so, you'll also typically lose
crucial information about what went wrong and how to prevent it in the future. 

The `Debug Info` package allows you to retain all of that knowledge. It also is very useful if you need to contact
the RavenDB support team, since it saves a _lot_ of back and forth. 

#### Advanced details

The last item in Figure 16.2 is the `Advanced` options, where RavenDB puts details that can be very interesting, but
aren't usually needed. Among them, we have two very interesting views.
You can see these in Figure 16.5.

![Advanced information about what is going on inside RavenDB](./Ch16/img05.png)

Figure 16.5 shows the threads details inside RavenDB. This can tell you what exactly is costly in terms of CPU time. 
The list is sorted by usage order, so threads that burns through a lot of CPU will show up first. In this case, you 
can see that the most expensive thread is the `Voron Global Flushing Thread` (we'll discuss I/O, which is what this
thread is doing, later in this chapter) and then an indexing thread. 

RavenDB typically names threads to make it easier for you to figure out what is it that they are doing. This allows
the operations team to figure out what is costing you and take actions accordingly. 

Another interesting peek into the internals of RavenDB is offered by the `Cluster Observer Logs` view. 
In Chapter 7 we discussed how RavenDB Cluster will assign work and even move databases between nodes automatically.
Figure 16.6 shows the logs of such decisions, allowing you to examine and understand them.

![The cluster observer logs explains how the cluster makes decisions](./Ch16/img06.png)

The logs in Figure 16.6 shows that Node B was down, and at what point the cluster detected that this node's databases
caught up with the rest of the nodes in the database group, allowing the cluster to move node B from a rehab state
to a normal member state.

> **Operational stability**
>
> One of the worst things a piece of software can do to an operations team is to surprise and mystify them. 
> Surprises are almost never a good idea in production. And a good mystery is wonderful when it comes in book
> form, but not if it means figuring out what blew out production. 
> 
> The cluster observer routinely  monitor the health of the entire cluster, making
> decisions about where traffic should go and what nodes are reliable. 
> By default, it is only going to take action if a node is down for a long time (15 minutes or more). 
>
> This means that it isn't going to start taking action before you can react. You can also ask the cluster 
> observer to suspend any actions for a while, if you know that you are going to take some maintenance operation
> that will make the observer take action usually. This is done using the `Suspend cluster observer` button shown
> on Figure 16.6. 

This can be very useful if you want to understand how the system got into its current state, and what has led the
cluster observer to make the decisions it did. This is also logged, of course, but it is usually much easier to 
see the information condensed in such a manner than search through the log files. 

### I/O operations in RavenDB

As a database, I/O _matters_ to RavenDB. In particular, I/O performance. A lot of problems in production can be tied
directly into issues with I/O. And I/O issues typically come with one (or both) of the following:

* insufficent depth - the I/O system cannot handle the amount of data read/write fast enough. 
* insufficent width - the I/O system cannot handle the number of concurrent read/write requests fast enough.

Both these issues are going to end up looking like slow I/O, but their root cause are different. In the first case
we have an I/O system that gets requests that are too big for it. A good example of that is trying to make a large
write (greater than the buffer size, typically) and seeing very high latency for the write.

> **Imagine I/O bandwidth as a shipping channel**
>
> I use the terms depth and width here because of the following metaphor. Imagine a shipping channel that has ships
> going through it. If you have a ship that is heavily loaded, it is sitting deep in the water and if the channel
> doesn't have enough depth, it will hit the bottom. In our cases, the I/O request will usually still work, but
> _very_ slowly. 
>
> As for the width of the channel, imagine a shipping channel that have a lot of canoes 
> going through it. The bigger the width of the channel, the more concurrent canoes can go through. But if the 
> channel has insufficent width, you are going to see a traffic jam. The same with I/O. If you have a lot of 
> requests (even if they are individuall very small) the I/O system might struggle to serve them all.

If you are running on an HDD and making a lot of random reads, which requires seeking (which is slow). For a single
continuous write, the HDD is wonderful (high depth). But for concurrent work, not so much (insufficent width).

If you are wondering how to spend your hardware budget, get better I/O, it is almost always going to be a good
idea. With the way RavenDB handles memory, if you have a fast enough disk (NVMe comes to mind) you can even trade
off the amount of memory the machine has. Getting the memory mapped data from NVMe disk is usually fast enough that
it doesn't need to reside in main memory for many scenarios.

> **What to look out for?**
>
> Watch the disk queue length. If the queue length grows, it is usually a sign that there are requests waiting
> for the I/O system. If the disk queue length is high for long period of times, that means that there is I/O
> starvation and your database is likely suffering.
>
> High disk queue length means that some form of action is required (upgrading hardware, increasing IOPS, changing
> the load pattern, etc). 

RavenDB will alert you if it detects slow I/O for writes, as you already saw in Figure 16.1. But for more detailed
I/O monitoring you will need to look in more places. RavenDB makes this easy and gathers all its I/O statsitics for 
writes in one place. In the RavenDB Studio, go to your database and then go to `Stats` and then to `IO Stats`.
You can see how this looks like in Figure 16.7.

![RavenDB's internal stats for I/O write operations for a production database](./Ch16/img07.png)

There are a lot of details in Figure 16.7, and it probably won't make sense in isolation. In the studio, you can zoom
in and out and insepct every detail in the performance graph. That can give you a lot of insight into any particular
operation that was made by RavenDB.

The green bar on the bottom is the most important datum. There you can see the cost over time of writes to the journal
file. A transaction cannot complete before it has durably written its changes to the journal file, so that is the 
number one hot path for any issues with slow writes.

The size of each write indiciate how long it took and the color tells you how big it was. The darker the value, the 
bigger the write. In general, you want to see a healthy situation, with mostly small and thin writes. Having a few
wider and darker spots is also fine (it may take longer to write more, obviously). However, if you see wider areas
that are birght (small writes that took a long time) that is usually an indication that the I/O system is saturated.

In such cases, you want to turn to the operating system own tools to figure out what is going on. Some ideas about
what you should be looking for and how to get it are showing in Table 16.1.

|          |        Windows         |    Linux    |
|----------|------------------------|-------------|
| Tool     | `perfmon`              | `iostat`    |
| Counters | Disk Reads/sec         | `r/s`       |
|          | Disk Writes/sec        | `w/s`       |
|          | Avg. Disk Queue Length | `avgqusz`   |
|          | Avg. Disk sec/Transfer | `await`     |

Table: I/O monitoring tools and what metrics to look at.

Table 16.1 is a good start to investigate why your systme is showing slow I/O sympthoms. You can also use Resource
Monitor in Windows and `iotop` / `sysdig` on Linux to figure out what files exactly are taking all the I/O.

> **Do you have enough IOPS?**
> 
> When running on the cloud, a surprisingly common mistake is to forget to reserve enough IOPS for your data
> disks. In many cloud providers, you have some sort of burst capability, after which you'll be throttled. In 
> that case, you will see initial good performance (until the burst window is done) and then your I/O performance
> will drop sharply. This is the very first thing to check for I/O issues when running on a cloud.
>
> When running on your own hardware, talk to the storage team and ensure that the database server has good enough
> QoS rating to be sufficent for the workload you have on it. I have seen cases where RavenDB was put on a powerful
> server, backed by a SAN full of speedy drives, and the Quality of Service setting restricted the database to 
> about 5% of the available resources. 

In this section, we looked at how to gether information about what is going on. Whatever it is via RavenDB and
the `I/O Stats` graphs or through the operating system tooling. What we are still missing is how to make this
actionable. In order to know what to do when you have I/O issues, we need to understand how RavenDB is using I/O.

#### RavenDB I/O behavior

The graph in Figure 16.7 shows how RavenDB is keeping track of its writes. In general, RavenDB writes mostly to 
documents and to the indexes. Each write is done first to the journal (the green bar in Figure 16.7) and then flushed
to the data file (blue bar). Every now and then RavenDB will sync the data to disk (using `fsync`) to allow us to
reuse the journal files.

> **Flushing data and syncing to disk**
>
> RavenDB use the term `data flush` to denote the process of copying modified transaction data to the memory mapped
> data file. This operation is typically very fast, since it works by copying from memory to memory, involving no
> disk I/O. Every now and then RavenDB will ensure that the data written to the data file actually resides on disk
> by calling `fsync`. This is a `data sync`, and it is typically much more expensive operation. 

It's important to understand that within a given storage environment (the documents store for a database, or each
individual index) there can only be a single outstanding write, data flush or file sync. However, writing to the disk, 
flushing the data and syncing to disk can all run cocnurrently.

Each index is also a full blown storage environment, so indexes will have their own cycle of journal writes, data 
flushes and fsync calls. Each of those run completely independently from other indexes and the documents store. When
you have multiple databases inside the same server, each database and each one of the indexes will also operate 
independently and concurrently of each other. 

> **What about reads?**
>
> I went over a lot of details about how RavenDB writes data, but what about reads? How is that handled? 
>
> All of the read I/O operations in RavenDB are done using memory mapped I/O. In other words, we let the operating
> system buffer cache to manage what is resident in memory and when to read from the disk. In most cases, the
> operating system will know how to select an effective working set that will minimize the amount of I/O required
> for RavenDB.
>
> If that isn't the case, for example if you don't have enough physical memory for the workload you have, you'll 
> start seeing high number of page faults. In particular, pay attention to hard page faults. A small number of them
> is expected, but it you are seeing a jump is the number of hard page faults/sec counter, that indicate a possible
> issue.
>
> If you are using fast disks, you might not care about this. NVMe reads are fast enough that in some cases they 
> can replace main memory, for example. Or you might be reading cold data because you have created a new index that
> need to go over the existing documents.
> But in most cases, seeing high hard page faults/sec is indicative of an issue and require investigation. 

By default, RavenDB limits the number of concurrent syncs and flushes that are perform per physical device, to avoid
overwhelming the disk with too many concurrent writes. This is controlled by the `Storage.MaxConcurrentFlushes` and
`Storage.NumberOfConcurrentSyncsPerPhysicalDrive` options. 

For best performance, having each database use its own device is recommended, in this way, they don't have to fight
for the same hardware resources. For that matter, you can also dedicate specific drives for indexes or journals by
using symbolic links and junction points, as discussed in the previous chapter.

#### How the data is laid out on the disk

After looking at how RavenDB is using I/O, it is only natural that we will look at how RavenDB is actually storing
the data on disk. Table 16.2 lists the files that are stored and what use RavenDB make of each of them.

| Path        |                                                      Purpose                                                       |
|-------------|--------------------------------------------------------------------------------------------------------------------|
| Raven.voron | The main data file for a database                                                                                  |
| headers.*	  | Snapshot of the global state of the database                                                                       |
| Scratch/    | Directory to hold temporary data while the database is opened. Such as compression buffers, uncommitted data, etc. |
| Journal/    | Holds the write ahead log (transaction journals) that are key to ensuring ACID behavior.                           |
| Indexes/    | Recursively hold the same structure (Raven.voron, headers.one, Scratch, Journal) for each of the indexes.          |

Table: File structure of a RavenDB database on disk

You can go to the RavenDB database directory and look at this files, but they won't really mean much. They hold binary 
data and are opaque to the user. 

> **Remember to exclude the RavenDB directories from indexing / anti virus scans**
>
> If your machines have any kind of anti virus or file indexing services, you should exclude the RavenDB directories
> from these services. The reason is that these services often add a very high degree of latency to the I/O. 
>
> It is also fairly common for these services to lock files, cause I/O failures and in general mess around in what
> RavenDB considers its own back yard. Please keep anything (from a noisy AV program to a curious user) from dealing
> with the RavenDB directory directly.

You cannot learn anything about the structures of the files from the file system, but you _can_ learn quite a lot from
asking RavenDB directly what is going on. In the RavenDB Studio, go to your database and then to `Stats` and 
`Storage Report`. Figure 16.8 shows how this storage breakdown looks like for a blog database.

![Internal storage detail for a blog database](./Ch16/img08.png)

You can see the tree map in Figure 16.8, which should help you figure out at a glance what is taking space in your
database. Below the tree map, you'll see a table similar to Table 16.3.

|   Type    |           Name            | Size (∑ 800.62 MB) | % Total |
|-----------|---------------------------|--------------------|---------|
| Documents | blog.ayende.com           | 320 MB             | 39.97%  |
| Index     | PostComments/CreationDate | 96 MB              | 11.99%  |
| Index     | Posts/ByTag               | 48 MB              | 6.00%   |
| Index     | Auto/Commenters/ByKey     | 34 MB              | 4.25%   |
| Index     | Posts/Statistics          | 24 MB              | 3.00%   |
| Index     | Tags/Count                | 20 MB              | 2.50%   |


Table: Detailed breakdown of size of disk for each item inside a RavenDB database.

You might notice that the numbers are nice and round, that is quite intentional. An index in RavenDB
reserves a minimum of 16 MB on the disk. RavenDB also uses a pre-allocation pattern when requesting disk space from
the operating system. What does that means?

Asking the operating system for more disk space in small increments, will result in high file fragmentation. Instead,
RavenDB asks for file size increase up front, it starts with doubling the size of the data file on disk every time more
space is needed, until it gets to 1GB. From that point on it will grow by 1 GB at a time. 
This allocation pattern gives the operating system the best chance to allocate continious disk space for the file and
reduce metadata I/O overhead in most cases. 

You can see the internal breakdown of space usage inside a particular item by clicking on it in the Studio. At that
point you'll see the breakdown between journals and data for that storage environment. Journals are used whenever a 
transaction is committed. RavenDB writes all the changes that happened in the transaction to the journal file, ensuring
that the data is stored in a persistent manner. It also means that the hot path of I/O is sequential writes, which is
the fastest option.

Journal files are also pre-allocated (starting from 64KB and doubling in size until they get to 
256MB^[All these sizes, as well as the max size of database growth are configurable, of course.]). Whenever RavenDB is
done using a particular journal file, it will not delete it, but reuse it as needed. This saves the amount of on disk
space allocated by the journals. RavenDB does rename the files, to maintain consistent numbering. But the journal
reuse ensures that the file system allocation is fixed and only the file name is changed.

Going back to the tree map, you can click on the `Datafile` entry and see the breakdown inside the actual database. In
Table 16.4 you can see how this looks like for a blog database.

|   Type   |         Name          |    Size (∑ 256 MB)    | % Total |
|----------|-----------------------|-----------------------|---------|
| Tables   | Tables                | 156.22 MB             | 61.03%  |
| Free     | Free                  | 93.70 MB              | 36.60%  |
| Trees    | Trees                 | 2.28 MB               | 0.89%   |
| Reserved | Pre Allocated Buffers | 8 KB (out of 6.00 MB) | 0.00%   |

Table: Space breakdown of a 256MB database hosting a blog.

Table 16.4 has a few very interesting details. The data is divided into Tables, Trees, Reserved and Free. Let's talk
about what each of these mean.

A table in this context is not a relational table, but rather an internal structure inside the low level Voron 
storage engine. This is how RavenDB stores data internally. You can click on the tables to further see how much space
is used by each, as shown in Table 16.5.

| Type  |                Name                | Entries | Size (∑ 156.22 MB) | % Total |
|-------|------------------------------------|---------|--------------------|---------|
| Table | Collection.Documents.postcomments  |   6,604 | 50.65 MB           | 32.42%  |
| Table | Collection.Documents.posts         |   6,589 | 40.21 MB           | 25.74%  |
| Table | Collection.Documents.commenters    |  10,714 | 8.18 MB            | 5.24%   |
| Table | Collection.Revisions.posts         |     363 | 4.18 MB            | 2.68%   |
| Table | Collection.Revisions.postcomments  |     369 | 3.5 MB             | 2.24%   |
| Table | AttachmentsMetadata                |       0 | 2.25 MB            | 1.45%   |
| Table | Collections                        |       9 | 2.25 MB            | 1.45%   |
| Table | Collection.Tombstones.postcomments |       0 | 2.13 MB            | 1.37%   |
| Table | Collection.Tombstones.posts        |       0 | 2.13 MB            | 1.37%   |
| Table | Attachments.Tombstones             |       0 | 2.13 MB            | 1.37%   |


Table: Space breakdown of the storage table tables inside a blog database.

As you can see in Table 16.5, a stroage table is part of how RavenDB implements collections (you can see the `posts` collection
is using `Collection.Documents.posts`, `Collection.Tombstones.posts`, `Collection.Revisions.posts`). There are also other
tables, such as `Collections` and `AttachmentMetadata` that are used inside RavenDB to implement its functionality. In 
general, you shouldn't really care about how this is structured. Usually you'll look at this view to figure out why you are 
using this much disk space. 

The `Trees` section is Table 16.4 is the amount of space that is used by internal storage indexes. You can click on it to
see more details, but this is rarely interesting to operation people. The indexes tend to be a small fraction of the data
size and rarely matter. 

The `Reserved` section is reserved by RavenDB to ensure high locality of reference internally. This is an implementation
detail that is used for optimizing access patterns. This will never be very big, and you can safely ignore it. 

This leaves us with the free space. And that takes a whopping 36.6% of the data.  In this case, this is RavenDB pre-allocating 
data from the file system, but it can also be free space that is reclaimed by RavenDB when data is deleted. In general, 
you don't need to concern yourself with the actual free space management inside RavenDB.

This is handled completely internally and deleted data will free space back to RavenDB and reused as needed. This means that
deleting a lot of documents and then inserting them again will result in disk space reuse. However, RavenDB will not release
free space back to the operating system. In other words, if you had a large database and delete all the data inside it, you'll
note get any disk space back. 

RavenDB will reuse this space as needed, but if you want RavenDB to give it up you need to take an action, compact the database.

#### Database compaction

You can ask RavenDB to compact the database to the smallest possible size. This is done using `Compact database` option in 
the database page, as shown in Figure 16.9. 

![You can start a database compaction from the ](./Ch16/img09.png)

Compaction is an offline process, it will take the database down (only on the node where you actually run the compaction)
and recreate the database files from scratch. This has the effect of reading and writing the entire database and can cause
signficiant amount of I/O. This also requires disk space sufficent to create the compacted database representation in 
addition to the current database.

Compaction reserves no extra free space from the operating systme and can arrange the data on disk in a more optimal 
fashion. But unless you have the need for the disk space that this will free, it is probably best to leave it 
well enough alone. 
RavenDB is designed to required as little operational overhead as possible and managing the data on disk is one of its 
primary tasks, after all. Any free space inside the data file is already going to be reused by RavenDB, so manual 
compaction is typically not worth the time to do so. 

### Troubleshooting connection issues

After spending so long talking about disk I/O, let's turn our eyes to other I/O sources. In particular, the network is 
well known for being a troublesome and finicky beast. 
Actually delving into all the things that can go wrong in modern network envrionment will take multiple books, so I'm
going to assume that you are familiar with the usual concepts.

The common network issues with RavenDB includes:

* Firewall - both at the network and machine level. Remember that RavenDB uses two ports for communication, one for
  HTTP(S) and one for TCP communication. If you didn't explicitly configured using `ServerUrl.Tcp` RavenDB will use
  a dynamic port. This works inside a data center, but in the cloud, for example, you'll need to set a fixed port 
  and define the firewall rules to allow it. 
* Latency - if you are deploying in a geo distributed manner or have high latency between the nodes, you'll need to
  account for that in the RavenDB configuration. The `Cluster.ElectionTimeoutInMs` setting control this behavior and
  is set to 300ms by default. This is meant for a cluster deployed in the same data center, you'll need to set this
  to a higher value on other configurations. Replication between database instances on different nodes aren't that
  sensitive to delays and was designed for deployment in high latency situations, so you typically don't need to
  change anything there.
* Intrusive middleboxes - in many environments, a network connection goes through multiple network applicances, often 
  called middleboxes. These can be firewalls, intrusion detection systems, WAN optimizers, load balancers, etc. The
  problem with them is that the often add behavior (and even modify the payload) and can result in unexpected results.
  RavenDB is almost always deployed in a secured configuration in production, so the communication to and from the
  server is encrypted and cannot be modified. This reduce the nubmer of times that a network appliance can apply
  some tranformation to the TCP connection and break things.
* SSL Termination - a special case of a middlebox is tha SSL teminating appliance. A client connects using SSL to the
   endpoint, where the SSL connection is terminated and a plain HTTP connection is made from the applicance to RavenDB.
   This can work only if RavenDB is run without security. Authentication requires a client certificate to reach the 
   RavenDB server, which obviously cannot happen if the SSL connection was terminated at the appliance level.
* TLS downgrade - some appliances can only accept certain versions of SSL/TLS conenctions. RavenDB _requires_ that
   you'll use TLS 1.2 to connect to it and will error if a connection is attempted using TLS 1.0 or 1.1. 

Troubleshotting the network usually means using low level tools like packet inspectors such as `WireShark`. I'm not 
going to go over any detail about those, but I want to point out that in many cases, especially for figuring out 
issues in TLS/SSL connections, using `openssl s_client`^[Full documentation for this feature can be found in the 
[OpenSSL Cookbook](https://www.feistyduck.com/library/openssl-cookbook/online/ch-testing-with-openssl.html).] 
can be much easier.

The full command is: `$ openssl s_client -connect a.oren.ravendb.run:443 -cert client.pem -key client.pem`

The output of this command can tell you quite a lot about the connection, in particular, whether the client has
been able to successfully create a secured connection with the server, mutually authenticate and what are the 
relevant certificates. It is a good way to pinpoint or rule out common issues at the beginning of your 
troubleshooting session. 

#### Internal, external and multiple IPs

When you run on your own hardware, the situation is pretty simple. Your server has _an_ IP address that it will 
listen to and that clients will use to talk to it. When you run in the cloud, that is usually not the case. Your
server has an internal IP that it listens to (for example `172.31.25.240`) and a public IP (such as 
`52.9.72.38`)^[These are real internal / external IPs from one of our AWS machines.].

The server cannot actually bind to `52.9.72.38`, because it isn't directly exposed to the network. Instead, any 
connection to `52.9.72.38` will be routed to the server by the cloud's network appliances. This lead to an interesting
problem. The server needs to bind to an IP address so it can listen to incoming packets. This is the `ServerUrl` 
configuration setting. You can usually set it to `0.0.0.0:443` and have RavenDB bind to all the available network
interfaces.

However, the server often needs to tell clients and other servers how to talk to it, so we also need to provide the
server with it's publicly visible details. This is the `PublicServerUrl` setting (as well as `PublicServerUrl.Tcp`).
It's important to remember to set this properly, otherwise the server will tell clients to use its internal IP, which
isn't reachable from the outside world. You'll usually set this up as part of the initial setup, but if you are doing
things manually, it is easier to forget this step.

RavenDB uses HTTPS for securing the communication, and that means that we also validate the hostname used matches the
hostname on the certificate. That can cause issues if you attempt to connect to the server using a different hostname.
For example, if your certificate was issued for `a.oren.ravendb.run` any connection made with `https://rvn-srv-01` will
fail due to the hostname mismatch. 

On the client side, you can ignore this (from the browser or from the client API), although that is not recommended. But
on the server side, RavenDB will always validate the hostname, so the hostname must match the certificate's hostname 
for server to server communication.

### The admin JavaScript console

We have gone over quite a few of the way RavenDB externalize state and allow you to poke into what is going on inside
the server. In addition to all of the above, RavenDB contains an additional option that is very powerful. And with great
power come great responsability.

In the Studio, go to `Manage Server` and then to the `Admin JS Console`. You can see how this looks like in Figure 16.10.

![The Admin JS Console allows you to run scripts against the current server instance.](./Ch16/img10.png)

The Admin JS Console opens up a way to run scripts inside the server itself. This gives you full access to anything that
is going on inside the server. Figure 16.10 shows how we can use that to get some data from the server but it isn't 
limited to just reading data. You can also mutate the state of the server. This includes calling methods and interfering
in the internal state.

The JS Console comes with no safety net, nor a harness. You are absolutely able to do things there that would crash the
server or merely stop it from accepting additional requests. This is provided to those who know the internals of RavenDB
inside and outside. There is no explicit documentation on the API available nor any promise of compatability of the 
scripts between releases.

This tool is mostly meant to be used during support incidents, where you'll get the scripts to run from a RavenDB support
engineer. In such cases, it can be invaluable to be able to make changes to the server on the fly, but other than that
we don't recommend touching this in production. 

The JS console is always available via the `rvn admin-channel` tool, which can connect to a running instance of RavenDB
and perform operations directly from inside the server, bypassing the network layer. The `rvn` tool was mentioned briefly
before, but I think it is time to talk about it. The `rvn` tool is part of a larger disaster recovery strategy for RavenDB.

### Disaster recovery strategies

RavenDB is used in mission critical systems and holds valuable information. Part of the very core of the design was to
consider failure modes and reactions to them. There have been quite a few of them, from a hardware failure to losing the
admin certificate to your RavenDB instances. 

> **Minimum survivability: Run a cluster and minimize single point of failure**
> 
> At a minimum, if you care about being up at all, you should run RavenDB in a multi node cluster. Running RavenDB on 
> a single node is something that should only be done during development / testing. All of RavenDB's high availability
> features rely on the fact that you have multiple nodes.
> 
> By the same token, try to avoid having a single point of failure, such as a single SAN that all the nodes use to store
> their data. If the SAN is down, the entire cluster is down. 

It goes without saying that you should have a proper backup strategy (see the next chapter, dedicated for this topic).
But just having backups isn't enough. If you have a large database, just the time to copy the backed up data to the 
machine can take hours. You need multiple layers of defense to ensure that you'll always be online.

The first layer is running in a multi node cluster, and ensuring that each of your databases in the cluster has a high
enough replication factor to survive the loss of a node or two. The exact value for replication factor depends on the
size of your cluster and the importance of your data. The more important the data and the continued operation of the
system, the most resources you can utilize for that. 

The next layer of defense is having a offsite clone of the data. This is usually defined using external replication and
is meant to serve as a live server that holds all your data that you can manually fail over to if you have managed to
lose _all_ the nodes in the cluster.

> **Paranoia? You mean common sense**
>
> For high value production systems, parania is just the way things are. You don't necessarily have to follow all the 
> suggestions in this section. They are meant for systems that must always be up and survive even the most unlikely
> of things. 
> 
> There is a cost associated with such duplicated measures (offsite hot node, backups, additional nodes in the cluster,
> etc). Part of the operations team job is to take the risk vs. cost scenario and wieght on how much risk is acceptable
> and what is the cost to prevent it. At some point, the risk become negligible and the cost prohibitive. 
> 
> To my knowledge, no one setup a server farm on the moon to handle the case of a thermonuclear nuclear war, for example.
> You need to make your own determination in the matter.

You can also setup a delay on external replication, which can allow you to react if you run a query that modified data 
it shouldn't across the cluster. 

#### Admin access without the admin certificate

Imagine trying to get into your house, only to figure out that you lost the keys. The same can happen to RavenDB if you
lost the admin certificate. Consider a key employee being on vacation, with the admin client certificate locked on their 
machine. You don't have access to the certificate, but need to perform some admin operation. What do you do then?

RavenDB follows a simple rule. If you are have admin / root privileges on the box, you can access RavenDB. As a root user
you have a few options available to you to access RavenDB. You can use your root privileges to get the server certificate
that RavenDB itself is using to run. That server certificate can also be used for client authentication and RavenDB will
always give the certificate that it is using cluster admin privileges. 

In other words, if you can access the machine RavenDB is running on as root or administrator, you can get the certificate
that RavenDB is using and then access RavenDB directly as admin. At this point you could generate a new admin client 
certificate or perform any other action you need to.

> **The security of allowing root access**
>
> This kind of behavior raise the obvious question, isn't this a security risk? Why should it be so easy for the root
> user to have full access to RavenDB?
>
> The answer is that it doesn't actually matter. If an attack has enough privileges to run as the root user, they have
> enough access to do whatever they want. This includes injecting code into the RavenDB process, read and write to its
> memory, etc. 
> 
> If there is no protection from the root user, there is no point in making the operations team work any harder than it
> should be. 

In addition to getting the server certificate and using that to authenticate against your RavenDB server, there is 
another option, the `rvn` tool. The `rvn` tool is used for various tasks. We've discussed `rvn offline-operation` in 
Chapter 13, where we saw how it can be used to encrypt the server store. The `rvn` tool can also be used in 
`rvn admin-channel` mode in which case it will use a named pipe to connect to the server and allow you to do various
tasks.

In particular, you can run commands such as adding a new client certificate, dig into stats, force a GC collection, 
restart teh server and even access the Admin JS Console previously discussed in this chapter. 

The `rvn admin-channel` uses a named pipe, controlled by the operating system permissions. Only root / administrator
and the user that is running the RavenDB process have access to this named pipe and can send such commands to RavenDB.

#### Dealing with data corruption

Hardware breaks and disk fail, eventually entrophy will take us all. I'll assume that you can get over the depression
inherint to the previous statement and actually want to do something when such things happen. The scenario we have to
deal with in this case is a bad sector, hardware failure or something similar that resulted in data corruption of the 
RavenDB on disk files.

RavenDB uses hashing to ensure that the data has been successfully written to the disk. This allow RavenDB to detect if
there have been unexpected changes to the data.^[By default the hash function use is XXHash64, which is a non cryptographic
hash function. It doesn't provide any level of authentication and a user can update the data and the hash and RavenDB will
have no way of knowning this happened. When using encrypted database, RavenDB uses cryptographic authentication to ensure
that the data was modified only by someone who holds the secret encrypt key.] In this way, data corruption can be detected
and handled early on, rather than detected when it already had the chance of spreading its corruption. 
RavenDB will mark a database with such error as corrupted and shut it down.

If you are running in a cluster, the usual way to handle such a scenario is to replace the hard disk on the faulty node and
just let RavenDB replicate the data back to the node. This is this easiest way to handle things. But if you don't have the 
data on another node, you might need to take additional steps. 

The `Voron.Recovery` tool is provided to handle such scenarios. What it does it to read a RavenDB database (data file and 
journals) and go over the data, one byte at a time. It doesn't rely on the structure of the file and _assume_ that the file is
corrupted. It tries to recover any data that it can by going over the raw data.

The output of this tool is a set of `.ravendump` files (gzipped json, essentially) that contain all the recovered data as well
the list of errors that it encountered. You can import these `.ravendump` files into a new database to recover the documents.

### Routine maintenance

By design, RavenDB is not one to require regular maintenance. For most things, RavenDB will quite happily clean after itself
and not require a human involvement in the process. Monitoring is the way to go here, to alert you when there are things that
RavenDB cannot recover from automatically.

Things like running out of disk space or having an index definition that throws on many documents are two examples of such 
issues. Tasks such as backups and ETL jobs are defined once and then executed by the cluster on its own, without the need 
for a babysitter. 

The most common task for an admin to do, in fact, it to go over the list of tasks that the cluster has to do and see whether
they are still relevant. For example, the number of subscriptions on a database may be very large, but many of them haven't
had a client for weeks or months. This doesn't actually _hurt_ anything, but it can be nice to trim such subscriptions to 
make it easier to see what is actually going on in your cluster.

### Summary

In this chapter we have gone the core details of keeping RavenDB running in production. A lot of that involves setting up 
proper monitoring and alerts if something happens that RavenDB isn't actually able to fix on its own. RavenDB doesn't need
a lot of regular maintenance, it does most things automatically. However, when something breaks, you want to have a red 
light flashing somewhere and a person to look at things and help RavenDB recover. As a result, RavenDB contains a lot of 
features aimed specifically at enabling proper monitoring and externalizing its internal state to make the operations team's
job easier.

The first topic we discussed was SNMP monitoring, a protocol that is supposed by all operational monitoring system to check
the state of servers and network services. RavenDB support this protocol and expose quite a lot of data to the monitoring 
system for observation and analysis. You also have more detailed information avaialble in JSON endpoints, giving you multiple
ways to access the data and act upon it.

In addition to simple exposure of state, RavenDB also continiously analyses its own state and decide what to do about it. In 
most cases, these actions are internal and reflects RavenDB's automatic adjustment to the environment. There are also cases
where RavenDB will issue alerts and performance hints for the operators to look at. The performance hints in particular should
interest the operations team, because they can usually head off problematic direction for the system. 

The alerts are of more immediate concern, of course, but we have found that the performance hints may show up today but the 
problem they point to may happen in a few weeks or months. It is good to take action upon them early, before you have a crisis
on your hands.

Routine monitoring of our application will let us know when something is... _off_. But we might need more information, so we
looked at how to figure out what is going on inside the server. Tools such as Admin Logs and Traffic Watch give you immediate
insight into what is going on. Admin Logs can enable logs on the fly, even if the logs has been disabled in the config, you 
can enable them on the fly. Traffic Watch allows you to watch the incoming requests and monitor latency and usage.

In most troubleshooting scenarios, however, you'll head directly to the debug info package. This can capture a snapshot of
all the details about the running server (or even the entire cluster) in a single zip file. The package make it very 
convenient to explore the entire state of RavenDB at a given point in time. It is also useful to first capture the state of
the server, then restart it (which fixes most problem). You can do offline analysis of the package at your leisure, while 
restoring the system to full functionality as soon as possible.

We then discussed network and connection issues. We went over some common problems that may occur in your environment, in 
particular related to firewalls and other network appliances. The `openssl s_client` command is very useful in such scenarios
since it gives you a place to start such an investigation in a very simple manner. We also went over internal and external
IPs and how to resolve a potential identity crisis for RavenDB when running in such an environment. We have to explictly
tell RavenDB what IP to bind to and what hostname to tell clients to use to connect to the server, otherwise it is likely
to create confusion. 

The Admin JS Console is a way for you to run scripts directly inside RavenDB. This can allow you to mutate the internal
state of RavenDB (for example, to enable logging to a file without having to restart), but it can also be dangerous as 
you are literally modifyin RavenDB on the fly. 

The final topic we have gone over is how to deal with disasters. RavenDB was explicitly designed to run in a hostile 
environment, where nodes may go down and hardware fail. We went over several strategies to deal with such cases, mostly
by adding redundency to handle failures. There is also the `Voron.Recovery` tool that can extract data from a partially
corrupted RavenDB database if the hard disk itself has suffered failure.

Even though we talked about disaster recovery, we haven't covered one crucial topic. Backup (and restore) of RavenDB. 
This is because this is an important topic and it deserved a proper forum for discussion. The next chapter is dedicated
to backups, and even more importantly, how to restore your systems from backup.
