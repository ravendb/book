
## Operational recipes

This chapter is going to be a short one, mostly consisting of walkthroughs of particular tasks that you might need to 
do as part of operating a RavenDB cluster. This isn't meant to replace the online documentation but complement it. 
I tried to cover most of the common scenarios that you might run into in your systems. Beyond using this as a reference
I also suggest going through the various recipes to get some insight about the manner in which such issues are resolved.

Many of the methods listed in this chapter aren't new, we have covered them in previous chapters. Here they are just 
laid out in practical terms as a set of operations to achieve a specific goal.

Before we start, I want to mention something quite important. All of this recipes are meant to be used to solve the 
problem in which they are introduced. However, they often include deep assumptions about what is going on and it is
_not_ safe to blindly apply the techniques described here for other situations or without following the exact same
procedures listed here.

This chapter is where we learn how to make fixes to the system while it is running. RavenDB was explicitly designed
to allow such things. However, it is still modifying a live system, often in situations that it is already stressed
or in partial/full failure mode. The recipes here will help you get out of such scenarios, but mistakes can cause
hard failures.

Now that I've finished scaring you, let's us get to the real meat of this chapter. I included specific warnings for
common issues that you might want to pay attention to and you'll probably conclude that it isn't _that_ scary. 

### Cluster recipes

The cluster is the workhorse that users of RavenDB rarely think about. The cluster is responsible for managing all the 
nodes in the cluster, assign work, monitor the health of the nodes, etc. Most of the time, you setup the cluster once
and then let it do its thing.

This section is going to talk about when you need to go in and intervene. The usual reasons for that are when you want to 
modify your cluster topology. The cluster topology isn't meant to be static and RavenDB explicitly supports online 
modifications of the topology, so there isn't a high level of complexity involved. There are some details that you should
take into account, though.

#### Adding a new node to the cluster

You might have started with a single node or have few members in the cluster already, but at some point, you'll want to
add a new node to your cluster. Most of the time, you setup the cluster itself as a whole using the setup wizard, so you
might not even be aware how to do this task.

> **Prerequisites for the new node**
>
> I'm assuming that you are running in a secured mode (the only reason you aren't is if you are playing around on the 
> local machine). That means that in order to add the new node, you'll need to have mutual trust between the new node
> and the existing cluster. The easiest way to do is to use a single wildcard certificate for all the nodes in 
> the cluster.
> In this way, because the new node is using the same certificate as the rest of the cluster, it will intrinsically
> trust the remote node from the cluster and accept its addition to the 
>
> cluster. If you don't have the same certificate for all the nodes, you'll need to register the cluster certificate in the new
> node using (`rvn admin-channel` and then the `trustServerCert` command). 

You'll have to take care of the certificate (it is easiest to use a single wildcard certificate for all the nodes in the 
cluster, although that is not required), DNS updates, firewall configuration, etc. I'm assuming that all of this is all
properly setup ahead of time. There is nothing special about that part of the preparation.

When you start a new node for the first time, it defaults to using `Setup.Mode=Initial` and will start the setup wizard.
This is _not_ what you want in this case. Take a look at the `settings.json` file from one of your existing cluster nodes
and use that as the template for the configuration of the new node. You'll have to change the `ServerUrl`, 
`PublicServerUrl`, `ServerUrl.Tcp` and `PublicServerUrl.Tcp` options to match the new node location. Beyond that, keep the
configuration identical. While it is _possible_ to run with different configurations on different nodes, it is not 
recommended and too high a variance of some options will cause errors. 

Once you are done with the configuration, you can start the new node. You can connect to it and you'll see the studio
as usual, but there are no databases or documents in there, obviously. In fact, this new node is currently running in 
`passive` mode. In this mode, it is not yet determined whether the node will be joined to an existing cluster or form
a completely new cluster.
A new cluster will be formed if you create a new database or setup client certificates. In general, anything that shows
that you are preparing this node to stand on its own. 

Once you have verified that the new node is up, running and accessible (but don't perform any actions on the new node) go
to the _existing_ node and head to `Manage Server`, `Cluster` and then to `Add Node to cluster`. You can see how this 
will look in Figure 18.1.

![Adding a new node to an existing cluster](./Ch18/img01.png)

Figure 18.1 shows the new node screen. There isn't much there, because you don't need to do much beyond provide the URL
of the new node to add to the cluster. Once you click on the `Add` button, the cluster will connect to the new node, 
add it to the topology and start updating it with the current state of the system. During this period, the node will
show up in the `Promotable` state. 

In the `Promotable` state, the node is part of the cluster in the sense that it is getting updates, but it is not actually
taking part in votes or applicable to run for the cluster leadership. When the cluster is finished updating the new node
with the existing cluster state, the cluster will move the node into the `Member` state. At this point, the new node will
be able to become a leader and its vote will be counted by the cluster. 

At this point, the node is added to the cluster and can become the leader. However, it is still missing something. It has
no databases. RavenDB will not auto move databases between nodes when you add or remove a node from the cluster. This is
left to the discretion of the operations team. New databases created on the cluster will take the new node into account 
and the cluster may decide to migrate databases from a failed node to the new node, but these are rather exceptional
circumstances.

You'll typically wait until the node is fully joined to the cluster and then tell RavenDB to start migrating specific 
databases to the new node yourself. Remember that such a process is transparent to the client and will not impact the
usual operations in any manner.

#### Removing a node from the cluster

The flip side of adding a now to the cluster is removing a node. The _how_ of it is pretty simple, as you can see in 
Figure 18.2.

![Cluster node in the `Cluster` view, the red trash icon allows you to remove the node from the cluster](./Ch18/img02.png)

In Figure 18.2 you can see the nodes view in the studio. The red trash icon at the top right allows you to remove the node
from the cluster. Clicking this will cause the cluster to remove the node from the cluster. What does this mean?

At the cluster level, it means that we'll remove the node, obviously. Any database groups that contain this node will have
the node removed from the database group (and the replication factor adjusted accordingly). Any databases that reside only
on this node will be removed from the cluster entirely.

At the removed node level, it will revert back to passive mode. In this mode, the node is still accessible, but the 
databases that reside on this node are unloaded. This is because the node is in passive mode. You'll need to either re-join
the node to the cluster or let it know that it should form a new cluster before the databases on this node become available
again.
This is done to avoid clients or ETL tasks from talking to the now isolated database instance on the node that was removed
from the cluster. 

Forming a new cluster is a one-way option. After the new node was moved from passive mode, you cannot add it back to the 
previous cluster (or any other, for that matter). A cluster can only add either empty nodes or nodes that were previously
attached to the same cluster.

When a new cluster is formed on the formerly passive node, the topology of all the database groups is adjusted. All the 
database groups that include the current node are shrunk to include just the current node. All the database groups that
do not contain the current node are removed. 

> **Don't create a new cluster with the old URL**
>
> Forming a new cluster on the removed node is fine, but you should make sure that this is done with a _new_ URL. This
> is to avoid the case of a client using an old URL or an ETL task that hasn't been updated. The new cluster will share
> the same security configuration and you'll want to avoid existing clients and tasks talking to the newly independent
> node while thinking that they are talking to the cluster as a whole.

All other cluster-wide settings, such as the certificates and database access authorization remain the same. You should
take that into account if you are intending to keep the node around after removing it from the cluster.

#### Migrate a node between machines

Node migration can happen for any number of reasons. The most obvious one is that the server IP has changed, and you need
to update the cluster topology. For this reason, among many others, it is not recommended to use raw IPs in the cluster
topology. Instead, use proper URLs and DNS to control the name resolution.

In such a way, moving node C from `10.0.0.28` to `10.0.0.32` can be done simply by updating the DNS of `c.prod.rvn` to the 
new value and forcing a DNS flush. You can do that with machine names, of course, but that would just move the problem if 
you need to change the host machine. A good example is if you setup a RavenDB node on `prodsrv1` and you need to move it to 
`prodsrv3`. At the same time, `prodsrv1` is used for other services, so you cannot just change the DNS.

> **Use dedicated DNS entries for RavenDB nodes**
>
> Avoid using IPs or hostnames for RavenDB nodes. The easiest option is to have a DNS entry for each RavenDB node that
> can be changed directly if needed. This can make changing the physical topology of the cluster as easy as running a
> DNS update. This section is how to handle the case when you did _not_ do that.

I'm assuming that you have to move a node here by updating the cluster topology, not just updating the DNS (which is the 
recommended mode). There are a few things that you need to do to ensure that the transition is handled properly. Starting
from obtaining a certificate for the new node URL. Once that is done, the process is simple. 

We'll assume that you have a cluster of three nodes (A, B and C) and that you want to move node C to a different machine. 
The first thing to do is to shut down node C. Then go to one of the other nodes and remove node C from the cluster. You
can also do things in the reverse order, first remove node C and then shut it down, it doesn't really matter.

Move node C to another machine, setup the certificate and update the `ServerUrl`, `PublicServerUrl`, `ServerUrl.Tcp` and 
`PublicServerUrl.Tcp` options in the `settings.json` file. Then go to node A or B and follow the same procedure to add
a new node to the cluster. The new will be re-added (still called node C). RavenDB remembers the node and will perform
all the necessary hookups to make sure that the newly added node return to its rightful place in the cluster.

#### Replacing a node on the fly

What happens if a node in the cluster suffers a catastrophic failure? For example, let's assume that node C had a hard
disk failure and went down completely. You restore the machine, but all the cluster data and databases on node C is gone.
What is the best way to handle this scenario?

Because node C effectively suffer from amnesia, the first thing we need to do is to go to the cluster and demote it from
a full member to a watcher. We'll discuss this in more detail in the next section, but the core idea is that we want to
avoid giving node C any decision making power (which, as a full member node, it has) until it recovers from its period
of forgetfulness.

Once node C was demoted to a watcher, we can let the cluster handle the rest of it. Just startup RavenDB again, and since 
it has no data, it will start up as a passive node. Because it is using an existing node URL, the cluster will connect 
to it and update it with its current state. That includes any databases that should reside on node C. The other nodes 
that have databases which also reside on C will start replicating the data back to node C. 

In a short order, the cluster will make sure that node C is backed up again, has all of the relevant data, is fully up
to speed and can be a contributing member of the cluster. At this point, you can promote it to be a full member again.
We talked about members and watchers in Chapter 7, but a refresher is probably in order. 

#### Promoting and demoting nodes in the cluster

The nodes in a RavenDB cluster can be in the following states: Member, Promotable, and Watcher. A member node is a fully
functional part of the cluster, able to cast votes and be elected as the leader. Typically, a cluster will have up to 
seven members at any one time. A promotable member is one that is currently in the process of catching up with the state
of the cluster and is typically only seen when you add a new node to the cluster.

A watcher is a member of the cluster that is managed by the cluster (assigned work, monitored, etc) but has no way of
impacting the cluster. It isn't asked to vote on commands, nor can it be elected as the leader. Once your cluster grows
beyond seven nodes, you'll typically start adding new nodes as watchers instead of full members. This is to reduce the
latency for cluster-wide operations by involving a smaller number of nodes.

In Figure 18.1 you can see the `Add node as Watcher` option, which will add a new node to the cluster already as a watcher.
You can also demote member nodes to watchers and promote watchers to be full members. Figure 18.2 shows the `Demote` 
button for node D. 

You'll typically only demote a node when you are recovering from some sort of fatal error that caused amnesia, as was 
discussed in the previous section. Alternatively, you might want to shift the responsibility of the cluster to the newer,
more powerful nodes in the cluster. Each of the nodes is exactly the same, the only difference is what role they are 
assigned to. Any watcher node can become a member node and vice versa. 

#### Bring up a cluster from a single surviving node

All the cluster-wide operations (adding/removing nodes) require that the cluster itself be healthy. A healthy cluster
is one where a majority of the nodes are able to communicate with one another. There are certain disastrous cases where
that doesn't hold. In a three node cluster, if you lost two of the nodes, the cluster cannot recover from this. When one
of the failed node recovers, so will the cluster.

However, what happens if it _can't_ recover? Imagine that you had a hard failure on both of these machines, leaving you
with a sole surviving node. What can you do at that point?
You cannot remove the failed node from the cluster on the surviving node, because there is no majority to confirm this 
decision. 

At this point, we have a nuclear option to resolve this scenario. Unilaterally seceding from the cluster. As you can 
imagine, this is considered to be a _rude_ operation and not something to be handled lightly. This feature exists to 
handle these specific circumstances. It will forcibly update the internal cluster topology on a node, without waiting for
a majority vote on the cluster and create a new single node cluster. 

This can be achieved only through the Admin JS Console that we discussed in Chapter 16. Go to `Manager Server`,
`Admin JS Console` and ensure that you are running in the context of the server, then run the command in Listing 18.1.

```{caption="Emergency command to initiate cluster secession on the current node" .js}
return server.ServerStore.Engine.HardResetToNewCluster();
``` 

This command will output the new topology id. And you can now go to the `Cluster` view and see that there is only a 
single node in this cluster now and that the current node is the leader. At this point, you'll be able to run 
cluster-wide operations, such as adding a new node to the cluster.

A small wrinkle here is that RavenDB validates that a node that is added to the cluster is either a brand new node or was
previously part of the same cluster. Because the node has seceded, it is now its own cluster. This can be an issue if you lost
three out of five nodes in a cluster. 

> **Do _not_ pull the emergency handbrake**
>
> This, like anything else provided under the Admin JS Console, is provided with an important caveat. This is modifying the
> internal structure of RavenDB in a very invasive way. It isn't meant to be something that you use except in the most
> dire of emergencies.
> 
> You need to verify that this is the only thing that is running on the system while this is going on and execute the 
> sequence of steps exactly as specified. Otherwise, it might result in bad system behavior or unexpected results. 

Losing three nodes means that you don't have a majority and must cause one of the nodes to secede from the cluster. But you
can't have two nodes secede in this manner since each will create their own single node cluster. 
Let's assume that you have only nodes A and B from a five nodes cluster remaining. We can't just move to a single node cluster
so we need to take a slightly more complex series of steps.

On node A, run the `HardResetToNewCluster` command and note the topology id that was provided. On node B, in the Admin JS 
Console you'll need to execute the command shown in Listing 18.2 (remember to update the topology id from the previous step).

```{caption="Emergency cluster secession in favor of a paritcular cluster topology" .js}
server.ServerStore.Engine.HardResetToPassive(
        // the topology id from Listing 18.1 goes here
         "xxxxxxxx-c4d2-494e-bc36-02274ccc6e4c"
);
```

Listing 18.2 shows the command to secede from the existing cluster in favor of a new one. This will also mark
node B, where this was run, as passive. You'll need to go to node A (where `HardResetToNewCluster` was run) and add node B
to the cluster again.
If the live nodes aren't named nodes A and B, by the way, this process will rename them to _be_ node A and B. 

> **Which node should be made passive?**
> 
> In such a disaster scenario, it is not uncommon to have the different surviving node each with their own specific
> view of the state of the cluster and the command log. It is imperative that you'll select the node with the most
> up to date command log to be the one to reset to a new topology. 
> 
> You can use the `/admin/cluster/log` endpoint to check the status of the command log on each of the nodes. The node
> with the highest `CommitIndex` and the latest log in the `Entries` array is the one you should run `HardResetToNewCluster`
> on and `HardResetToPassive` should be run on the other(s).

At this point, you'll have a two nodes cluster and can start adding brand new nodes as usual. This process is only valid to
run on nodes that are part of the same cluster. It is not possible to use this to merge clusters and in general, this should
be treated as an option of last resort. 

### Database recipes

So far we talked a lot about all the things that happen to RavenDB at the cluster level. It's important to understand but
it is not something that you'll use on a day to day basis. It is far more common to have to deal with operations at the 
individual database level.

To clarify the terminology, a database in RavenDB can refer to a database group (a named database in the cluster, which resides
on one or more nodes) or a database instance (a named databased on a particular node in the cluster). We don't usually have to
distinguish between them because the process is transparent. We can look at the databases from any node and we don't need to
specify which node we are talking to for each database from the client side.

This is all thanks to the fact that RavenDB stores the database group topology on all the nodes in the cluster. This is what
allows any node in the cluster to tell a client where to find the database.

#### Reordering nodes and why it matters

You can see the database group topology in the studio by clicking on the `Manage group` button on the `Databases` page. Doing 
this will take you to the `Manage Database Group` page, as you can see in Figure 18.3.

![Examining the database group topology in the Studio](./Ch18/img03.png)

If you look closely at Figure 18.3 you might notice something odd. The order of the nodes there is _wrong_. It goes `[A, C, B]`
but _obviously_ it should be sorted, no? What is going on?

The order of the elements in the topology matters and RavenDB allows you to control that using the `Reorder nodes` button. But
what is so important about the order of the nodes? Put simply, this is the priority list that the clients will use when deciding
which node in the database group they will talk to. 

Usually, RavenDB will manage the list on its own, deciding the order in which nodes should talk to the different nodes as well
as the other tasks that are assigned to this database group. If the cluster detects that a node is down, it will drop it to the
bottom of the list and let clients know about this.

Clients, for their part, will use the list to decide what node to call whenever they need to query RavenDB. If the node they 
chose is down, they will automatically failover to the next node in the list. Note that in this case, both cluster and the 
clients are working cooperatively and independently of one another. The cluster gives its opinion on the best node to use at
any given point. The client will use that list but may face different conditions. If the cluster is able to reach a node and
the client isn't, the client can still proceed to failover to the other nodes in the topology.

Clients will update their topology of the database group on the next request after the topology has changed. This is done by
the server setting a header in the response of a request that let the client know that the server topology has changed. In 
practice, this means that changing topologies are usually visible to all clients within a very short amount of time.

That, in turn, leads to interesting options. For example, you can use this feature to shape the way the clients will talk to
a particular node. You can move it to the bottom of the list to have the clients avoid talking to it (assuming no round robin
or fastest node options are in place). You can also move it to the top so the clients will prefer to use that particular node.

This is useful if you want to take a node down and want to gracefully move traffic away from it instead of having clients fail
(and then recover by failing over to another node in the database group). 

#### Moving a database between nodes

A database instance is the term used to refer to a specific database inside a particular node. Sometimes, for whatever reasons,
you want to move a database instance between nodes. There are a few ways of doing this, from the easy (letting RavenDB do that
for you) to the manual (when you do everything). We'll discuss the easy way to do things next, right now I want to focus on 
the manual mode.

If there is an easy way, why should we go to the trouble of doing this manually? Mostly because in some _specific_ cases (by
no means all of them, mind) it can be faster to do things directly. It is also a real exposure of how RavenDB is actually
managing databases internally and may be helpful in other places.

The way RavenDB is actually managing your databases across the cluster is interesting. At the cluster level, RavenDB coordinates
between the various nodes to achieve consensus on the `Database Record`. The `Database Record` is a JSON document that describes
that database itself. You can see it by going into one of the databases, then to `Settings`, `Database Record`. Listing 18.3 
shows a (simplified) example of such a database record.

```{caption="A (somewhat simplified for space reasons) database record defining a database in RavenDB" .json}
{
    "DatabaseName": "Northwind.Orders",
    "Disabled": false,
    "Encrypted": false,
    "Topology": {
        "Members": [
            "A",
            "B"
        ],
        "DynamicNodesDistribution": false,
        "ReplicationFactor": 2
    },
    "Indexes": {
        "Orders/ByCompany": {
            "Name": "Orders/ByCompany",
            "Priority": "Normal",
            "LockMode": "Unlock",
            "Maps": [
                "from order in docs.Orders /* redacted */"
            ],
            "Reduce": "from result in results /* redacted */",
            "Type": "MapReduce"
        }
    },
    "Revisions": {
        "Collections": {
            "Orders": {
                "Disabled": false,
                "PurgeOnDelete": false
            }
        }
    },
    "PeriodicBackups": [],
    "ExternalReplications": [],
    "RavenEtls": [],
    "SqlEtls": [],
    "Etag": 113
}
```

What you see in Listing 18.3 is what goes on behind the scenes. You have the database topology (the members of the Database
Group), you have the index definitions, revisions configurations, and you can see where we would define tasks for the database.
When I talk about the cluster managing the database, what ends up happening is that the cluster mutate this document and ensures
that all the nodes in the cluster have a consistent view of it.

> **Pay no attention to the man behind the curtain**
> 
> There is no magic. RavenDB uses the `Database Record` to tell where database needs to go, and a node will create an instance
> of the database when it is assigned to the node. A database instance is the set of files, threads, and in-memory data structures
> required to handle queries and operations.

The database instance managed at the node level is, to use a very crude resolution, just the set of files that make up the database
data on that node. How does all of this relate to moving a database between nodes? Well, let's see how we can take advantage of this
behavior to move a database manually between nodes.

The first thing to do is to actually remove the database from the origin node. You can either soft delete the database (if it exists
on only a single node) or remove it (using soft delete) from the Database Group. This can be done in the `Manage Database Group` view
under the database's `Settings`. You can see how this would look like in Figure 18.4.

![Removing a database from a node can be done using soft-delete, leaving the database files untouched](./Ch18/img04.png)

This soft delete measure leaves the database files on the disk. Furthermore, it also means that the RavenDB server on the origin node
will close all the file handles and release any resources associated with this database. At this point (and at this point _only_) we
can take the database folder and move it to another machine.

> **Don't make a habit of schlepping databases around manually**
>
> It is _not_ safe to generally much about in the RavenDB directory. That is true for users in general as well as other things.
> Stuff like antiviruses, file system indexing, file monitoring, etc. RavenDB has firm ideas about how it interacts with the disk 
> and it expects things to match what is expected. Interfering with this can cause issues.
>
> In this case, we have started off by explicitly shutting down the database (by soft deleting it). This gives RavenDB the 
> chance to do a graceful shutdown, close all the file handles and free all related resources. That is the only reason it is OK
> for us to muck about in RavenDB's data directory.

The "move it to another machine" is the key reason why we went to all this trouble. This presupposes that you have some fast way to
get the data from one machine to the other more quickly than sending it over the network. A common scenario where this is actually
achievable is when you have a large database and it is faster to literally move the disk from one machine to another.

Another such scenario is when your "disk" is actually a cloud storage volume. This allows you to detach it from one machine and then
attach it to another easily enough. In that case, it might be worth this hassle. Otherwise, just use RavenDB's built-in replication
for this scenario.

> **This is _not_ going to work to clone a database**
>
> You might be tempted to use this approach to quickly clone a new database when you want to add a new node to the database group.
> This will not work because the internal database ID will be the same across multiple nodes, something that is not allowed and 
> can cause complex issues down the road. RavenDB will not be able to tell where exactly a specific change happen, so it will not
> be able to tell if documents should be replicated to the other nodes or not. 
> 
> This approach is only valid if you remove the data from the old node entirely, so there is just one copy of the database with
> the same database id. 

Once you have the database directory on the other machine, make sure that it is in the expected path on the destination and
add it to the new node. When updating the `Database Record` RavenDB will open the directory and find the pre-existing files 
there. From that point on, it will just proceed normally. 

#### Renaming a database

Given the previous topic, can you imagine how you'll rename a database with RavenDB? 

This seems quite easy to do. All you need to do is soft delete the database and then re-create the database with a new name
and the same path. 

There are a few things that you need to pay attention to. Only the _database level_ data is going to be preserved in this 
scenario. Tasks and data at the cluster level will _not_ be retained in this scenario. This includes tasks such as subscriptions,
backups, ETLs, etc. It also includes database _data_ that is stored at the cluster level such as identities and compare exchange values.

In short, don't do this. If you want to rename a database, the proper way to do this is to backup and restore it under a different name.

#### Expanding and shrinking the Database Group

I mentioned that there is an easy way to move a database between nodes earlier. This way is quite simple, first expand the Database
Group to include the new node, wait for replication to complete and then remove the node you want from the group. 

You can do this from the `Settings`, `Manage Database Group` view. Expanding the Database Group to include a new node is as simple
as clicking on `Add node to group` and selecting which node you want to add. Once that is done, the RavenDB cluster will create the
database on the new node and assign one of the existing nodes in the Database Group for the initial seeding of data. 

The `Manage Database Group` view will allow you to monitor the status of the initial seeding until the cluster detects that the 
new node is up to speed and promote it to a full member status. At this point, you can remove another node from the Database Group
and the cluster will adjust. 

Following up what we already know, you'll probably first move the node that is about to be removed from the Database Group lower
in the priority and wait a bit to give clients the chance to learn about the new topology and connect to another server. This way,
when you remove the node, no client will be talking to it.

> **Removing the last node in the group will delete the database**
> 
> Removing a node can be done as either a soft delete (keep the files, as we previously) discussed or a hard delete. Even if you 
> chose a soft delete, if this is the last node in the Database Group, the database will be removed from the cluster. The same rules
> about the cluster level data apply as when we talked about the wrong way to rename a database. 
> 
> A soft delete of the last node in a Database Group will keep the database level data intact on that node, but all the cluster data
> (tasks, identities, compare exchange values, etc) will be removed from the cluster. 

The advantage of this method for the operations team is obvious. There is very little hand holding that is actually required. 
Expanding the group means that RavenDB will handle all the details of moving the data to the other node(s) and setting everything up.

The downside is that it requires RavenDB to do a fair amount of work (effectively, read the entire database and send it over the
network) and on the other side, RavenDB will have to write out the entire database. This is intentionally handled in such a way that
will not impact overall operations too much. The idea is that the stability of the cluster is more important than raw speed. 

This might sound like we intentionally throttle things down, but nothing could be further from the truth. It is just that we are
careful to ensure that adding a new node isn't going to consume so many resources that it is prohibitively expensive. After all,
the most common reason to want another node is that we want to share an already high load.

#### Scaling database under load

RavenDB contains several features that are intended to allow you to scale your systems easily. Among them, we have the distribution of
tasks across the nodes in the Database Group and the ability to do load balancing between the nodes. 

Assuming that you start seeing a very high load on your system, what options do you have? The first thing to do is to look at the kind 
of client configuration you have. This is available in `Settings`, `Client Configuration`, as shown in Figure 18.5.

![The client configuration allow you to change the load balancing behavior of clients on the fly](./Ch18/img05.png)

Client configuration, just like the cluster topology, is defined on the cluster and disseminated to clients automatically. This means
that if this isn't set, the first thing you can do is to set the `Read balance behavior` to `Round Robin` and wait a bit. You'll see
how clients start distributing their reads across the entire Database Group.

If you do have this value already setup and you seem RavenDB struggling even with the load split among the different nodes of 
the Database Group, the obvious next step is to add new nodes to the group. This specific scenario is why RavenDB favors stability
when you add a new node to the database group. Only a single node in the group will be in charge of updating the new node and even on
that node, we'll ensure that we send the data at a rate that both sender and receiver can handle without choking. 

So it is possible to do this under load, but quite obviously not recommended. If you have a large database, the initial seeding may 
take a while. During this time, you don't have any extra capacity and in fact, the replication is actually taking (some limited amount)
of system resources. If at all possible, try doing this sort of things ahead of time or during idle periods.

If you do find yourself in this situation, remember that because the initial seeding of a new node happens from a single designated node, 
you can actually add _multiple_ nodes to the Database Group at the same time and they will be assigned to different nodes. In some 
cases, that can significantly help reduce the time to increase the overall capacity.

### Miscellaneous operational tasks

Beyond managing the cluster and the database, there are also operational concerns inside a particular database. This section touches
on some of these cases and will provide you with options on how to resolve them.

In particular, we are going to go over RavenDB features that were already discussed. However, we are going to see how they can be 
applied and chained together to achieve some really nice results.

#### Offline query optimization

We discussed RavenDB's query optimizer and its ability to detect changes to the operational enviornment (such as deployment of a 
new release of your software, with different queries). Under such conditions, the query optimizer will adjust the indexes on the 
database to better serve the new behavior.

You _can_ do that in production on live deploy. Indeed, this feature is explicitly meant to be used in such a scenario. However, 
in many cases it is valuable to avoid having the adjustment period on live production. Instead, it is better to do this on the 
side and introduce the changes to production at an early enough date that by the time the new code is deployed, the database is
already expecting the changes. So, how do we go about doing this?

Use a separate instance of your database (such as the UAT / QA instance) and run the new version of your application against that
instance. At this point, I suggest running whatever system tests you have, run your load testing procedure and encourage users 
to click around randomly. Basically, exercise your system to make it query the database.

The query optimizer on the database will analyze all this activity and create the relevant indexes for you. After a while, you'll
see that the system is now in a steady state. There is enough information now that all the queries in use are properly being 
served by indexes.

At this point, you go to the database and export _just the indexes_. This will export any static indexes you have created on the
database, but most importantly, this will also export any automatic indexes that were created by the query optimizer. You can now
take the indexes export and import that on your production system.

This will introduce the new indexes to production. RavenDB will start building them on the fly, replacing older inexes once the 
indexing is done. You can monitor that and wait until all the additional indexing activity is complete. Because your application
isn't going to be using the new indexes, there will be very little impact while they are being build. Once the indexes are done,
you can deploy your new version to production knowing that you already taught the database what it can expect.

#### Daisy chaining data

In Chapter 11 we talked about using MapReduce indexes and their ability to output the results to a collection. In Chapter 8 we
talked about ETL processes and how we can use them to push some data to another database (RavenDB or a relational one). Brining
these two features together can be surprisingly useful when you start talking about global distributed processing. 

It might be easier to understand if we'll use a concrete example. Imagine a shoe store (We'll go with Gary's Shoes) that needs to
track sales across a large number of locations. Because sales must be processed regardless of the connection status, each store
host a RavenDB server to record the sales. Figure 18.6 shows the geographic distriubtion of the stores.

![Gary's Shoes stores locations (and the locations of the servers)](./Ch18/img06.png)

To properly manage this chain of stores, we need to be able to look at data across all stores. One way of doing that would be
to setup external replication from each of the store locations to a central server. This way, all the data is aggregated into
a single location. Indeed, in most cases, that would be the natural thing to do. In fact, you would probably have two way 
replication of most of the data. This way, you can figure out where a specific model is located in nearby stores by just looking
at the local copy of their inventory.

But for the purpose of this discussion we'll assume that there are enough shoe sales that we don't actually want to have all
the sales replicated. We just want some aggregated data. But we want this data aggregated not just on any individual store
but across all of them. Here is how we can handle this. We'll define an index that would aggregate the sales across the 
dimensions that we care about (model, date, demographic, etc). 

This index can answer the kind of queries we want, but it is defined on the database for each store, so it can only provide
information about local sales, not what happens across all the stores. Let's fix that. We'll change the index to have an output
collection. This will cause it to write all its output as documents to a dedicated collection.

Why does this matter? These documents will be written to solely by the index, but given that they are documents, they obey
all the usual rules of documents and they can be acted upon like any other document. In particular, this means that we can 
apply an ETL process to them. Figure 18.7 shows how this ETL script will look like.

![ETL Script to send aggregated sales to a central server](./Ch18/img07.png)

The script will send the aggregated sales (the collection generated by the MapReduce index) to a central server. Note that
we also add some static fields that will be helpful on the remote server (to tell which store each aggregated sale came 
from). At the central server, you can work with these aggreated sales documents to see details per store, or aggregate 
them again to see the state across the entire chain.

The nice thing about this approach is the combination of features and their end result. At the local leve, you have independent
servers that can work seamlessly with unreliable network. They also allow a store manager to have a good overview of their
local state and what is going on inside their own store. At the same time, across the entire chain, we have ETL processes 
that will update the central server with details about sales status on an ongoing basis.

If there is some network failure, there is no interruption in service (except that sales date for a particular store will
obviously not be up to date). When the network issue is resolved, the central server will accept all the missing data and 
can update its reports. The entire process rely entirely on using features that are already there in RavenDB and easily
accessible.

The end result is a distributed, highly reliable and fault tolerant, MapReduce process that gives you aggregated view of 
sales across the entire chain with very little cost. 

#### Disconnect external replication / ETL from a remote source

The bad thing about push based ETL processes and replication^[Push based in this context refers to the fact that they
are defined in their source and they push the data to the destination] is that sometimes you want to disable them on 
the destination side. That can be a bit awkward to do

For example, you might want to disable the Jersey City store from the aggregation process we outlined in the previous 
section. That store has been sold and is no longer part of the chain (so you don't want its reports), but it will take
some time until the IT staff from the new owners will get around to replacing the internal systems. During that time,
you want to ignore the pushed data from that store.

The obvious way to do that is to shut down the ETL process at the source, but we are assuming that this is not an 
option that is currently available to you. How do you disable this at the _destination_? The answer is quite simple,
remove the credentials of the certificate that this store is using. 

With the credentials no longer being valid, that store will fail to connect to the central server and will be blocked
from pushing any additional data. Note that the certificate's credentials are checked during connection setup, not on
each individual request. You might need to drop already established connection to complete the process.

### Summary

This is an odd chapter. On the one hand I was trying to give you detailed recipes and steps on what to do if you run
into particular situations in production. Adding a node, expanding a database group and moving a node between machines
are all tasks that you may find yourself doing.
At the same time, all of these tasks (and more) are covered in detail in the online documention. 

This chapter isn't meant to replace the online documentation but to complement it. I intentionally chose some of the 
most common scenario and then talked not just about the list of steps that you have to go through but also the kind
of things that you need to watch out for and how it plays into the overall architecture of RavenDB.

The cluster recipes we have gone over should also help you better understand how the cluster itself is put together.
Not just in the sense that you follow the notion of the Raft protocol and consensus algorithms but what is actually
going to happen. The interaction between the nodes as they are added and removed from the cluster, for example, gives
a lot of detail on what it is that is going on at the node level.

Removing the magic behind what is going on and giving you a feel for what RavenDB is doing behind the scenes is very
important. It means that you can predict what RavenDB will do and _why_ it would do so. 

We then moved to discuss how databases are composed. The notion of the `Database Record` that is managed at the cluster
level while each node holds the databases instances that are assigned to it is crucial to understand. We looked at how
this can be useful if we want to move a database physically between machines (for example, but moving the disk volume
directly). 

We also talked about some scale and performance tricks. How to expand your database group and load balance work between
the nodes. This is something that you should be doing ahead of time, but it isn't a perfect world. RavenDB expects that
you'll have to do this kind of thing under fire, so to speak. Expanding the database group to add more capacity is an
operation that is controlled to limit how much additional load it will generate. 

Speaking of things that should be done ahead of time, we talked about how we can take the results of the query optimizer
from your load testing machine and apply them to your production systems ahead of new version deployment. This will 
allow the production cluster to benefit from the experience of the test server. This allow you to smooth the process
of introducing new behaviors in production.

We finished the chapter with an interesting challenge, distributed aggregation of data across many physically separated 
locations using several distinct features that were used in tandem. This kind of usage is pretty common. One of the 
strengths of RavenDB is the ability to treat features in an orthogonal manner. You can mix, match and combine them to 
get to some really nice results. 