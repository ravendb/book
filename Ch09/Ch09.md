
# Full text search

RavenDB indexes are based on top of the Lucene search engine library. Lucene was created specifically to allow to do fast searches on large amount of data using advanced Information Retrieval techniques. Colloquially, this is usually called Full Search Text.

It probably wouldn't surprise you that RavenDB is really good at that. Mostly because of the use of Lucene, but also because RavenDB does a whole _lot_ of work behind the scenes to get everything just right. We already discussed in detail all the process leading up to the actual indexing stage. In this chapter, we'll discuss the behavior of Lucene and how RavenDB is using it, as well as explain how you can take advantage of this information in your own systems.

Note that this isn't meant to be an exhaustive discussion of Lucene itself. The book [Lucene in Action ](http://www.manning.com/hatcher3/) is a great resource for that, and something that we have made use of during the development of RavenDB.

The first part of this chapter will discuss the theory behind full text search briefly, and the second part will show you how to make use of that inside RavenDB.

## The magic behind full text search

The major problem with full text search is that computers are pretty bad at it. A computer can look at sorted data very quickly, but doing free form searches is quite beyond its abilities. Our sample data is shown in Table 9.1.

|Post Id|Post title |
|-------|----------|
|posts/3570|Does you application has a blog? ^[This post is from 2008, and I cringe at the number of grammatical errors in just its title. ]|
|posts/165090|Early lock release, transactions and errors |
|posts/164869|Transaction merging, locks, background threads and performance |
|posts/167362|The fallacy of distributed transactions |
|posts/169859|Excerpts from the RavenDB Performance team report: Comparing Branch Tables |

: Real posts ids and titles, from [my blog ](http://ayende.com/blog).

Now, what would happen if I wanted to search for blog posts about transactions? Well, one way of doing that is to run the code in Listing 9.1.

```{caption="{Searching for all posts about transactions}" .cs}
var results = new List<BlogPost>();
foreach(var post in GetAllPosts())
{
	if(post.Title.Contains("transactions"))
		results.Add(post);
}
return results;
```
As you can imagine, this has... issues when the size of the data grows too big. This method is called a table scan, and it is generally a very bad idea in production systems.

So, how can we do this efficiently? We create an inverted index. Such an index breaks down the content that we want to index to individual terms, which then point to the document in question.

Here is what the inverted index for the data in Table 9.1.

|Term|Post Ids |    |Term|Post Ids |
|----|--------|----|----|--------|
|a|posts/3570| |and|posts/165090 posts/164869|
|application|posts/3570||background|posts/164869|
|blog|posts/3570||branch|posts/169859|
|comparing|posts/169859||distribute|posts/167362|
|do|posts/3570||early|posts/165090|
|error|posts/165090||excerpt|posts/169859|
|fallacy|posts/167362||from|posts/169859|
|have|posts/3570||lock|posts/165090|
|locks|posts/164869||merging|posts/164869|
|of|posts/167362||performance|posts/164869 posts/169859|
|ravendb|posts/169859||release|posts/165090|
|report|posts/169859||table|posts/169859|
|team|posts/169859||the|posts/169859 posts/167362|
|thread|posts/164869||transaction|posts/164869 posts/165090 posts/167362|
|you|posts/3570|

: Inverted index for the post titles in Table 9.1

Now that we have the data in this format, we can very easily find the posts that have transaction in them by doing binary search in the sorted data in Table 9.2 and then getting the relevant post ids. Even if we have a very large amount of data, we wouldn't have to make a lot of effort to search through it.

That is the magic of O(log N) vs. O(N) operations. But the inverted index in table 9.2 is just the beginning of the work we need to do for full text queries. Our next topic for discussion is analyzers, and their role in creating the inverted index.

### Analyzers

The analyzer role is to break up a piece of text to discrete terms. A term can be a word or a phrase that we'll later search upon. A trivial example of an analyzer processing some text would be just breaking it apart on a word boundary.

For example, given the text: "Transaction merging, locks, background threads and performance", we can break it into the following discrete terms: 

* Transaction
* merging, 
* locks, 
* background
* threads
* and
* performance

That would work, but it has a few issues. Note that we still have words with punctuation in them, there are upper case words and we have a term in its plural form.

All of those are typically handled by the analyzer. In most languages, the casing of the word isn't meaningful during a search. If I'm looking for "transaction", I would expect to find documents with the term "Transaction" in them. And it is very rarely meaningful to search with punctuation, so those are stripped.

After that, each analyzer goes ahead and do its own thing. An analyzer usually have a good understanding of the language involved, because a lot of the work it needs to do relies on doing language specific operations. 

In English, such operations can involve changing term from plural form to singular form, reducing "-ing" suffix, etc. A more complex analyzer can add meaning to the terms, so when we index the word "puppy", it will also recognize that this is a document about "dog". 

This isn't the place to go into all the details about the work that analyzers are doing, but hopefully you have good grasp of what they _can_ do, and what their role in the full text search system is. They are responsible for turning a whole bunch of text into smaller terms that we can search on.

RavenDB comes with several built-in analyzers, and you can add Lucene Analyzers as you need them for your own specific use cases.

* _Default Analyzer_ - Unless you specify otherwise, this is the analyzer that RavenDB will use. This analyzer will simply lowercase the whole string. This is useful to allow case insensitive searches on a specific value. Find me all user users named 'John' would be a typical example.
* _Whitespace Analyzer_ -  This analyzer will break apart the string to be searched into discrete words, but won't try to process them any further.
* _Standard Analyzer_ - This is the default analyzer we'll use if you mark a field as Analyzed. This analyzer will break apart the string into words, remove punctuation, remove the possessive S and is smart enough to recognize numbers and email addresses and keep them as is.  

Analyzer can be specified per field, or globally for all the fields in the index. We'll see exactly how that works a bit later in this chapter.

### The querying process

Analyzers aren't applied only during indexing. They are also applied during queries. For example, if I'm querying on the index shown in Table 9.2 to find all documents that match "the transactional fallacy". What am I supposed to get?

Well, if we were trying to search for the whole query, we would obviously fail. So instead, we break the query as well into terms. In this case, the terms would be: "the", "transaction" and "fallacy". 

> **The human analyzer**
>
> In order to make things simpler, the analyzer used to create table 9.2 and to analyzer the
> query is actually yours truly. 
> I've not run it through one of the existing analyzers to give better terms for the discussion. 
>
> The standard analyzer does a good job most of the time, but it isn't so good for showing the full
> power of full text search, and other analyzers that do a better job at that will generate non word
> terms.
>
> Let us take the terms "Transactions" and "transaction". The Standard Analyzer will lower case both, 
> but will not remove the plural S suffix from the first term. Analyzers that will do that, like the 
> Porter Analyzer will take a statement such as this: "Transactions merging, locks, background threads 
> and performance" and turn it into the following terms: transact merg lock background thread and 
> perform.
> 
> As you can see, some of those terms aren't English words. That doesn't matter for searching, but it 
> does introduce an extra hurdle that people need to go over when learning about full text search. 
> Hence, my decision to use a human analyzer instead. It makes it easier to explain the _concept_, and
> you can get all the gory details of that in Lucene in Action or other Information Retrieval texts.


	          	the 			transaction 	fallacy
-------------	------------- 	------------- 	-------
posts/164869 					Y	
posts/165090					Y
posts/167362	Y 				Y 				Y
posts/169859	Y						 

: The results for each term in the documents.

We are now going to search Table 9.2 for each of those terms, and we'll get the following results shown in Table 9.3, which is sorted by the post id, with the matching posts are shown on the same line. Now that we have the query results, we can process them further. There are two ways that we can go about giving the user the answer. We can apply an OR modify or an AND modifier.

If we were using an AND modifier, the result of this query would be a single document: posts/167362, with the title: "The fallacy of distributed transactions".

This is an exact match of what we wanted. But a single missed term would result in us getting no results. That is generally a bad thing. So let us see what would happen when we are querying using and OR modifier. In this case, the query results would be as shown in Table 9.4.

|Post Id|Post title |
|-------|----------|
|posts/165090|Early lock release, transactions and errors |
|posts/164869|Transaction merging, locks, background threads and performance |
|posts/167362|The fallacy of distributed transactions |
|posts/169859|Excerpts from the RavenDB Performance team report: Comparing Branch Tables |

: The final results of the query, using an OR modifier for all terms.

And here we get into the real conundrum of full text search, the ever present tension between the search precision and recall. 

Precision is the number of documents that matched the query that are relevant to the user. While recall is the number of relevant documents in the system that were returned out of all of the matching documents in the index.

It might be easier to look at them from the other direction. Precision is measuring how many results the users got that they didn't care about and recall is how much stuff that the users wanted to see and didn't get.

Google became really popular because they were really good at being relevant. It isn't uncommon for you to make query Google and have it find you _what you meant_ vs. what you actually searched. The first few times that happened, I was quite amazed, but then I got used to it and am now annoyed when it doesn't happen everywhere.

Full discussion of how to optimize your system for best precision and recall is out of scope for this book, unfortunately. This is fascinating topic with a lot of research and practical applications. The good news is that RavenDB is using Lucene to do the indexing, so a lot of the stuff justs works for you, and when you need to pull the heavy guns and tweak things, you can do it quite easily.

Without going too deep into the topic of optimizing search quality, let us see some simple techniques that we can use to filter out noisy results. 

#### Stop words

Take a look at table 9.2, it contains terms such as "a", "and", "has", "from", "of", "the" and "you". Those terms are _extremely_ common words in English. In fact, you would be hard pressed to find any meaningful text in English that doesn't contain them.

When we run the "the transactional fallacy" query, we got a result (posts/169859) that had nothing to do with the query at hand, just because it had the term "the" in it. This is clearly going to cause us issues.

In order to handle exactly that, we have the concept of stop words. Those are noisy words that happen all the time, and for most full text search purposes, we just filter them out. They are filtered out both
during the querying phase and during indexing, to reduce the size of the index.

If you have stop words (and for the Standard Analyzer, they come as part of the package), you cannot search for terms like "the" or "you". If you really need that, you need to configure another analyzer. You can setup the Standard Analyzer to avoid using stop words, by creating a custom analyzer and deploying it with RavenDB.

After removing the stop words from the index and the query, we are left with the results in Table 9.5.

|Post Id|Post title |
|-------|----------|
|posts/165090|Early lock release, transactions and errors |
|posts/164869|Transaction merging, locks, background threads and performance |
|posts/167362|The fallacy of distributed transactions |

: Query results without stop words.

That is much better, right? Except for a small tiny part. The most relevant result is last on the list! That is where scoring can help us.

#### Scoring query results

Usually when we make queries, we like to think in binaries term. Did this document match the query or not? And ordering the results is done by some other factor (like last update date, or the user's name, etc). 

For full text search queries, the situation is somewhat different. Instead of just being a binary true/false match for the query, the results have different level of relevancy. In the current example, we have the query "the transactional fallacy". And the data shown in Table 9.2.

Our analyzer turns that into a query on the "transaction" and "fallacy" terms, with the matches shown in Table 9.6.

				transaction 	fallacy
-----------		------------ 	-------
posts/164869 	Y				
posts/165090	Y 				
posts/167362	Y 				Y

: All matches for any term in the query, without stop words.

We got three results for this query, and now is the time to given them a score. A score is an indication of how close a match a document is to what the user has searched. An obvious way to score the results in this case would be to just give a value of 1 to any match.

What this means is that we would have the following scores:

Posts Id 		Score 		
-----------	----------- 
posts/167362	2 			 		
posts/164869 	1 			
posts/165090	1 			

Because posts/167362 is matches twice, once for "fallacy" and once for "transaction", it is scored higher, and will be the first item in the returned results.

This is roughly how document scoring works, if you are willing to look at this from high enough point of view. The details are more complex, of course. I'm going to just touch the very surface of the topic, and reference you to other works if you want all the gory details.

We can't just score documents by the number of terms that were found, because not all terms are equal. There are two important concepts that we need to deal with here. The term frequency–inverse document frequency, usually shorten to tf–idf.

The term frequency refers to the number of times a specific term appears in a document. While the inverse document frequency refers to the number of documents that contain a specific term. 

How do we use them during scoring? For each term, we consider how common that term is in the entire index (inverse document frequency). The more common a term is, the less important it becomes. If we create an index over documents discussing RavenDB, it is fairly certain that the term "ravendb" would be quite frequent. Because of that, matches on RavenDB aren't very good, because they are so common, they are not likely to be relevant.

We then consider how many times the term appears in the specific document we are scoring. A document that mention transaction once in passing is likely to be less relevant than a document that mention them multiple times.

So there is actually quite a lot that goes into scoring the results when using full text queries. Note that even with this description, we are ignoring a lot of other factors. The query might have specified that a certain match is very important and should be boosted higher than the others, or a document in the index was created with a note that if it is any kind of match, it should be a high one, etc.

And that is quite enough with theory, we know enough to be dangerous, so let us see what we can do with this with RavenDB.

## Practical full text search with RavenDB

We are now back in our familiar Northwind database, and we want to create our first real full text search index. For that, we'll perform such search on the products. Create a new index in the studio, named it "Products/Search" and use the following index definition:

	from p in docs.Products
	select new 	{ p.Name }

Then click on the Add > Field and setup the Name field as Analyzed. The end result should look like Figure 9.1.

![Full text index on products' names.](.\Ch09\Figure-01.png)

A really powerful tool to help you better understand indexing is the Terms viewer, which you can access by clicking on the Terms button. You can see some of the terms for this index in Figure 9.2.

![The generated terms for the products' names.](.\Ch09\Figure-02.png)

For now, let us focus on the term "anoton". Let us see how we got there. We can go to the index and query it using:

	Name: Anton

And that will find us the following two documents:

Id 			Name
---------	-----------
products/4 	Chef Anton's Cajun Seasoning
products/5 	Chef Anton's Gumbo Mix

Knowing what you know about how full text indexing works, you now know _why_ this happened. The Standard Analyzer broke the name into discrete terms, removing the possessive S and lower casing the term. When we query, the analyzer also lower cased our term, then match the queried term with the term in the index, thereby giving us those two results.

You can play around with different queries in the studio, to see how this behaves.

### Full text search from code

As fun as it always is to play with the studio, let us switch back to code and do the exact same steps as we have just done in the studio. First, we need to create the index, as shown in Listing 9.2.

```{caption="{Full text search index on Products}" .cs}
public class Product_Search : AbstractIndexCreationTask<Product>
{
    public Product_Search()
    {
        Map = products =>
            from product in products
            select new { product.Name };

        Index(x => x.Name, FieldIndexing.Analyzed);
    }
}
```

The only new thing here is the `Index()` method, which tell RavenDB that we need to index the Name filed as Analyzed. The options for `Index()` are:

* Default - just lower case the field as a single value.
* No - Don't index this field at all. This is sometimes use to just store a field in the index, but is very rarely used in practice.
* NotAnalyzed - index the field as is, without making any modifications. That is useful if you want to make a case _sensitive_ query.
* Analyzed - use an analyzer to index this field.

Unless specified otherwise, when Analyzed is selected, the StandardAnalyzer is used. You can control which analyzer will be used for a specific field using:

	Index(x => x.Name, FieldIndexing.Analyzed);
	Analyze(x => x.Name, "My.Lucene.Analysis.Sv.SwedishAnalyzer, My.Lucene.Analysis");

These two lines will tell RavenDB that you want the Name field analyzed, and that the analyzer to use will be the SwedishAnalyzer.

Note that this also apply during queries. So any query made on the Name field with this configuration will have the SwedishAnalyzer run on it before it is
## Complete search example

## Sorting

### Culture specific collation

## Facets

## Suggestions

expensive