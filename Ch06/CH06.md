
## Setting up a RavenDB Cluster

[Clustering Setup]: #clustering-setup

You might be familiar with the term "murder of crows" as a way to refer to a group for crows^[If you are interested in
learning why, I found [this answer](https://www.quora.com/Why-is-a-group-of-crows-called-a-murder) fascinating]. It has
been used in literature and arts many times. Of less reknown is the group term for ravens, which is "unkindness". 
Personally, in the name of all ravens, I'm torn between being insulted and amused. 

Professionally, setting up RavenDB as a cluster on a group of machines is a charming exercise (however, that term is 
actually reserved for finches) that bring a sense of exaltation (taken too, by larks) by how pain free this is. I'll
now end my voyage into the realm of ornithology's etymology and stop speaking in tongues.

On a more serious note, the fact that RavenDB clustering is easy to setup is quite important, because it means that it
is much more approachable. You don't need to have an expert at hand at all times to setup a cluster, and it should mostly
self manage itself. That means that the decision to go from a single server to a cluster is much easier and you can get
the benefit of that sooner.

### An overview of a RavenDB Cluster

A RavenDB cluster is three or more machines^[It doesn't make a lot of sense to have a cluster with just two nodes, since 
we'll require both of the nodes to always be up in most cases. There are certain advanced scenario where such topology 
might make sense, and we'll touch on that briefly in the [Clustering in the Deep End](#clustering-deep-dive) chapter.] 
that have been joined together. 

But what is the point of doing that? Well, you can create databases on a RavenDB cluster, and you can specify that the 
cluster should manage them on its own. A database can live on a single node, some number of the nodes or even all the 
nodes. Each node will then hold a complete copy of the database and will be able to serve all queries, operations and 
writes. 

The cluster will also distribute work among the various nodes automatically, handle failures and recovery and in general
act to make sure that everything is humming along merrily. 

Operations in RavenDB are usually divided to cluster wide operations (including cluster wide operations that impact only
a single database) and internal database operations. For example, creating a new database is a cluster wide operation, 
while writing a document to a database only impact that particular database.

The reason this distinction is important is that RavenDB actually operate two distinct layers in its distributed system. 
The first, at the cluster level, is composed of nodes working together to achieve the same goal. This is done by using
the [Raft consensus protocol](https://raft.github.io/) and having the cluster members vote to select a strong leader 
among themselves. 

This leader, in turn, is responsbile for such things as monitoring the cluster health, selecting the preferred node that
clients will use for each database, configuring the databases and making sure that there is a consistent way to make 
decisions at the cluster level. At the database level, however, instead of selecting a leader, the nodes are all working together, cooperatively and as equals. 

Why do we want that? Wouldn't it be better to have just a single mode of operation? The answer is that it probably would 
be simpler, but not necessarily better. Let us examine the pros and cons for each approach, and how they are used by 
RavenDB.

Cluster consensus with strong Leader (Raft algorithm) provide for strong consistency and ensure that as long as a 
majority of the nodes are functioning and can talk to one another we'll remain in operations. The strong consistency
mode is quite nice, since it means the cluster can make a decision (such as add a database to a node) and we can be 
sure that this decision will either be accepted by the entire cluster (eventually) or we fail to register that decision.
That means that each node can operate on its on internal state for most operations, resulting in a more robust system. 

> **The CAP theorem and database design**
> 
> The CAP theorem, also named Brewer's theorem states that given Consistency, Availability and Partition tolerance, a 
> system must choose two out of the three. It is not possible to provide all three options.
> 
> In practice, since all production systems are vulnerable to partitions, it means that you can select to be either CP
> (Consistent and Partition tolerant) or AP (Available and Partition tolerant). Different database systems have decided
> to take different design decisions to handle this issue. 
>
> RavenDB has opted to be CP and AP. That isn't quite as impossible as it sounds. It is just that it isn't trying to be
> CA and AP on the same layer. With RavenDB, the cluster layer is CP (it is always consistent, but may not be available
> in the precense of a partition) but the database layer is AP (it is always available, even if there is a partition, but
> it is eventually consistent).

However, if a majority of the nodes aren't available, we cannot proceed. This is pretty much par the course for consensus 
algorithms. Another issue with consensus algorithms is that they incur additional network roundtrips for each operation. 
For cluster maintenance and the configuration of databases, RavenDB uses the Raft consensus protocol. The RavenDB 
implementation or Raft is codenamed Rachis^[Rachis is the central shaft of pennaceous feathers.].

Databases, on the other hand, are treated quite differently. Each node in the cluster have a full copy of the topology, 
which specify which nodes host which databases. That information is managed by the Rachis, but each node is able to act
upon it indepdnently. 

The connection between the databases in different nodes do not go through Rachis or any other consensus protocol. Instead,
they are direct connections between the various nodes that hold a particular database, and they from a multi master mesh. 
A write to any of those nodes will be automatically replicated to all the other nodes.

This design result in a very robust system. First, you define your cluster, distribute your databases between the nodes,
assign work, etc. Once the cluster is setup, each database instance on the various nodes is independent. That means that
it can accept writes without consulting the other instances of the same database.

The design of RavenDB was heavily influenced by the [Dynamo paper](http://dl.acm.org/citation.cfm?id=1294281) and one of 
the key features was the notion that writes are _important_. If a user is writing to the database, you really
want to hold on to that data. In order to achieve that, RavenDB uses multi master replication inside a database, and is 
always able to accept writes.

In other words, even if the majority of the cluster is down, as long as a single node is available, we can still process 
read and writes.

This is a lot to digest, and at this point, it is probably all a bit theoretical for you. We'll get back to the different
layers that compose the RavenDB distributed architecture later on. For now, let us get ready to setup our first cluster.


### Your first RavenDB Cluster

We have worked with RavenDB before, but we have done that only with a single node. What you might not be aware of is that
we actually worked with a cluster. Admittedly, it is a cluster of a single node, and it isn't that interesting from the
point of view of distributed systems. However, even a single node instance is always running as a cluster. That has the 
advantage that the same codepaths are always being used and excersized. 

Another major benfit is that you don't need any special steps to get to a cluster, all we need to do is to add a few 
nodes. Let us see how it works in practice. Close any instances of the RavenDB server that you currently have running 
and open the command line termnial at the RavenDB folder.

> **Downloading RavenDB**
>
> In [Zero To Hero](#zero-to-ravendb) we have gone over how to download and seutp RavenDB for various environments. 
> If you skipped on that you can go to the RavenDB Download Page at 
> [https://ravendb.net/download](https://ravendb.net/download). And download the zip package for your platform. 
> The rest of this chapter assumes that you run all commands inside the unzipped RavenDB directory.

The first thing we want to do is to run an instance of RavenDB. Assuming you are running in `PowerShell` in the RavenDB
folder, please run the following command:

	$cmdLine =  "--ServerUrl=http://127.0.0.{0}:8080 " `
		+ 	" --Logs.Path=Logs/{0} " `
		+ 	" --DataDir=Data/{0}"
	start ./Server/Raven.Server.exe ( $cmdLine -f 1 )
		
The `$cmdLine` is the commnad line arguments that we'll pass to RavenDB, and we use this construct because it allows 
us to customize the arguments easily. In the string, we use `{0}` which are later replaced by the `1` later on using the
`$cmdLine -f 1` syntax. This is a somewhat convoluted way to ask RavenDB to listen to `127.0.0.1:8080`, write the logs to
`Logs/1` and write the data file to `Data/1`. We'll see exactly why we do it this wan in a bit. 

You can point your browser to `http://127.0.0.1:8080` and see the familiar studio. Let us create a new database and call
it "Spade". The result should look like Figure 6.1. 

![A simple database on a single node](./Ch06/img01.png)

Let us go into the database and create a couple of documents ("users/1" and "users/2" with your first and last name as 
the name). We are merely going about creating this database and those documents so we can see how we can grow a RavenDB
cluster. 

Now, we need to bring up another RavenDB node. Typically, you would do that on a separate machine, but to make the demo
easier, we'll just run it on the same node. Still in the same `PowerShell` session, now run the following command.

	start ./Server/Raven.Server.exe ( $cmdLine -f 2 )

This used the previously defined `$cmdLine` with a new format string and result in another RavenDB instance, this time 
bound to `127.0.0.2:8080` with logs at `Logs/2` and the data in `Data/2`. You can go to `http://127.0.0.2:8080` and 
see that you can also see the studio. However, unlike the instance running in `127.0.0.1:8080`, there are no databases
in the `127.0.0.2:8080`'s studio. 

Another difference between the two nodes can be seen at the bottom center, as you can see in Figure 6.2. 

![The node tag in two nodes](./Ch06/img02.png)

You can see in `127.0.0.2:8080` that the node is marked with a question mark. This is because this is a new node that 
didn't have any operations made on it. On the other hand, the `127.0.0.1:8080` node had a database created on it and 
as such is a single node cluster marked with the node tag `A`. 

Now, let us add a node to our cluster. Go to `127.0.0.1:8080` and then to `Manage server` and then to `Cluster`. The
cluster management screen is shown in Figure 6.3. 

![A single node cluster in the management screen](./Ch06/img03.png)

Click on `Add node` and enter `http://127.0.0.2:8080`. The new node will show up as a cluster member , as shown in Figure 6.4.

![Our cluster after adding the 2nd node](./Ch06/img04.png)

If you'll go the `127.0.0.2:8080` in the browser you'll several interesting things. First, at the bottom, you can see 
that we are no longer unknown node, instead, the `127.0.0.2:8080` server has been designated as node B.

> **Node tags and readability**
>
> To simplify matters, RavenDB assing each node in the cluster a tag. Those tags are used to identify the nodes. They
> do not replace the node URLs, but supplement them. Being able to say "Node B" instead of `127.0.0.2:8080` or even 
> WIN-U0RFOJCTPUU` is quite helpful, we found.
> It also help in more complex network environments where a node may be access via several hosts, IP and names. From
> now on I'm going to refer to node by their tags, instead of keep wrting 

You can also see that we have the "Spade" database, but it is marked as offline on Node B. This is because this database
is only setup to be held on Node A. Click on the "Manage group" button on the "Spare" database, which will take you to
a page allowing you to add a node to the database. Click on Node B and then click on "Add node to group". 

This is it, you now have a database that is spanning two nodes. You can go to Node B and see that the "Spade" database
status moved from `Offline` to `Online` and you can enter into the database and see the `users/1` and `users/2` documents
that we previously created on Node B. RavenDB has automatically moved them to the secondary node.

Play around for a bit with this setup. Creating and update documents on both nodes, see how they flow between the two 
database instances. Next, we are going to take our cluster and beat on it until it squeal, just to see how it handles 
failure conditions.

#### Kicking the tires of your RavenDB Cluster

Running on two nodes is an interesting exprience. On the one hand, we now have our data on two separate nodes, and assuming
that we have actually run them on two separate machines, we are safe from a single machine going in flame. However, two nodes
aren't actually enough to handle most failure cases.

Let us consider a situation when a node goes down. We can simulate that by killing the RavenDB process running Node A (the one
running the server on `127.0.0.1:8080`). Now, go to Node B (`127.0.0.2:8080`) and go to Manage server and then to Cluster. You'll
see both nodes there, but that none of them is marked as leader. This is because Node B is unable to communicate with Node A 
(this makes sense, we took Node A down). But there isn't anyone else in the cluster that it can talk to. 

The cluster at this point is unable to proceed. There is no leader to be found and no way to select one. The _database_, on the
other hand, can function just fine. Go to the "Spade" database on Node B and add a new document (`users/3`). 
You'll observe that there is no issue with this at all. The new document was created normally and without issue. This is because
each database is independent. Each database instance will cooperate with the other instances, but it isn't dependant on them.

> **How clients respond to node failures?**
>
> When a node fails, the clients will transparently will move to another node that hold the database that they are connected to.
> One of the tasks of the cluster's leader is to maintain the database topology, which clients will fetch as part of their 
> initialization (and then keep current). We'll talk about failover from the client side more later in this chapter.

Now, start the first node. You can do that using the following command:

	start ./Server/Raven.Server.exe ( $cmdLine -f 1 )

The node will start, reconnect into the cluster and if you'll check the cluster state, you'll see something similar to Figure 6.5.

![New leader selected after recovery of failed node](./Ch06/img05.png)

It is possible that in your case, the leader will be Node A. There are no guarantees about who the leader would be. Now you can 
check the "Spade" database inside Node A, and you should see the `users/3` document inside it that was replicated from Node B. 

So our database remained operational even while we had just a single node available. Our _cluster_ was down, however, since it 
couldn't elect a leader. But what does this mean? During the time that we only had Node B, we wouldn't be able to do any cluster
wide operations.

Those include creating a new database and monitoring the health of other nodes. But a lot of database specific configuration 
(anything that impact all instances of a database) goes through the cluster as well. For example, we wouldn't be able to schedule 
a backup with the cluster down^[Of course, we will be able to take a backup manually. See the discussion about cluster wide 
tasks later in this chapter] and you'll not be able to deploy new indexes or modify database configuration. 

> **Operations in failed cluster**
>
> The list of operations that you can't perform if the cluster as a whole is down (but isolated nodes are up) seems frightening.
> However, a closer look at those operations will show you that they are typically not run of the mill operations. Those are one
> time operations (creating indexes, setting a backup schedule, creating databases, adding nodes to cluster, etc).
> 
> Normal read/write/query operations from each database instance will proceed normally and your applications shouldn't fail
> over automatically and immediately. On the other hand, background operations, such as subscriptions or ETL will not be able
> to proceed until the cluster is back up (which require a majority of the nodes to be able to talk to one another).

Remember the previous mention that running with two nodes in a cluster is strange? This is because any single node going down will
push us down to this half & half state. Normally, you'll run three or more nodes, so let us exapnd our cluster even further.

#### Expanding your cluster

You can probably guess what we are going to do now. In exactly the same way that we previously added Node B, we are going to add
the three new nodes. Please execute the following commands:

	start ./Server/Raven.Server.exe ( $cmdLine -f 3 )
	start ./Server/Raven.Server.exe ( $cmdLine -f 4 )
	start ./Server/Raven.Server.exe ( $cmdLine -f 5 )

Then in the studio, go into "Manage server" and then into "Cluster" and add the new nodes (`http://127.0.0.3:8080`, 
`http://127.0.0.4:8080`, `http://127.0.0.5:8080`). The end result of this is shown in Figure 6.6.

![A five nodes cluster](./Ch06/img06.png)

You can click on the nodes in the cluster to open the studio on each node. If you'll do that, look at the tab headers, it will
tell you which node you are on, as you can see in Figure 6.7.

![An easy way to know which node you are on is to look at the tab icon](./Ch06/img07.png)

Now that we have our cluster, we still need to understand the layout of the databases in it. Here is what it looks like when we 
open the studio on Node D, as shown in Figure 6.8. 

![The Spade database as seen from Node D](./Ch06/img08.png)

The "Spade" database is marked as offline because it doesn't reside on this node. This lead to a few important discoveries, 
databases that we manually configured are going to remain on the nodes that they have been configured to run on. It also appears
that we can see the entire cluster topology from every node. 

Now, let us actually use our cluster and create a new database, called "Pitchfork"^[I'm on a garden tools naming streak, it 
appears]. Everyone knows that a proper pitchfork has three tines, the four tines pitchfork is used for shoveling manure while 
a novel tree tine pitchfork is the favorite tool of Poseidon. As such, it is only natural that our "Pitchfork" database will 
have a replication factor of 3. Once that is done, just create the database and observe the results.

Since we didn't explictly specified where the new database is going to reside, the cluster will distribute it to three nodes in 
the cluster. This means that we now have a 5 nodes cluster with two databases, as you can see in Figure 6.9.

![A spade and a pitchfork in our RavenDB cluster](./Ch06/img09.png)

Figure 6.9 shows the studio from the studio in Node D. So we have Pitchfork on three nodes and Spade on two. You can go ahead and
create a few documents on the Pitchfork database and observe how they are spread to the rest of the database instances in the 
cluster.

#### Appropriate utilization of your cluster

Setting up a five nodes cluster just to run a couple of database seems pretty wasteful, Node E doesn't even have a 
single database to take care of. Why would we do something like this? Typically, production clusters are setup with either three 
or five nodes. When a cluster size exceed five nodes, it'll typically have dozens of servers running in tandem, we'll discuss large 
clusters later in this chapter. If you have one or two databases, you'll typically deploy a three node cluster and make 
sure that the database(s) on it are spread across all the nodes in the cluster.

Sometimes you'll have a five node cluster with the data replicated five times between the nodes. For maximum survivability. 
But in most cases, when you go to five nodes or higher, you are running a number of databases on the cluster. For instance, 
consider the situation in Figure 6.10.

![A cluster hosting a whole garden shed of databases](./Ch06/img10.png)

In Figure 6.10 you can see that we created eight databases and that they are all spread out throughout the cluster. This means
that there is no single point of failure for the cluster. In fact, we can lose any node in the cluster and still remain in full
operations.

I intentionally defined some of the databases so they'll only run on two nodes, instead of three. In this configuration, it is 
possible to lose access to a few databases if we kill Node A and Node B. A proper configuration will have each database reside
on three nodes, so you'll have to lose more than half of your servers to lose access to a database.

> **Typical deployment in a cluster**
> 
> The normal approach is to decide how important your data is. It it common to require the data to reside on three separate 
> nodes, regardless of the cluster size. In more critical systems, you'll either have this spread out across multiple data
> centers or have a copy (or copies) of the database being maintained by the cluster in a second datacenter by RavenDB, we'll
> discuss this feature in the [Sharing data and making friends with ETL](#integrations) chapter when we'll discuss External
> Replication.

A very important note is that so far we are running completely opened to the world. This is possible because we are listening on
the loopback device, so no external actor can get to those RavenDB servers. This is obviously isn't how you'll run in production
and you oaught to read the [Securing your Ravens](#security) chapter on proper deployment and security of your cluster before you
expose a cluster to the wild wild web.

#### The role of the database group

We looked at what a cluster looks like, and how we can distribute databases among the different nodes in the cluster. This is all 
well and good, but we still need to understand what are we _doing_ with all this data as it is spread out among those nodes. First
and foremost, we need to define some shared terminology.

A database can refer to all the individual instances of the database in the cluster, to a specific instance or just to the abstact
concept of storing your system data in a database. Because this is confusing we'll use the following terms to make it clear to
what we refer.

* Database instance - exists on a single node, usually part of a larger database group. Typically refered to as "the database instance
  on Node A". 
* Database group - the grouping of all the different instances, typically used to explicitly refer to its distributed nature. "The 
  'Spade' database group is spread over five servers for maximum availability". 
* Database topology - the specific nodes that all the database instances in a database group reside on in a particular point in time. 
  "The 'Spade' topology is [B, A]".
* Database - the named database we are talking about, regardless of whatever we are talking about a specific instance or the whole group.
  "We use the 'Spade' database to store that information".

Now that we have a shared vocabulary, let us see if I can explain exactly what is going on. A database group is composed of one or more
database instances, each of who holds a full copy of the data in the database. The replication factor on the database group will determine
how many copies we hold for that database. 

The primary reason for this duplication of data is to allow us high availability. If a node goes down, we still have a copy (and usually
two) of the data around and the cluster can shuffle things around so clients will talk to another node without really noticing that 
anything happened. 

### Client interaction with a database group

We'll spend some time exploring the interaction between clients and the database instances, because that is usually what you would mostly
be concerned about. We'll describe the behavior of the nodes a bit later. A lot of this topic is intermingled, with both client and server
cooperating to get to the best result. We'll start with our explotation with the code in Listing 6.1.

```{caption="Creating a document store to talk to a cluster" .cs }
var store = new DocumentStore
{
    Urls = 
    { 
    	"http://128.0.01:8080","http://128.0.02:8080",
    	"http://128.0.03:8080","http://128.0.03:8080",
    	"http://128.0.05:8080" 
    },
    Database = "Spade"
};

store.Initialize();
```

The code in Listing 6.1 is listing all the nodes in our cluster, and will talk with the "Spade" database. This is interesting, because 
the "Spade" database only exists on nodes B and A (you can see the database topology in Figure 6.10). Why are we listing all the nodes
in this manner. 

As it turns out, we don't actually need to do so, it is enough that we'll list any node in the cluster for us to be able to properly 
connect to the "Spade" database. Each node in the cluster contains the full topology of all the databases hosted in the cluster. And the
very first thing that a client will do upon initialization is to query the `Urls` is has defined and figure out what are the actaul nodes
that it needs to get to the "Spade" database. 

> **Why list all the nodes in the cluster, if any will do?**
>
> By listing all the nodes in the cluster, we can ensure that if a single node is down and we bring a new client up, we'll still be
> able to get the initial topology. If small cluster sizes (3 - 5) you'll typically list all the nodes in the cluster, but for larger
> clusters, you'll typically just list enough nodes that having them all go down at once will mean that you have more pressing concerns
> then a new client coming up.
>
> For extra reliability, the client will also cache the topology on disk, so even if you listed a single node and it is down and the 
> client was restated, we'll still remember where to look for our database. It is only a completely new client that need to have the 
> full listing. But it is a good practice to list at least a few nodes, just in case.

Once the client got the database topology, it will use that to talk to the actual database instances themselves. We talked about the 
different layers of the RavenDB distributed machinery earlier in this chapter. We are now moving from the cluster layer to the database
layer. And each database instance is now working on its own, without relying on its siblings in the group.

That has a lot of implications on how RavenDB works. On the client side, if the client is uanble to talk to a node (TCP error, HTTP 503, 
timeouts, etc) it will assume that this particular node is down and will switch to the next node in the list. All the clients get their
topology from the cluster, and the cluster ensure that we'll always report the same topology to the clients^[Of course, it is possible 
that a node has an _outdated view_ of the topology, but there are mechanisms in place to ensure that clients will figure out that their 
topology it out of date and refresh it.]. 

By default, all the clients will talk to the first node in the topology. We typically call this the preferred node, and any of the other
nodes in the topology the alternates. A failure of any of the alternate wouldn't even register for the typical client configuration, since
the client will only talk directly with the preferred node. 

A failure of the preferred node will mean that all clients will failover to the same alternate, the cluster will also notice that the node 
is down and update the topology accordingly. The clients will get the updated topology, which would now have the first alternate as the 
prefreed node in the topology. The failed node would be demoted to a standby mode, since the cluster doesn't know what state it is in. 

Once it come back up, the cluster will wait for it to catch up and then add it back to the bottom pool of active nodes for this database. 
Because the failed node is added as the last option in the pool, it won't be usurping the role of the preferred node. This ensures that if
the failed node will experience further failures, we won't have the cluster having to cycle the preferred node each and every time.

> **The simplest failure mode**
>
> While it may seem that an alternate failing (the client isn't even going to notice) or the preferred node failing (cluster will demote,
> clients will automatically switch to the first alternate) is all that we need to worry about, those are just the simplest and most 
> obvious failure mode that you need to handle in a distributed environment.
>
> More interesting cases include a node that was split off from the rest of the cluster, along with some (but not all) of the clients. In 
> that case, different clients have a very different view about who they can talk to. That is why each client is able to failover 
> independently of the cluster. By having the database topology, they know about all the database instances and will try each in turn
> until they are able to find an available server that can respond to them. 
>
> This is completely transparent to your code, and an error will be raised only if we can't reach _any_ of the database instances. 
> While this ensure that we'll always have _someone_ to talk to, it can cause some interesting behavior for our system. We'll discuss
> this later in this chapter, in the section about Conflicts.

From the point of the view of the client, there isn't really much to be said about the cluster. As far as your code is concenred, you
operate normally, and RavenDB will take care of everything under the covers. However, there are still a few things that you need to
concern yourself with.

#### Write assurances for high value data

The way a database group work, whenever a write is made to any of the database instances, it will disseminate that write to all the 
other instances in the group. This happens in the background and is contiously running. Typically you don't need to think about it,
you write the data to RavenDB and it shows up in all the nodes on its own.

You do need to think about it if you have some writes that are very important. It isn't enough to ensure that you wrote that value 
to a single node (and made sure that it hit the disk), you need to be sure that this value reside in more than one machine. You can
do that using write assurance, which is available using the `WaitForReplicationAfterSaveChanges` method. You can see an example of 
that in Listing 6.2.


```{caption="Saving a very important task to RavenDB, ensuring it resides in multiple nodes" .cs }
using (var session = store.OpenSession())
{
    var task = new ToDoTask
    {
        DueDate = DateTime.Today.AddDays(1),
        Task = "Buy milk"
    };
    session.Store(task);
    session.Advanced
    	.WaitForReplicationAfterSaveChanges(replicas: 1);
    session.SaveChanges();
}
```

The code in Listing 6.2 is very familiar, there isn't much to change when you move from a single node to a cluster. But here we are
asking the database instance that we wrote to that it will not confirm the write until we have replicated the data we wrote to at 
least another replica. 

This increase the time it takes to get a reply from RavenDB, sometimes significantly so. You are now not only paying for network roundtrip
to the server and then writing the data to disk but also another network round trip and disk write cost per each additional replica.
Using this friviously will likely slow your application some, and introduce problems when you don't have enough replicas. 
While RavenDB will happily deal with going down to a single node, and your application can use that, if you use 
`WaitForReplicationAfterSaveChanges`, an error will be raised in such cases. 

An important aspect of this feature to remember is that when `WaitForReplicationAfterSaveChanges` is used, it doesn't involve a distributed
transaction, in other words, even if we haven't been able to write the value you just wrote to the number of replicas you wanted, we still
wrote it to _some_ of them. In the case of Listing 6.2 and the "Spade" database, if Node A is down, we will be able to write to Node B, but
we'll later fail because we can't replicate the write to Node A. The client is going to get an error, but the data _was written to
Node B_. This is a powerful feature, but you need to be aware of the possible pitfalls of using it. 

#### Load balancing and service level agreements

Earlier in this chapter I said that by default we have a single preferred node and a few alternates just standing by. This can be a big
waste. Our nodes are typically quite similar and just doubling or tripling our processing power just to let most of it go idle is not 
a good use of resources. 

RavenDB also allows you to change that behavior. Instead of all the clients talking to just the perferred node, you can ask them to load
balance all the reads between all the nodes in the group. It is even possible to take timing into account, and have each node prefer to
read from the fastest node that it observed.

//TODO: This is not implemented properly now


### Replication of data in a database group

We have seen that all the nodes in a database group will have a copy of all the data. This is easiest to reason about when you have a 
single writer node. However, RavenDB doesn't work in this manner. You can make writes to any node in the database group, and that write
will be recorded and replciated to all the other nodes in the database group.

How is this done? Each database instance holds a TCP connection to each of the database instances in the group. Whenever there is a write
on that instance, it will be sent to all the other instances immediately. Note that is not done as part of the write operation, but in an
async manner. If the database instance is unable to replicate the data, it will still accept it and send it later.

During normal operations, the lag time between a write on a database instance and it being replicated to all the other nodes is usually 
around twice the ping time between the machines, since we don't need to setup a connection just for that write, it is already ready and
waiting. This also means that you can see at any point the connections that a database instance has opened, which is useful when 
diagnoising issues.

//TODO: Show an image of the network graph

What about when there are failures? When a node fails, for any reason, the cluster will detect that and demote it, when it comes back up, 
the database instance will have to wait until it is up to date as the other database instances in the group, and only then will the 
cluster let it join fully into the group.

// TODO: Show an image of how this looks like.

We typically talk about document replication, but we are actually replicating more then just documents. All the data inside a database is
replicated to all the database isntances. That obviously includes documents, but it also includes tombstones (which is how we replicate
deletes), attachments and revisions. The last one deserve a bit of explanataion.

The async nature of replication in RavenDB lead to some interesting questions. For example, what happens if you modify a document 
several times in short order? Fast enough that it was modified several times before we were able to send the document to the other database
instances. In this case, when we replicate the document to the other siblings, we'll replicate the latest version that we have, and skip
replicating all the intermediate versions. 

Sometimes, however, you care about each individual change. For that reason, when you enable revisions on your database, we'll also send 
them to the other database instances. In this way, you can see the full modification history of your documents, regardless of which server
was used to write them and the speed of replciation.

> **Undefined order in a distributed environment**
>
> All the database instances are going to end up with the same revisions for the document, it is not certain that they will
> end up in the same *order*. It is possible that revisions created on different servers will show up in different 
> order on different servers, because there is no way for us to determine which came first. Consider it a scaled up version of
> a race condition. You'll learn more about how RavenDB is handling such manners in the section about Conflicts and Change
> Vectors.

Another question that is typically raised with regards to the async nature of replication is how are transactions handled across 
replciation boundaries. This it the topic of our next section.

#### Transaction atomicity and replication

Transactions in RavenDB are _important_. Relying on ACID transactions reduce a lost of the complexity that one has to deal with. However,
given that RavenDB transactions are _not_ distributed, it is interesting to consider how the transactional guarantees are preserved in a 
distributed cluster.

Consider for a moment the code in Listing 6.3, which is a simple bank transaction, moving 10$ from my account to yours^[This isn't even
close to how money transfers really work, of course, but it is a classic example and easy to reason about.], which I'm sure will be a 
delightful surprise.

```{caption="The classic bank transaction example" .cs }
using (var session = store.OpenSession())
{
  	var you = session.Load<Account>("accounts/1234-A");
  	var me = session.Load<Account>("accounts/4321-C");

  	you.Amount += 10;
  	me.Amount -= 10;

    session.SaveChanges();
}
```

There is nothing really interesting in Listing 6.3, to be fair. The interesting bit is what will happen over the network in this case? 
This is sent to RavenDB as a trasnaction, and persisted as such, but you already knew that. The key here is that it is also _replciated_
as a single transaction. In other words, all the other database instances will always have both of those changes replicated to them as 
a single batch, indivisible.

> **Replication batches**
>
> RavenDB isn't actually replicating a single document at a time anyway, whenever we need to replicate data between nodes we send a 
> whole bunch of data at once. The size of the batch and what is included in it is decided by RavenDB based on things like the size 
> of the data we need to send, the number of documents and a few other factors that are implementation dependant (such as the speed
> of replication, etc).
>
> In most cases, we send everything changed since the last successful replication. And during normal operation, we effectively send
> a replication batch for each transaction. If there are a lot of writes, each batch will likely include data from multiple transactions.
> 
> What RavenDB guarantees is that writes made in the same transactions will always be sent to the other database instances as a single
> batch and won't be broken into separate batches. In other words, we maintain the atomicity property across replication.

This applies to all the changes that you make in a transaction, modifying documents, attachments, etc. They are all going to be sent to 
the other database instances as a single transaction. There is one caveat, however. Consider the code in Listing 6.4, which transfer some
money from your account to the tax authority. 


```{caption="The classic bank transaction example" .cs }
using (var session = store.OpenSession())
{
  	var you = session.Load<Account>("accounts/1234-A");
  	var tax = session.Load<Account>("accounts/48879-B");

  	you.Amount -= 3;
  	tax.Amount += 3;

    session.SaveChanges();
}
```

The code in Listing 6.4 will probably sadden you a bit, since no one usually like to pay the taxman. Hoever, it also exposes an issue with
how RavenDB is replicating transactions. In this case, the document `accounts/1234-A` is involved in two separate transactions.
Let us assume that we now need to replicate everything from Listing 6.3 and onward (so my transfer of money to you and your payment of 
taxes). 

We'll further assume that we instructed RavenDB to send the data in as small a unit as possible.  When the database instance needs to 
replicate the documents to another instance, it will replicate them on a per transaction basis. Starting from the transaction that was
generated for Listing 6.3, we'll replicate `accounts/4321-C` but _not_ `accounts/1234-A`, why is that? 

The `accounts/1234-A` document will not be replicated because it was changed in the transaction for Listing 6.3, but it was also changed
by a later transaction (the one from Listing 6.4). Since we replicate just the current state of the documents, and in this case, we 
configured RavenDB to send the smallest possible batch size, we will replicate just the documents that were modified in Listing 6.3, but
not documents that were modified in the transaction for Listing 6.3 and then later modified by another transaction.

This means that we'll first have a batch replicated `accounts/4321-C` document (my transfer of the money) and then another batch with
`accounts/1234-A` and `accounts/48879-B` document (your payment to the taxman). A client that will read from another instance may then
get an invalid state.

Luckily, everything is not lost. If you need transaction atomicity in a distributed fashion, you can either make sure that the documents
you care about are always updated together, this will ensure that they are always part of the same transaction and sent in the same batch.
This is possible, but often it is awkward to do. Instead, you can use revisions.

When you enable revisions on a collection, whenever a docuemnt is modified, a revision is written. And it is written as part of the same
transaction and it flows across replication in the same indivisible batch as any other operation. When RavenDB gets a revision of a 
docuemnt that is newer then the document it has, it will update the docuemnt to match the revision. In effect, we'll send the entire 
transaction in Listing 6.3 as it was, so a client observing a second node can never see a partial transaction.

As you can see, revisions are a very powerful feature, and they are used in far more scenario then might be initially expected. The idea 
with relying on revisions to handle transaction consistency is that in many cases, it doesn't matter. A proper document model follows the
Isolated, Independent and Coherent tenants, which usually mean that you don't _need_ this feature. But when you do, and it most certainly
come up quite often^[It is not something that you'll use for everyting, but in most applications, there is one or two places where it can
be _really_ useful.]. The key here is that you aren't paying for tracking all the intermediate values unless you actually need this. 

A lot of RavenDB features use the "Pay to Play" model, where if you aren't using a specific feature, you don't need to pay the performance 
cost of supporting it.

#### Change vectors

In the previous section, I mentioned that when a database instance gets a document revision that is newer than its copy of the document it
will update the document to match the revision. This form the basis of ensuring transaction boundary consistency across servers. But that 
doesn't answer an important question. What does it mean, newer? 

The easiest way to answer this is to take a look at the clock. After all, newer means time, and we can just check at what time the revision
and the document were modified. That doesn't actually work, however, when running in a distributed sytem. Each machine in the cluster may
slightly different idea about the time, and clock drift is a serious problem. Beyond that, concurrency in the cluster may mean that 
operations have happened in the exact same time or close enough that we can't tell otherwise. 

This is confusing, and can cause quite a headache in a distributed system. In order to deal with this, RavenDB uses change vectors (they
are sometimes also called Vector Clocks) to tell when things hapened. A change vector comprise of the node id and a sequence number (the 
etag). Whenever a modification happens on a database, a change vector will be generated. Here are a few examples of change vectors:

* A:1
* B:2
* A:3, B:1
* A:3, B:3, C:5

For the purpose of discussion, `A`/`B`/`C` are the node ids and the numbers are the per node sequence number. You can see that with `A:1`
and `B:2`, there is no way for us to provide any order. This is because they are local to each node, and there is no dependency between 
them. 
On the other hand, the `A:3, B:1` change vector provide us with more infromation. We know that it came after `A:3` and after `B:1`. Because
of this we can tell that it is after `A:1`, but we can't tell whatever it happened before or after `B:2`. On the other hand, it is clear
that `A:3, B:3, C:5` happened after everything else.
In other words, we use change vector to record the observed state in each node, and we can piece out the timeline from that point onward. 

The different nodes in a database group will try to ensure that each database instance is aware of the last etag on each node, so they can
create a coherent timeline across the cluster. This isn't meant to be a 100% solution, but it is a useful optimization for certain operations.
Always having the last etag from the nodes also make it easier to reason about concurrency in the cluster. You can see for yourself that when
you save a document, it will have a change vector entry from the node you saved it on and also for all the other nodes in the cluster.

Change vectors and how they come into play are pretty hard to grasp just by talking about them, so let us do some practice. We'll create a
new database, called "ChangeVectorTrailRun" on 2 of the nodes in the cluster. Then go into the database and create a document called `users/1`.

Change it a few times and observe the `@change-vector` property in the `@metadata`. Even though you have only modified this document on a single
node, you can see that the database instance also include the change vector from the other instance as well. You can see this example in Figure
6.11

![A document change vector in a cluster](./Ch06/img11.png)

Using this change vector, this document was written at or after etag 7 on node `A` and at or after etag 5 on node `B`. We created this document 
on node `A`, so how can we have a change vector entry for node `B` here? The answer is simple, we know
what the etag of node `B` was because it told it that on our last replication batch and we incorporate this information in future change vectors.

Change vectors are incredibly important inside RavenDB (and in many other distributed systems) because they allow us to know, without relying
on a shared clock, when things have happened. This turns out to be quite useful in many cases. For example, let us assume that a database 
instance recieve a document via replication, but it already have this document stored.

How would the database instance know if the document that arrived via replication is an old version of the document, that we should ignore, or
if this is a new version, that need to overwrite the existing document? It uses the change vector to tell that. In that way, even if a document
if modified on different server each time, we can still now whatever our version is newer or older then the version that we just got via 
replication.

However, newer or older aren't the only options that we have. There is also the option of a conflict update. 

#### Conflicts

A conflict occurs when the same document is updated on two nodes independently from one another. This can happen because of a network split or
several client updates that each talked to a different node faster then we can replicate the information. Let us consider the simplest case, we
have the document `users/1` whose current change vector is `A:7, B:5` and we modify it on nodes A and B concurrently. 

On the database instance on node `B`, we'll have the change vector `A:8, B:5` and on node B we'll have `A:3, B:8`. In this case, there is no way
for us to tell which version of the document is newer then the other. This is a conflict. In a distributed system, you can either choose to run a 
consensus (which require consulting a majority on every decision) or accept the potential for conflicts. For documents writes, RavenDB chose to 
accept conflicts as a tradeoff of always being able to accept writes.

The question is, how are we going to handle such a case? Let us generate a conflict manually and see how this is handled. Here is how we'll do it:

* Kill node `B` (it is running on `127.0.0.2`). You can do that by simply closing the console window that is running it.
* Modify the `users/1` document on node `A` so its name property will be: "John" and save the document. You can see how this will look like in 
  Figure 6.12.
* Kill node `A` (it is running on `127.0.0.1`). 
* Bring up node `B` by executing `start ./Server/Raven.Server.exe ( $cmdLine -f 2 )`.
* Modify the `users/1` document on node `B` so its name property will be: "Doe" and save the document. You can see how this will look like in
  Figure 6.13.
* Bring up node `A` by executing `start ./Server/Raven.Server.exe ( $cmdLine -f 1 )`.

![The `users/1` document after modifications on node `B`](./Ch06/img12.png)

![The `users/1` document after modifications on node `A`](./Ch06/img13.png)

At this point an interesting thing will take place. If you'll watch the `Users` collection, you'll notice that it is _empty_, and that the 
`Conflicts` entry in the Studio is showing that there is a conflict. Go to `Documents` in the studio and then to `Conflicts` and you'll see the
missing document. 

In fact, you'll see _three_ versions of this document, as you can see in Figure 6.14. For developers, this screen show be very familiar, it is
showing a conflict just as you would get in any source control system. You can also resolve it in just the same manner, by deciding what changes
to accept and saving. Change the `Name` property to be "John Doe" and click on `Save`.

![Manual conflict resolution in the Studio](./Ch06/img14.png)

The conflict is now resolved and you can see the document is back in its proper place, the `Users` collection.
As you could see, getting to the point where we could reliably generate a conflict was a rather involved process, they aren't something that is
expected to happen frequently. At the same time, production systems have a nasty habit of throwing up a _lot_ of hurdles even during normal 
operations. Conflicts can and do occur in production systems, especially busy ones.

It wouldn't be reasonable to expect a manual intervention each and every time that you need to resolve a conflict. Instead, RavenDB has several
conflict resolution strategies that you can use:

* If the documents are identical (same change applied on multiple nodes), they can automatically be merged. This also apply if the document was
  deleted in multiple nodes and we detected a conflict.
* You can specify your own conflict resolution behavior by writing JavaScript function that would apply to all the conflicts and prdouce a merged
  output. We'll see an example of that shortly.
* You can designate a node whose changes will be considered authoritative and all conflicts will be resolved in its favor.
* Finally, you can chose to resolve to the latest version based on wall clock time.

The appropriate strategy to use heavily depends on your actual use case. In some cases, just accepting the latest version is perfectly fine. In 
others, you don't want to loss writes just because they conflicted. Probably the most well known scenario for such a requirement is the notion
of the shopping cart.

A user's shopping cart document may have gotten into a conflict, but we absolutely don't want the user to have stuff fall off their cart just 
because we had a network hiccup. Consider the documents shown in Listing 6.5

```{caption="Two conflicting versions of a shopping cart" .json }
// on node A
{
    "Name": "Oren Eini",
    "Items": [
        {
            "ProductId": "products/2134-B",
            "Quantity": 2,
            "Name": "Milk"
        },
        {
            "ProductId": "products/9231-C",
            "Quantity": 1,
            "Name": "Bread"
        }
    ]
}
// on node B
{
    "Name": "Oren Eini",
    "Items": [
        {
            "ProductId": "products/2134-B",
            "Quantity": 1,
            "Name": "Milk"
        },
        {
            "ProductId": "products/8412-B",
            "Quantity": 2,
            "Name": "Eggs"
        }
    ]
}
```

RavenDB, of course, have no idea how to merge such a conflict. But we can teach it. TODO - figure out how this is exposed in the studio.

Listing 6.6 has the code to properly resolve such a conflict. 

```{caption="Script to merge conflicting shopping carts" .js }
var final = docs[0];

for(var i = 1; i < docs.length; i++)
{
	var currentCart = docs[i];
	for(var j = 0; j < currentCart.Items.length; j++)
	{
		var item = currentCart.Items[j];
		var match = final.Items.find(function(i){
			return i.ProductId == item.ProductId;
		});
		if(!match)
		{
			// not in cart, add
			final.Items.push(item);
		}
		else
		{
			match.Quantity = Math.max(
				item.Quantity,
				match.Quantity);
		}
	}
}

return final;
```

The code in Listing 6.6 will merge the two different versions of the shopping cart, resulting in the document shown in Listing 6.7.
With a merge script such as the one in Listing 6.6, RavenDB can automatically resolve conflicts as they happen, with no interruption in
service. The cart in Listing 6.7 has been intelligently merged, so we don't have any duplicate products and we kept the higher number of milk 
bottles between the versions of the cart that we merged. 

```{caption="The merged shopping cart" .js }
{
  "Name": "Oren Eini",
  "Items": [
    {
      "ProductId": "products/2134-B",
      "Quantity": 2,
      "Name": "Milk"
    },
    {
      "ProductId": "products/9231-C",
      "Quantity": 1,
      "Name": "Bread"
    },
    {
      "ProductId": "products/8412-B",
      "Quantity": 2,
      "Name": "Eggs"
    }
  ]
}
```

Not all collections deserve such attention, of course. For some you'll choose to just get the latest version or define an authoritative source
that will resolve such conflicts. When you need to write such a script, the studio can be a great help here, because in addition to just defining
the conflict resolution script, you can also debug it inside the studio. 

TODO: Need to show some screenshots for configuring and debugging

### Architectual considerations in cluster setup

The separation RavenDB makes between the cluster and the database group can be artifical. If you are running a single database on all your nodes, you usually
will not make any distinction between the cluster as a whole and the database group. This distinction starts to become a lot more important if you are working
in a system that utilize many databases.

The simplest example for such a system is a micro service architecture. Each micro service in your system have its own database group that is running on the 
cluster, and you can define the number of nodes each database group will run on. This tend to be easier to manage, deploy and work with then having a separate
cluster per micro service. 

Other examples where you'll have multiple databases is for multi tenancy, where each tenant gets their own separate database. This make it very easy to deal
with tenant separation and you can adjust the number of nodes per tenant easily. This approach will also allow you to scale your system easily. As you have 
more tenants, you can just add more machines to the cluster and spread the load among them. 

That said, note that there is a certain cost for running each database instance, and that it is usually easier for RavenDB to have a single large database then
_many_ small ones. The general rule of thumb is that you shouldn't host more than a hundred or so active databases per machine.

#### Growing your cluster

RavenDB is using Raft as the underlying consensus protocol for managing the cluster. The [Raft Paper](https://raft.github.io/raft.pdf) is a truly impressive 
reading, mostly because the paper manage to make one of the hardest tasks in distributed programming _understandable_. I highly recommend reading it even if
you never intend to dig into the details of how RavenDB or other distributed system does their magic. 

The simplest way to explain how it works is that the cluster make decisions based on majority confirmation. That does great injustice to both the algorithm and
the paper, but it simplify things and allow us to reason about them without deviating _too_ much from what is really going on. Majority confirmation is defined
as having a particular value on `N/2+1` of the nodes, using integer math and assuming that `N` is the number of nodes. In other words, if your cluster size is
3, then a majority would be 2 and any value that was confirmed by any two nodes is considerred committed.

Table 6.1 shows the majority for several common cluster size. Note that even numbers have the same majority as the odd number preceding them. Because of that,
you'll typically have an odd number of nodes in your cluster. 

| Cluster size | Majority |
|--------------|----------|
|            2 |        2 |
|            3 |        2 |
|            4 |        3 |
|            5 |        3 |
|            7 |        4 |
|            9 |        5 |
|           15 |        8 |
|           21 |       11 |
|           51 |       26 |

Table: Majories for different sized cluster

This majority behavior has a few very interesting implications that you need to consider. First, and quite obvious, if we have a failure condition that 
took out more than half of our cluster, the cluster as a whole will not be able to make any decisions (even while individial database instances will operate
normally). In a cluster of 5 nodes, if there aren't any 3 nodes that can communicate with each other, there is no way to reach any decision. 

On the other hand, the more nodes there are in your cluster, the more network traffic you need to make to reach any decision. In patalogical cases, such as a 
cluster size of 51, you'll need to contact at least 26 servers to reach any decision. That is going to impose a high latency requirement on anything that 
the cluster is doing. 

In practice, you very rarely grow the cluster beyond seven members or so. The cost of doing that is usually too high. At that point, you will either setup 
multiple independent clusters or use a different method. The cluster size we mentioned so far is for voting members in the cluster, but the cluster doesn't
have to contain only voting members. We can just add nodes to the cluster as watchers.

These nodes do not take part in the majority calculations and are only there to watch what is going on in the cluster. As far as RavenDB is concerned, they are
full blown members in the cluster, they can be assigned databases and work to be done, but we aren't going to include them in the hot path of making decisions 
in the cluster.

Using this approach, you can decide that five nodes in your cluster are the voting members and all the rest are just watchers. This gives us the ability to 
make decisions with a majority of only three nodes while the actual size of the cluster can be much higher. Of course, if three of the the voting members of
the cluster are unable to talk to one another (because they are down, the network failed, etc) then the cluster as a whole will not be available. 

Given that a cluster being unavailable doesn't usually impact ongoing database operations, the actual topology at scale is something that the operations team
need to consider. A single large cluster is usually easier to manage and the cluster can add as many watchers as you need to handle the load you are going to 
put on it. The databases themselves do not care what node they run on, whetever it is a voting member or just a watcher. And the cluster can manage database 
instance assignment across all machines with ease. 

#### Sharing data between clusters

A RavenDB cluster is a standalone unit, it manages itself and doesn't concern itself much with the outside world. There are situation, however, where you want to
have data shared between multiple clusters. This can happen if you want an offsite replica of all your data or if you have decided to have different clusters in 
each data center rather than a single cross data center cluster.

The offsite replica scenario is probably the easiest to grasp, so we'll start with it. In addition to the nodes you have in your cluster, you also want to have a 
full replica of a database that is outside the cluster and typically in a secondary data center. This can be done because you want the ability to spin up the 
system in another data center or because you want to setup a replica to do certain tasks (analytics, big queries, etc).

> **Replica isn't a backup**
>
> It is tempting to think that an offsite replica is also going to serve as the backup and not pay careful attention to the backup / restore portion of your
> database strategy. That would be a mistake. RavenDB's External Replication suppoert means that you get an offsite replica, but it doesn't provide good
> answers to a lot of backup scenarios. For example, protecting you from a "delete this collection" or "what was the state of the system in 9:03 AM last Friday?"
>
> An offsite replica gives you an offsite live copy of the data, which is very useful if you need to shift operations to a secondary data center, but it isn't
> going to allow you to skimp on backups. We cover backups (and restoring databases) in [Disaster Recovery and Plan B (for Backups)](#disaster-recovery).

This can be done quite easily, because RavenDB make a strong distinction between cluster and database group interactions. In this case, this allows us to define
a replication target that is not part of the cluster itself. We can do that in the database by going to `Tasks` and then `Manage Ongoing Tasks` and adding an 
`External Replication` task. You'll need to provide the url and database name for the destination, then save. Assuming you don't have an additional cluster to
test this on, you can specify one of your own nodes and create a _separate_ database to replicate to.

Ongoing tasks in general is quite interesting, and we'll discuss that at length in the next section. For now, we'll focus on what the External Replication feature
and what it means for us. Once we have finished configuring it, the cluster will assign on of the database group nodes to keep that replica up to date at all 
times.

It is important to remember that at the database level we treat it as just another destination, the cluster is _not_ managing it. That means that cluster 
level behaviors, such as defining conflict resolvers, failover for the client or index replication are _not_ send over. An External Replica is just that, 
external. You can configure both the source cluster and the destiation replica the same, of course, but there is nothing that forces you to do so. In fact, 
the other common reason why you will want to setup an External Replica is to have _different_ configuration.

A good example of this is when you want to have expensive indexes and only run them on a particular machine. Maybe you need to run expensive analytics or to do
certain work on a specific location. Using External Replication gives you the ability to control the flow of the data without also dictating how it is going to
be processed. 

I mentioned earlier that once the External Replication is setup, the cluster will assign it to one of the nodes. We haven't discussed it yet, but that is one of
the more important roles of the cluster, deciding what work goes where. 

### Ongoing tasks and work distribution

The cluster as we know it so far isn't really smart. It give us the ability to distribute configuration, such as what databases goes where, what indexes are 
defined, etc. But it doesn't _do_ much. This is actually quite far from the way things are. This is because we have focused specifically on the flow of data 
inside a database group, rather then the flow of _work_. 

What does it mean, to have work assigned to a database group? The simplest example is the one we just talked about, external replication. If we have three nodes 
in this database group, which node is going to update the External Replica? We don't want to have all three nodes do that, there is no real point in doing so, 
and it can cause unnecessary network traffic. Instead, the cluster will assign this work to one of the database instances, which will be responsible for keeping
the External Replica up to date. 

Another example is hourly incremental backup of our data as well as a full backup on a weekly basis. We don't want to have this backup run on all three nodes at 
the same time, leaving aside the fact that this will increase the load on the system across the board, we don't really have anything to do with triplicate 
backups of the same data. This is where the work assignment portion of the cluster come into play. 

Whenever there is a task for a database group to do, the cluster will decide which node will actually be responsible for it. That seems pretty easy, but there is
actually a bit more to the story. Just assigning work is easy, but the cluster is also watching the nodes and checking how healthy they are. If a node is down, 
then the cluster will reassign the work to another node for the duration. 

In the case of a backup, if the node that is responsible for the backup is down when the backup is scheduled, another will shoulder the load and make sure that 
you don't have any gaps in your backups. In the case of External Replication another node will transparently take over keeping the External Replica up to date 
with all the changes that happened in the database group. 

Another type of work in the cluster that we already talked about in this book is subscriptions. The cluster will divide all the subscriptions between the various
nodes in the database group and reassign them on failure without your code needing to change anything (or even typically even be aware of that). Other types of
work that we can define for the database group include ETL processes, which are covered in the next chapter. 

### The supervisor's promotion

The cluster is continously measuring the health of each node in the cluster, using a component known as the supervisor^[I always imagine it in bright neon green
tights and a cape, flying around spouting cliches as it keeps everything in order.]. The supervisor will detect any errors in the cluster, react to a failed node
by reasssigning work and in general keep everything running.

This is important enough to mention here and not in the Operations portion of the book because the Supervisor also gives you a wealth of information about the 
state of the different nodes and database instances that are running on them. In particular, one of its most important roles is the promotion and demotion of 
database instances.

Given that RavenDB is a distributed database and not a military organization, what does it means, promotions and demontions? Consider the case of a database 
group that has two database instances. We want to increase the number of replicas so we add a new node to the database group. The process is pretty easy, all we 
need to do is to click on `Manage group` in the Studio and add the new node. 

On large databases, the process of adding a new replica can take a while. During that process, the newly added node isn't really part of the group. It cannot 
take over work in the group (we certainly don't want its backup until it is completely caught up, for example) and we don't want to failover clients to it 
(since it doesn't have the full data yet). Because of this, when you add a new node to a database group, it isn't added as a full fledged member, instead, it is
added as a promotable instance. 

What does it means, promotable? It means that it cannot be assigned work for the database group, that it cannot be failed over to. In fact, it _is_ work for the
database group, since one of the full members is going to have to start pushing data into this new node until it is fully up to date. This goes on until the 
supervisor can confirm that the new node is caught up with the state of the database group and that it finished indexing all the data that we sent to it. At 
this point, the supervisor will promote the node into a full member in the database group, work will be assigned to it and clients will consider as a 
failover target.

Conversely, when the supervisor detects that a node is down, it will demote it from a full member status into a promotable. This is done so when the node is 
coming back up, it will not start sending out outdated information. Instead, one of the other members in the database group will connect to it and update the 
recovered node until it is fully up to date, at which point the supervisor will promote it to full member again.

Using demotions and promotions as needed, the supervisor is able to communicate to clients (and the rest of the cluster) which are the authoritative nodes for a 
particular database group. Note that a failure doesn't have to be just a down node, something as simple as running out of disk space can also cause a database
on a node to fail and demote a node from being a full member. 

You won't generally need to handle such demotions and promotions yourself, or even really be aware of them. RavenDB is taking great pains to make the experience
of using the cluster as seamsless as possible, but you do need to understand _how_ this is happening so you can take advantage of RavenDB's behavior.

### Summary

In this chapter, we have touched most of the distributed portions of RavenDB. We started by spinning up our own cluster and played around with creating various
databases on it. We looked at how we can manage a database inside the Studio and how RavenDB is actually spreading a database group into multiple database 
instances on the various cluster nodes.

We talked about how the client API work with a RavenDB cluster, how we can ensure that a write is replicated to a minimum set of replicas before the server will
confirm it and how we can ask the client API to load balance our work between the different nodes in the database group. 

We discussed the difference in the distributed nature of RavenDB at the cluster layer (consensus, majorities required) and the database later (multi master, can
act independently) and how it affect our code. If there is a failure that knocks down half of the cluster, we'll not be able to do cluster operations, but 
reading and writing on each database will proceed normally, and clients will failover silently to any surviving nodes. Certain operations (creating databases or
defining new indexes) will not work until the cluster is in a stable state, but the most common day to day operations can proceed with each database instance 
acting on its own and merging its operations with its siblings.

That behavior can lead to conflicts, and we explored some of the options RavenDB has for resolving those conflicts. You can setup automatic conflict resolution 
based on a preferred node, selecting the most recent version or even provide your own logic to merge conflicting versions automatically. The RavenDB Studio also
contains a Conflicts section that allows you to inspect conflicts and manually resolve them. 

When then discussion transaction atomicity in replication, how RavenDB will ensure that all changes that happen in the same transaction are always replicated in
the same batch and how we can use revisions to ensure that holds true even for documents that were later changed independently. We looked at change vectors and
how they are used by RavenDB to piece together the absolute order of events and know what happened where and when. 

We looked at some architectual considerations for building a RavenDB cluster and how to scale it to a large number of nodes and how we can share data 
between different clusters. Finally, we peeked into the management of the cluster itself, with the supervisor assigning work to the various nodes and constantly
monitoring their health, promoting and demoting nodes as needed.

This was anything but a trivial chapter, it is packed with new ideas and it tries to cover the whole of running a distributed cluster in a single chapter. I 
tried to make sure that the topic in this chapter follow logically one another as we tour through the distributed portion of RavenDB, but there is quite a lot
to go through. In particular, I haven't talked about operations, monitoring, statistics etc at all. I'm leaving all of that a chapter dedicated just for that,
[Statistics, Monitoring and Insight](#monitoring). We'll also talk a abit more about the implementation details that I intentionally skipped here, because the
operations team need to understand what is going on exactly, while this chapter was about the overall concept.

In the next chapter, I'm going to talk another key part of working with RavenDB in a distributed environment, integrating with additional systems in your 
environment.