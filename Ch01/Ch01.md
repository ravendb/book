
# A little history

This is a technical book, but as important as the details about how things work, or what the design process is, I would like to tell you the actual story about how RavenDB started out. I'm going to be talking about myself quite a lot for this section, but we'll resume talking about RavenDB immediatley afterward.

You can skip this chapter and go directly to the technical content, coming back to it at a later time.


## The back story

In Sep 2007, I was sitting in JAOO, listening to [Joe Armstrong](http://joearms.github.io/) talk about Erlang, distributed systems and 9 nines reliability. 
Afterward, I bought his [Programming Erlang](http://pragprog.com/book/jaerlang2/programming-erlang) book and read it cover to cover. 
I thought the ideas presented in both the talk and the book were facinating, but just _reading_ about them in a book wasn't enough. Armed with just enough knowledge about Erlang to be able to sort of understand what it does, I decided that I need something bigger.

At the time, [CouchDB](http://couchdb.apache.org/) was one of the most cited Erlang projects, so I decided that I would go over its code and learn how production Erlang code is written. 
I [blogged about the experience](http://ayende.com/blog/3607/reading-erlang-inspecting-couchdb), so you can read my raw thoughts at the time.
This was my first real introduction to NoSQL. And it was at the level of actually reading through the codebase of a production quality database. It was a facinating journey.

But I don't _like_ the Erlang syntax. And I had several disagreements with the way CouchDB does things. And that was how it stayed for quite some time. Since 2004 or so, I have been dealing primarily with Object Relational Mappers and relational database. I'm a core team members in the NHibernate Project and I've been a consultant for much of that time.

Taken together, this means that ever since 2004, my job was mostly to go to clients and try to get their applications to perform well, be stable and scale to their requirements.
The problem with that was that at some point, I made a mistake. I was doing a code review for two clients, and I sent each of them the report about the issues that I found in their code and how to resolve it.

Mistakes like that aren't pleasant, but they happen. So imagine my surprise when the clients _didn't notice that they had a report about a completely different codebase_ before I pointed it out. In fact, one client was actually fast enough to _implement_ my suggestions before I could get back to them. Although they did comment that I was probably working on top of a different branch :-).

### All the same mistakes...

That led me to go back and review all of my work for the past several years, and realize that I was really doing the exact same thing, over and over again. I was working for clients that ranged from Fortune 100 to small startups, and in diverse industries such as health care, social media, risk management and real estate management. But the _problems_ that the clients run into were the same. And the _solutions_ were often the same as well.

It got to the point where I would tell my clients that if I wasn't able to give them a *major* performance boost within two days of arriving, I would cut my rate by 50%. I never had to do that, because usually within two hours of arriving at a customer, I was able to point out one of two or three very common issues that would drastically improve their situation.

I was very deeply into NHibernate consulting at the time. Which meant that most of my consulting was done on either the database interaction or the overall architecture of the system. In one memorable case, I helped a customer reduce the load on his system from roughly 17,000 (seventeen thousand) queries per page to a "mere" 75 queries per page. That is corrent, they have 17 _thousands_ queries per page. In production. For several years.
Admittedly, that is quite an extraordinary case. But my target was 75% - 90% reduction in the number of queries that the system made, and a comparable just in performance.

Usually I actually got to use my NHibernate knowledge to optimize the way the system accessed the database. But that usually happened at least several days into my visit. The first few days were _always_: Find the common Select N+1 issues, find the common Unbounded Result Set issues, etc.
Note that I didn't even try to find all those issues, because they were so prevelant. Just fixing the first three or five would usually give the system a very big boost in performance and make the customer very ahppy.

It made _me_ very sad, however. I was the database consultant equivalent for a plumber. I would go to a client, "unclog" the database, give some advice about how to do better, and move to another client, to do the exact same thing. 
I blogged about those issues extensively, and most of the people who invited me to look over their code were readers of my blog. By definition, then, they weren't stupid, careless or malicious. In fact, most of them were dedicated, hard working and very good at their jobs.

### Maybe tooling will help?

Something had to give, because if I saw yet another Select N+1 bug, I felt that I would just scream. So in 2008 I started working on what became the NHibernate and Entity Framework Profilers. Those tools monitor the interaction between your application and a relational database. Based on that information, they can show you how your code interacts with the database, what queires are being generated by what methods, and on top of that, they analyze the database interactions use that information to raise warnings and suggest improvements.

Building this tool meant that I could give it to my clients, and they would have the same insights as I did about how to properly work with a relational database. It also gave me an interesting project to work on that didn't involve finding the same issues over and over again with slight variations on the naming conventions.

The profilers are wonderful tools when you are working on relational databases, but they didn't actually solve the problems that I saw over and over again. It was not easy to pinpoint where the application was doing silly things.
But that just meant that we moved the problem. No longer having to deal with the common errors, we now had to deal with a much more complex ones. How do we get all of this complex data to the user, manipulate it properly and save it back in an consistent, performant and maintainable way.

The answer was that we really couldn't. There were quite a lot of constraints placed upon us by the choice of relational data store. Relational databases are wonderful things, masterworks of engineering, thought and literally decades of experience. They are also, quite often, a wrong tool for the job.

### Think different (database)...

It was at that point that I actually took the very daring steps of trying to set out and actually plan what would be my ideal database. The kind of database that wouldn't make me fight it. The kind of database that will allow me to really just get things done, instead of writing another schema migration script.

I thought about it often enough that I actually had a [whole design sketched out](http://ayende.com/blog/3897/designing-a-document-database) in a series of blog posts. And the idea just didn't leave me. I eventually broke down and just sat down and started writing the code.
Initially, I thought it would just be a spike, or another OSS project. I called it Rhino.DivanDB, I think you can guess what inspired me.

The problem is that saying Rhino.DivanDB is pretty hard (try saying it out loud several times), so eventually I realized that I'm actually calling in RivanDB. From there, it was just a matter of actually making sense, and RavenDB was born.

![Raven on rock](.\Ch01\Figure-01.png)

Since then, I became quite fond of ravens, and a lot of the internal components inside RavenDB are named after various ravens. At around the same time that I decided on the name change, I also realized that just doing this as an open source project isn't going to be enough for me.

I had this burning desire to actually go and _make_ this. And not just as a random github repository, I wanted to make it great. That meant that it had to be product. A product vs. a project is not in the scope of this book (even in this often side tracking section), but in general, that meant sitting down and coming up with an actual business plan. Here is what I'm going to invest, here is how the money is going to come in, this is the cut off point if this is an utter failure, and this is the point when I drink the champagne. 

### Setting RavenDB free...

In May of 2010, there was a public release of RavenDB 1.0. That was quite an event, and I was very pleased with what we had managed to achieve. The plan called for working on RavenDB, showing it off, and building confidence in it. I expected this process to take 18 months, since the lead time for something as critical as a database is usually very long. Instead, we had a system running in production using RavenDB within four months! And the pick up since then has been well above what I initially expected.

Oh well, I'll settle for building great databases, rather than realistic business plans :-).

Today, the RavenDB Core Team has about 15 full time developers, and the limiting factor is the quality we require from our people. RavenDB runs mission critical systems from healthcare to banking from goverment institutions to real estate brokerage systems.
Several books has been written about RavenDB, articles about it show up regularly in trade journals and you can hear talks in user groups and conferences across the world.

I'm quite happy with this situation, as you can imagine. And RavenDB is just getting better...

All of that said, the back story might be interesting to you or not, but you aren't here to read about the history of RavenDB. You are reading this because you want to use RavenDB in your future. And that means that you need to understand why you'll want to do that...

## 2nd generation document database

A guy wakes up one day, and decides he want to build a database...

No, you don't get to hear the rest of the joke. But this is a serious question. Why build another NoSQL database? When sitting down to design what would become RavenDB, I had a few goals in mind. 
At the time, I mostly dealt with consulting to clients building line of business applications. Those ranged from small potatoes such as "we need to track expenses" to medium size risk management system to monster systems running the core business processes for Fortune 100 companies.

The core principal for most line of business applications is OLAP, which stands for OnLine Transaction Processing. That is the major area that I set out to building RavenDB to serve. RavenDB success means that it would be _the_ obvious choice for any OLAP system.
And I think we have gone quite a bit toward that destination.

RavenDB design goals included:

* Zero friction throughout (dev & ops)
* Zero admin
* Safe by Default
* Transactional / ACID
* Easily scalable

Note that most of those design goals are actually just different ways to say the same thing. Basically, the goal of RavenDB is that it Get Out of Your Way and Just Works.

### Let me do my job, and you'll do yours

We had to do quite a lot of work in order to achieve the It Just Works model. The RavenDB engine will monitor itself and (within configured limits) knows how to auto tune itself on the fly. The Safe by Default design means that it is harder (sadly, it is still not impossible) to shoot yourself in the foot.

But probably most importantly, we have look at other databases, relational and NoSQL alike, and figured out what the pain points were. What we found was that mostly, people were struggling because it was so _hard_. In order to deploy a NoSQL solution you had to become an expert on your database, and even then, you usually had to babysit it quite often.
I'm talking about everything from installation and configuration, to looking at the data your put inside the database, to understanding errors and monitoring the systme in production.

That was doubly true when you consider that my usual ecosystem is the .NET framework. That meant that most of my applications and systems are actually running on Windows. And most NoSQL solutions either flat out couldn't run, or could run, but only as alpha quality software. My goal was to create a really good database that .NET developers could use. As it turned out, we managed to do quite a bit more than that. 

As a small example of that. Here are the installation instructions for RavenDB:

* Go to the [RavenDB Download](http://ravendb.net/download) page.
* Download the latest stable build
* If you've downloaded the zip archive, unzip it and double click Start.cmd
* Alternatively, if you've downloaded the MSI file, install it and follow the wizard.

You now have a running RavenDB database, and you can browse to (by default) http://localhost:8080/ to access the RavenDB Studio. 
The point in these instructions isn't so much to explain how to install RavenDB, we'll touch on that later. The point is that this is literally the smallest number of steps that we could get to getting RavenDB up & running.

This is probably a good time to note that this is actually how _we_ deploy to our own production environment. As part of our dog fooding effort, we always run our systems using the default configuration, to make sure that RavenDB can optimize itself for our needs automatically.

Most people get excited when they see that RavenDB ships with a fully functional management studio. There is no additional tool to install, just browse to the database URL and you can start working with your data. 
To be honest, even though we invested a lot of time and effort in the studio, that is quite insulting. We've spent even more time making sure that the actual database engine is pretty awesome, and people get hung up on the UI.

A picture is worth a thousand words, and I think that Figure 2 can probably help you understand what we _didn't_ want to have.

![1st gen NoSQL](.\Ch01\Figure-02.png)

So this is a good reason to build a new NoSQL database, but what about all those _SQL_ databases? We've used them forever, and they _work_, so what changed?

### The case for a non relational data store.

Edgar F. Codd formulated the relational model in 1969. Ten years later, Oracle 2.0 comes to the market. And Sybase SQL Server came out with its first version in 1984. By the early 90s, it was clear that relational database has pushed out the competition (such as navigational or object oriented databases) to the sidelines.
It made sense, you could do a lot more with a relational database, and you could do it easier, usually faster and certainly in a more convenient manner.

Let us look at what enviornment those relational databases grew up in[^1]. In 1979, you could buy the [IBM's 3370](http://www-03.ibm.com/ibm/history/exhibits/storage/storage_3370.html) direct access storage device. It offered a stunning 571MB (read, megabytes) of storage for the mere cost of $35,100. 
For reference the yearly salary of a programmer [at that time](http://www.bls.gov/opub/mlr/1982/10/rpt2full.pdf) was $17,535. In other words, the cost of a single 571MB hard drive was as much as _two full time developers_.

[^1]: This is a somewhat apples & oranges comparisons that I'm making here. It is hard to get pricing on a consistent set of hard drives from those years. Some of the hard drive listed here would be considered enterprise grade hardware, while others are consumer grade. The intent is mostly to give you a good idea about the relative costs.

In 1980, we had the first GB drive, for merely $40,000, weighting 550 pounds and about as big as a refrigerator. By 1986, the situation [improved](http://walt.lishost.org/2010/11/musing-about-hard-disks/) and purchasing a good internal hard drive with all of 20MB at merely $800. For reference, a good car at the time would cost you less than $7,000.

Skipping ahead again, by 1996 you could actually purchase a 2.83 GB drive for merely $2,900. A car at that time would cost you $12,371. I could go on, but I'm sure that you get the point by now. Storage used to be _expensive_. So expensive that it dominated pretty much every other concern that you can think of.

At the time of this writing, you can get a 6 TB drive for less than $300 ^[WD60EFRX - if you really care]. And a 3 TB drive will cost you roughly $100. That is 2014 for roughly 30 _cents_ per gigabyte, and 1980 for 40,000 _dollars_ per gigabyte ^[Note that we are ignoring 30 years of inflation, too.].

Even leaving this aside, we also have to consider the type of applications that were written at that time. In the 80s and early 90s, the absolute height of user interface was the master/details form. And the number of users you had could usually be counted on a single finger. 

That enviornment produced databases that were optimized to answer the kind of challenges that prevailed at the time. Storage was _expensive_, so a major effort was made to reduce storage as much as possible. Users' time was very cheap by comparisons, so trade offs that meant that we could save some disk space at the expense of making the user wait were good design decisions.
Time passed, machines got faster and cheaper, disk space became _cheap_, making the user wait became unacceptable. But we are still seeing those tradeoffs today.

Those are mostly evident when you look at normalization, fixed schemas and the relational model itself.

### Normalization, compression and data corruption

You've very likely used a relational database in the past. That means that you've learned about normalization, and how important that is. Words like data integrity are thrown around quite often, usually.  
But the original purpose of normalization had everything to do with reducing duplication to the maximum extent.

A common example for normalization is addresses. Instead of storing a customer's address on every order that he has, we'll simply store the address id in the order, and we have saved ourselves the need to update the adress in multiple locations.
You can see a sample of such a schema in Figure 3 ^[You can also access the [schema online](http://dbdsgnr.appspot.com/app#agdkYmRzZ25ycg8LEgZTY2hlbWEYqc28Bww)].

![A simpel relational schema for orders](.\Ch01\Figure-03.png)

You've seen (and probably wrote) such schemas before. And at a glance, you'll probably agree that this is a reasonable way to structure a database for such a 
