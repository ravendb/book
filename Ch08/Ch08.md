
## Sharing data and making friends with ETL

[Sharing data and making friends with ETL]:(#integrations)

While talking about distributed work with RavenDB so far we have focused primarily on the work that RavenDB is doing to replicate data between different nodes
in the same cluster as part of the same database group or between different database in potentially different clusters. This mode of operation is simple, 
because you don't just setup the replication and RavenDB will take care of everything else. But there are other modes for distributing data in your systems.

Replication assumes that you have another RavenDB instance and that you want to replicate _everything_ to it. When replicating information, that is what you
want, but we also have a need to share a _part_ of the data. ETL^[Extract, Transform, Load] is the process in which we take data that resides in RavenDB
and push it to an exteranal source, potentionally transforming it along the way. That exteranal source can be another RavenDB instance or a relational database. 

Consider a micro service architecture and the Customer Benefits service. This service decides what kind of benefits the customer has. This can be anything
from free shipping to giving a discount on every 3rd jug of milk and the logic can be as simple as "this customer is in our membership club" to complex as 
trying to compute airline miles. Regardless of how the Customers Benefits service works, it need to let other parts of the system know about the 
benefits that this customer has. The Shipping service, the Helpdesk service and many others need to have that information. 

At the same time, we _really_ don't want them to poke their hands into the Customers Benefits database (or worse, have a shared database for everything)^[Doing
so is a great way to ensure that you'll have all the costs of a micro service architecture with none of the benefits.]. We could design an API between the 
systems, but then the Shipping service will be dependent on the Customer Benefits service to always be up. A better solution is to define an ETL process between
the two services and have the Customer Benefits service publish updates for the Shipping service to consume. Those updates are part of the publiccontract of
those services, mind. You shouldn't just copy the data between the databases.

Another example is the reporting database, RavenDB is a wonderful database for OTLP scenarios, but for reporting, your organization likely already have a 
solution, and there is really little need to replace that. But you can't just dump the data from RavenDB directly into a relational database and expect things
to work. We need to transform the data as we send it to match the relational model.

For all of those needs, RavenDB has the notion of ETL processes. RavenDB has builtin ETL to another RavenDB instance and to a relational database 
(such as MS SQL Server, Postgres, Oracle, MySQL, etc). 
Because RavenDB has native relational ETL, brown field systems will typically start using RavenDB by replacing a single component at a time, RavenDB is used
to speed up the behavior of high value targets, but instead of replacing the whole system, we use ETL processes to write the data to the existing relational
database. We'll cover that later in this chapter, discussing the deployment of RavenDB as a write-behind cache.

In most cases, the rest of the system doesn't even need to know that some parts are using RavenDB. This is using RavenDB as the write behind cache.
Some part of your application reads and writes to RavenDB directly, and RavenDB will make sure to update the relational system. 
We'll start talking about ETL processes between RavenDB instances, because we explore the whole ETL process without introducing another database instance.


### ETL processes to another RavenDB instance

Any non trivial system is going to have at least a few ETL processes and RavenDB has a good story on how to handle that. The simplest ETL process between
two RavenDB nodes is telling RavenDB that we want to send just a single collection to the other side. We first need to configure the connection string
we'll use. Because RavenDB ETL work between clusters, we may need to define multiple URLs, and it is easier to put it all in a single location. 

Go to `Settings` and then to `Connection Strings` and create a new RavenDB connection string. The easiest is to use create a new database in your cluster
and setup the connection string to it. As you can see in Listing 8.1, I've defined a connection string to the `helpdesk` database in the live test instance.

![Defining a connection string to another RavenDB instance.](./Ch08/img01.png)

With the connection string defined, we can now go ahead and build the actual ETL process to the remote instance. Go to `Settings` and then to 
`Manage Ongoing Tasks` and click the `Add Task` button and then select `RavenDB ETL`. You can see how this looks like in Figure 8.2.
Give it a name and select the previously defined connection string.

> **What about security?**
>
> We'll cover security in depth in the ["Securing your Ravens"](#security) chapter, but given that we have shown a connection string I wanted
> to address a not so minor issue that you probably noticed. We don't have a place here to define credentials. This is because we are talking
> to another RavenDB server, and for that we use x509 certificates. 
>
> In practice, this means that inside the same cluster ETL processes don't need any special configuration (nodes within the same cluster 
> already trust one another). Outside the cluster, you'll need to register the cluster's certificate with the remote cluster to allow the ETL
> process to work. You can read the full details of that in the security chapter.

![Defining an ETL process to another RavenDB instance](./Ch08/img02.png)

Now we need to let RavenDB know what will be ETL'ed to the other side. We do that by defining scripts that control the ETL proceess. Click on the 
`Add New Script` and give it a name such as "Employees to Helpdesk" and then select the Employees collection below, click on `Add Collection` and
then on `Save`. The result should look like Figure 8.3.

![Defining a simple "copy the whole collection" ETL script](./Ch08/img03.png)

Because we didn't specify anything in the ETL script, RavenDB will just copy the entire collection as is to the other side. This is useful on its own
because it allow us to share a single (or a few) collections with other parties with very little work. 

> **ETL is a database task, with bidirectional failover**
>
> In the previous chapter, we learned about database tasks and how the cluster will distribute such work among the different database instances. If a node
> fails, then the ETL task responsability will be assigned to another node. 
>
> It is important to note that in cases where we replicate to another RavenDB instance, we also have failvoer on the recieving end. Instead of specifying a 
> single URL, we can specify all the nodes in the cluster, and if one of the destination node is down, RavenDB will just run the ETL process against another
> node in the databse group topology.

It is _very_ important to remember that ETL is very different from replication. When a RavenDB node perform ETL to another, it is not replicating the data, it 
is _writing_ it. In other words, there are no conflicts and no attempt to handle such. Instead, we'll always _overwrite_ whatever exists on the other side. 
As far as the destination node is concerned, the ETL process is just another client writing data to the database. This is done because the data is _not_
the same. This is important because of a concept that we didn't touch so far, data ownership. 

> **Data ownership in distributed systems** 
>
> One of the key differences between a centralized system and a distributed system is the fact that in a distributed environment, different parts of the system
> can act on their local information without coordination from other parts of the system. In a centralized system, you can take a look or use transactions to
> ensure consistency. But in a distributed system, that is not possible or prohibitely expensive to do so.
> Because of this limitation, the concept of data ownership is very important.
> 
> Ideally, you want every piece of data to have a single well defined owner and only mutate that data through that owner. That allows you to put all the 
> validation and business roles in a single place and ensure overall consistency. Everything else in the system will get updates to that data through its 
> owner. 
>
> A database group, for example, needs to handle the issue of data ownership. Conceptually, it is the database group as a whole that owns the data stored in 
> the document. However, difference database instances can mutate the data. RavenDB handles that using change vectors and conflict detection and resolution.
> That works for replication inside the database group, because all the nodes in the group share the ownership of the data. But it doesn't work for ETL.
>
> The ETL source is the _owner_ the data and it is distributing the updates made to its data to interested parties. Given that it is the owner, it is expected
> that it can just update it to the latest version it has. That means that if you made any modification to the ETl'ed data, they will be lost. Instead of 
> modifyingg the ETL'ed data directly, you should create a compantion document that _you_ own. In other words, for ETL'ed data, the rule is that you can look,
> but not touch.
>
> An example of such a companion document is when you have ETL for users to the helpdesk system. The `users/123-B` document is owned by the users database and
> the helpdesk system will store all the information it needs about the user in `users/123-B/helpdesk` document, ensuring that there is no contention on the 
> ownership of documents.

So far we have only done ETL at the collection level, but we can also modify the data as it is going out. Let's see how we can do that, and why would we want
to do this. 

#### ETL Scripts

Sometimes you don't want to send a full document in the ETL process. Sometimes you want to filter them or modify their shape. This is quite important  
since ETL processes compose a part of your public interface. Instead of just sending your documents to remote destination willy nilly, you'll typically only
send data that you are interested in sharing, and in a well defined format.

Consider the employees we sent over the wire, we sent them as is, potentially exposing our own internal document structure and making it harder to modify in 
the future. Let us create a new ETL process which will send just the relevant details and add a script for sending redacted employee information over the wire. 
You can see how this looks like in Figure 8.4.

![Using ETL with a script to redact the results](./Ch08/img04.png)

It is important to understand that when we are using an ETL script to modify what is sent, we need to take into account that RavenDB will send just what we told
it to. This seems obvious, but it can catch people unaware. If you don't have a script, the data sent to the other side will include attachments and will go to 
the same collection as the source data.

However, when you provide your own script, you need to take responsability for this yourselves. Listing 8.1 shows an example of a slightly more complex example.

```{caption="Creating a subscription to process customers" .js}
var managerName = null;
if(this.ReportsTo !== null)
{
    var manager = load(this.ReportsTo);
    managerName = manager.FirstName + " " + manager.LastName;
}
    
loadToEmployees({
    Name: this.FirstName + " " + this.LastName,
    Title: this.Title,
    BornOn: new Date(this.Birthday).getFullYear(),
    Manager: managerName
});
```

The script in Listing 8.1 will send the employees, their title, birth year and manager over to the other side. You can see that the script is 
actually full blown javascript and allow you complete freedom with how you extract the data to load into the remote server. A word a caution
is required about using functions such as `load` in this context, though. While this will work just fine, the referencing document will not be 
updated if the referenced document has been updated. In other words, if the manager's name has been udpated, it will not trigger an update 
to the employees that report to this manager.

It is common to limit yourself to just the data from that particular document, since that make it easy to ensure that whenever the document is
changed, the ETL process will reflect these changes completely on the other side. 

> **Reseting the ETL process after update** 
>
> It is common to test out the ETL process as you develop it, but by default, updates to the ETL script will not applied to documents that
> were already sent. You can use the "Apply script to documents from beginning of time" option during the script update, as shown in 
> Figure 8.5 to let RavenDB know that it needs to start the ETL process for this script from scratch, rather than apply the update only 
> to new or updated documents.
> 
> ![Reseting the ETL process after a script update](./Ch08/img05.png)
>
> This is done to avoid expensive reset that would force RavenDB to send all the data all over again for a minor change.

Looking on the other side, you'll be able to see the replicated document, as shown in Listing 8.2.

```{caption="ETL'ed document on the side" .json}
{
	"BornOn": 1966,
	"Manager": "Steven Buchanan",
	"Name": "Anne Dodsworth",
	"Title": "Sales Representative",
	"@metadata": {
		"@collection": "Employees",
		"@change-vector": "A:84-4Xmt8lVCrkiCDii/CfyaWQ",
		"@id": "employees/9-A",
		"@last-modified": "2017-12-04T12:02:53.8561852Z"
	}
}
```

There are a few interesting things in the document in Figure 8.2. First, we can see that it has only a single change vector entry (for the destination
database) and the last modified date is when it was written to the destination, not when it updated on the source. 

#### Multiple documents from a single document

Another interesting ETL example is when we want to push multiple values out of a single document, as showing in Listing 8.3.

```{caption="Sending multiple documents from a single source document in ETL" .js}
loadToEmployees({
    Name: this.FirstName + " " + this.LastName,
    Title: this.Title,
    BornOn: new Date(this.Birthday).getFullYear(),
});

loadToAddresses({
    City: this.Address.City,
    Country: this.Address.Country,
    Address: this.Address.Line1
});
```

The results of the script in Listing 8.3 can be seen in Figure 8.6. You can see that the `Employees` documents were sent, but are also the addresses documents. 
For those, we use the prefix of the source document to be able to identify them after the fact. 

![Multiple outputs from a single source document on the destiantion](./Ch08/img06.png)

An important consideration for sending multiple documents from a single source document is that on every update to the source document _all_ the documents that 
were created from this document are refreshed. You don't have control over the ids being generated and shouldn't assume that they are fixed. 

> **Attachments and Revisions with RavenDB ETL** 
>
> Attachments are sent automatically over the wire when you send a full collection to the destination. However, revisions are not. 
> If you do use a script, there is currently no way to indicate that attachments should also be sent. This feature is planned but wasn't completed in time
> for the 4.0 RTM release.
> In the same vein, another feature that is upcoming is support for ETL processes on top of the revision data, similar to how it is possible to 
> use subscriptions with the `current` and `previous` versions of the document.

### Use cases for ETL between RavenDB instances

When you have a complex system, composed on more than a single application, it is considered to be a _Bad Form_ to just go peek inside
another application's database. Such behavior lead to sharing way too much between the applications and will require constant coordination
between the applications as you develop and deploy them. A boundary between application is required to avoid such issues.

> **Shared Database Integration Anti Pattern**
>
> This kind of behavior is called the Shared Database Integration and is considered to be an anti pattern. For more information on 
> why you should avoid such a system, I refer you to [Martin Fowler's post on the matter](https://martinfowler.com/bliki/IntegrationDatabase.html)
> and in particular to the summary: "most software architects that I respect take the view that *integration databases should be avoided*."

One way to create such a boundary is to mandate that any time that an application needs some data from another application, it will go there.
In concrete terms, whenever the helpdesk system need to lookup a user, it will go to the users application and ask it to get that user's data.
This is often referred to as a service boundary.

The problem with such a system is that many interactions inside a particular service require information that is owned by another service. 
Any support ticket opened by a user will require a call from the helpdesk service to the users management service for details and updates.
As you can imagine, such a system still require a lot of work. In particular, even though we have a clear boundary between the services and
division of responsability between them, there is still a strong temporal coupling between them.

Taking down the users management service for maintenance will require taking down everything else that needs to call to it. A better alternative
in this case is to not rely on making remote calls to a separate service, but to pull the data directly from our own database. This way, if the
users management service is down, it doesn't impact the operations for the helpdesk service. 

Note a key difference here between this type of architecture and the Shared Database model. You don't have a single shared database, instead the
helpdesk database contains a section in it that is updated by the users management service. In this manner, the ownership of the data is retained
by the users management service but the responsability for maintaining it and keep it up is with the helpdesk service.

If the users management service is taken down four maintenance, it has no impact on the helpdesk service, which can resume operations normally.
The design of the ETL processes in RavenDB is meant to allow such a system to be deployed and operated with a minimum of hassle. That is also partly
why the ownership rules and responsability for changes is built the way it is. 

ETL is explciitly about sending data that you own to a 3rd party which is interested in it, but doesn't own it. As such, any change you make will, 
by necessity, overwrite any local changes in the destination. If you are interested in a shared ownership model ETL is not the method you should use
but rather External Replication, discussed in the previous chapter.

#### Modeling concerns for ETL processes

An important aspect to the use of ETL processes in a multi service environment is the fact that the ETL process itself is part of the service contract 
which needs to be deployed, versioned and managed as such. In particular, the format of the documents that are sent via the ETL process compose part 
of the public interface of the service. As such, you should think carefully about the shape of the data you expose and the versioning considerations
around that.

Listing 8.1 is a good example of exposing just enough information to be useful for the other side without leaking implementation details or other aspects
that may change over time. A part of the design for the ETL processes was the notion that you may have different processes and different outputs for 
different destinations. In this way, you may collaborate with another service to update your contract while maintaining the same behavior for all others.

Another option is to only allow additive changes, so adding a property would be fine, but removing one wouldn't be. That _usually_ works, but unforutnately
[Hyrum's Law](http://www.hyrumslaw.com/) applies and even such innocuous changes can break a 3rd party.

#### Controlled exposure of data via ETL

Beyond using ETL for dessiminating documents between services, there are a few other scenarios in which they can be useful. A typical usage scencario for 
ETL is sending data from production (after redacting any sensitive information) to UAT or CI instances, allowing you to test of realistic data sizes
and with a real world data.

Sending redacted data from production to UAT is just one application of a larger concept, controlling the degree in which you expose data to outside 
parties. We'll discuss authorization in detail in the [Securing your Ravens](#security) chapter but RavenDB allows you to define permission at the
database level.

Sometimes, you need to apply such permission on a per collection or a per document level. In some cases, it is per field or even depends on the exact
data in the document to control who can see them. The ETL process in RavenDB offers a nice way to manage that using the customization scripts. You can
select exactly which collections you want to send and you are free to decide exactly what data you want to send and what data should be held back.

The target database in this case will usually be another database in the same cluster, which you'll allow access to based on your own internal policies.
This gives the operations team a lot of freedom in designing and implementing data exposure processes and policies.

#### Case study: ETL usage in Point of Sales systems

Another example is data aggregation. One usecase that we have seem many times is embedding of RavenDB inside a larger application. In this manner, each 
instance of the application also has its own instance of RavenDB. Consider a Point of Sales system in a supermarket, running its own copy of the store
management application and talking directly to its own local database. 

In such a case, we want to send some of the information we have (the new sales generated on that particular POS system) to the central server. At the same
time, we have the central server send updates to each of the POS systems with new prices, products, etc. However, there is no need to send all the data
back to the server or two have each POS system contain all the sales across the system. Figure 8.7 shows the data flow between the various components of 
the system in such a scenario.

![Data flow diagram for a POS system with replication and ETL processes.](./Ch08/img07.png)

The central server in the store will send updates using External Replication to all the POS machines. In turn, whenever there is a new sale, the POS will
use an ETL process to update the central server in the store about the new sales that were rang on the machine. You can also imagine the same architecture
writ larger when you zoom out. With the central server of the supermarket chain updating each store in turn and aggregating all the sales across all its
stores.

#### Failover and recovery with ETL in distributed systems

The ETL feature in RavenDB, like any other piece, was design with the knowledge that networks fail, have outages and slowdown and in generate are _not_
something that you can rely on. Similar to the way replication is designed, the ETL process is resiliant to such issues.

In the worst case, when the ETL process has no way to communicate with the other side, it will wait until the destination is reachable again and proceed
from where it left off. In other words, it is a fully offline, async process. If the other side is not responding for any reason, we'll catch up when
we can.

That is the worst case, but we can usually do better. An ETL process is an ongoing task in the cluster, and that is a key phrase. That means that while
the task is assigned to a single node, it is the responsability of the cluster as a whole. If the node that was assigned to the ETL process is down for
whatever reason, the cluster will move the task assignment^[Assuming that your license allows dynamic task distribution, that is.] to another node and 
the ETL process will proceed from there.

Being robust on just the sender side is all well and good, but we are also robust on the recieving end. Part of defining the RavenDB connection string
in Figure 8.1 was to add the URL of the destination. In this case, we have just one server, but if we have a cluster on the other side, we can list
all the nodes in the cluster, and RavenDB will ensure that even if a node goes down on the other end, the ETL process will proceed smoothly.

### Sending data to a relational database

In addition to supporting ETL processes to another RavenDB instance there is also support of ETL process to relational databases (Microsoft SQL, Oracle,
PostgreSQL, MySQL, etc). The idea here is that RavenDB holds the master copy of the data and want to send the data to a relational database. This is
desirable for many reasons. 

You probably already have a reporting infrastructure in your organization and instead of having to build one from scratch for use with your data inside
of RavenDB, you can just let RavenDB schlep all that data to a relational database and use your pre-existing infrastructure. It is also common to use
this feature during migrations from a relational database to RavenDB. Instead of trying to do it in a big bang sort of way, you'll slowly move features
away from the relational database, but you'll use the ETL feature in RavenDB to make sure that the rest of the system is not actually aware that anything 
has changed.

Finally, a really intereting deployment model for RavenDB is as a write-behind cache. Because of RavenDB's speed and the different model, it is often
_much_ faster than a relational database for many types of queries. That make it ideal for the kind of user facing pages with the need for speedy 
reactions. Instead of moving the entire application to RavenDB, you can move just a few of the pages that have the most impact for the users.

In many cases, you can use RavenDB in this manner as a read only cache, although one that is much smarter than the usual cache and has excellent querying
capabilities. But it often make sense to use RavenDB for writes as well, if the scenario demands it. And in this case, you'll write directly to RavenDB
and let RavenDB write to the relational database behind the scenes. That can be very helpful if your relational database is struggling under the load, 
since a large portion of it will now be handled by RavenDB instead.

> **The impedence mismatch strikes back**
> 
> Translating between the document model and the relational model isn't trivial.
> Using the sample data as a good example, we have the notion of an `Order` document which contains and array of `Lines`. However, there is no good way 
> to represent such an entity in a single table in the relational model. In order to bridge the gap in the models we can use the ETL script to transform the data. 
> Instead of sending to a single table, we'll send the data from a single document to mutliple tables. 
>
> The concept of data ownership can also complicate things, kike with RavenDB ETL, we need to be the owners of the data. In particular, the way that RavenDB
> implements updates to the data is via a `DELETE` and `INSERT` calls, not via an `UPDATE`. This is because we don't know what the current state in the relational
> data is and we need to make sure that we have a clean slate for each write.
> 
> That means that you either need to modify your foreign keys on the destination tables, because they may be violated temporarily during the ETL operation until
> RavenDB make it all whole again. If you are using Oracle or PostgreSQL, you can set such constraints as `deferrable` so they'll be checked only on commit. 
> If you are using a database that doesn't support this feature (such as Microsoft SQL), you'll likely need to forgo foreign key constraints for the tables
> that RavenDB is writing to.

We'll start with a simple ETL process for `Employees` as a way to explore the SQL ETL feature. Before we can start, we need to have create a data on Microsoft SQL
with the appropriate table. You can see the table creation script in Listing 8.4.

```{caption="Table creation script for the sample ETL process" .sql}
CREATE TABLE Employees
(
	DocumentId NVARCHAR(50) NOT NULL PRIMARY KEY,
	Name NVARCHAR(50) NOT NULL,
	Birthday DATETIME NOT NULL
)
```

Once you have created this table in the relational database go into the RavenDB management studio and go to `Settings`, and then to `Connection Strings`. Create
a new SQL connection string, as shown in Figure 8.8. You'll need to provide your own connection string, note that you can test that the connection string is
valid directly from the studio. 

![Creating a new SQL connection string](./Ch08/img08.png)

> **Note the identity issue**
>
> In Figure 8.8, you can see a SQL connection string that uses `Integrated Security=true`. This is possible because the user that 
> RavenDB is running under has permissions to the database in question. In this case, it is running as in a debug mode under my own
> user. In production, RavenDB will often run as a service account, and you'll either need the operations team to allow  
> the service account access to the relational database or use a username and password to authenticate.

You can now go to `Settings`, `Manage Ongoing Tasks`, click on `Add Task` and select `SQL ETL`. You can name the ETL process and select the appropriate connection
string to use here. Before we can really start, we need to let RavenDB know which tables are going to be used in the ETL process and what column in the destination
table is going to be used as the document id column. Note that this doesn't have to be the primary key of the table in question, it just needs to be a column that
RavenDB can use to place its document id in. You can see such a configuration in Figure 8.9.

![Specifying a table to be used in the ETL process](./Ch08/img09.png)

Select the `Employees` collection on the trafromation script on the right and use the script from Listing 8.5.

```{caption="Simple ETL script to send employees to SQL" .js}
loadToEmployees({
    Name: this.FirstName + " " + this.LastName,
    Birthday: this.Birthday
});
```

Click on `Add` on the tranfromation script and then `Save` for the ETL process and head into your SQL database to inspect the results. You can see the output of
`select * from Employees` in Figure 8.10.

![Result of SQL ETL script on the relational database side](./Ch08/img10.png)

You can now play with the data, modifying and creating employees as you wish. RavenDB will make updates to the relational database whenever a document is changed.
This is done in an async and resilient manner. If the relational database is slow or not responding for any reasons, it won't impact operations for RavenDB and 
we'll retry the ETL operation until we are successful.

#### Multi table SQL ETL processes

The example in Listing 8.5 is pretty simple. We have a one to one match between the documents and the rows that represent them. We did have a bit of fun with
the concatenation of the first and last name into a single field, but nothing really interesting was going on there. Let's tackle a more complex problem, sending
the `Orders` documents to the relational database. 

Here, we can't just rely on one to one mapping between a document and a row. An `Order` document can contain any number of lines and we need to faithfully 
represent that in the SQL destination. We'll start by first defining the tables that we need on the receiving end, as shown in Listing 8.6.

```{caption="Table creation script for multiple tables ETL process" .sql}
CREATE TABLE Orders
(
	OrderId NVARCHAR(50) NOT NULL PRIMARY KEY,
	Company NVARCHAR(50) NOT NuLL,
	OrderedAt DATETIME NOT NULL,
	Total FLOAT NOT NULL,
	ShipToCountry NVARCHAR(50) NOT NULL
)

CREATE TABLE OrderLines
(
	OrderLIneId int identity NOT NULL PRIMARY KEY NONCLUSTERED,
	OrderId  NVARCHAR(50) NOT NULL INDEX IX_OrderLines_OrderId CLUSTERED,
	Price FLOAT NOT NULL,
	Product NVARCHAR(50) NOT NULL,
	Quantity INT NOT NULL
)
```

Once we have created the tables in Liting 8.6, we can proceed to actually define the ETL process for orders. First, we need to add the new tables, as you can see
in Figure 8.11. 

![Adding additional tables to the SQL ETL process](./Ch08/img11.png)

Once we have configured the additional tables for the ETL process, we can get started on actually sending the data over. We need to let RavenDB know both that we
are going to be using these two tables and that it needs to send multiple values to the `OrderLines` table. This turn out to be easy to do. The reason why we need
to define the tables upfront for SQL ETL is that we are generating matching functions to be called from the script. In the case of the script to send the `Orders`
to the relational database, these are `loadToOrderLines` and `loadToOrders`, as you can see in Listing 8.7.

```{caption="ETL Script for sending the Orders documents to the Orders and OrderLines tables" .js}
var total = 0;

for (var i = 0; i < this.Lines.length; i++) {
    var line = this.Lines[i];
    total  += line.PricePerUnit * line.Quantity * (1 - line.Discount);
    loadToOrderLines({
        Quantity: line.Quantity,
        Product: line.Product,
        Price: line.PricePerUnit
    });
}

loadToOrders({
    Company: this.Company,
    OrderedAt: this.OrderedAt,
    ShipToCountry: this.ShipTo.Country,
    Total: total,
});
```

The script in Listing 8.7 iterates over the `Lines` in the order, telling RavenDB that we need to send the object to that table as well as computing a running total
of the order. Finally, it call to `loadToOrders` with the final tally for the `Orders` table. Only when the script completes will RavenDB actually call into the 
relational database and update it. 

You might have noticed in Listing 8.7 and Listing 8.5 that we didn't specify the `OrderId` or `DocumentId` columns. These are implicitly defined by us when we set
up the tables that the SQL ETL process will use.

#### The care and feeding of production worthy SQL ETL processes

With RavenDB ETL, there isn't a lot that you need to verify to get things right, but with SQL ETL, there are a few common pitfalls that you need to be aware of.
Probably the most important one is the notion of indexes. Unlike RavenDB, relational databases usually don't learn from experience and require that you'll
define indexes explicitly. You should define indexes on the relevant tables on at least the column that is used to hold the document id.

Whenever RavenDB updates a document, it issues a set of `DELETE` statements for each of the relevant tables. For example, such a `DELETE` statement might look 
like the code in Listing 8.

```{caption="The first step when updating a document is to delete its existing data" .sql}
DELETE FROM OrderLines WHERE OrderId = 'orders/830-A'; 
DELETE FROM Orders WHERE OrderId = 'orders/830-A';
```

As you can imagine, during normal operations such statements are sent very frequently. That means that the relational database is going to be doing a lot of 
queries on the `OrderId` field (which is the one RavenDB is using to store the document id). If the field in question isn't indexed, that can cause a _lot_
of table scans, negatively impact the performance of the relational server and slow down the ETL process.

> **Append only systems**
>
> Sometimes you have a system that is built to never delete data. In some cases, you are _required_ to never delete data by a regulator. In such cases you can
> configure the SQL ETL process to use inserts only. This is configurable on a per table basis and can also be a peformance boost in some cases because we can
> directly insert the data without first running a set of `DELETE` statements to clean up the previous incarnation of the document we are sending over.

If you'll look at the tables creation script in Listing 8.6 you'll note that we explicitly handled that. In the `Orders` table, the `OrderId` field is marked
as the primary key (which is clustered by default on SQL Server). This means that searches on that field are going to be very fast. With the `OrderLines` table
the situation is a bit more complex. 

Here we can't use the `OrderId` as a primary key, because a single order has multiple lines. We define a throw away primary key using `identity`, but we mark
it as `NONCLUSTERED` and define a clustered indexed on the `OrderId`. This kind of setup is ideal for the RavenDB ETL process. All deletes and writes from 
RavenDB will always use a clustered index and it minimize the amount of operations that the relational database needs to perform.

#### Errors and troubleshooting

A usual, we cannot close a topic without discussing the error handling strategy. Just like the RavenDB ETL process, the SQL ETL process is a task at the cluster
level that is always assigned to a node. If the node in question fails, the cluster will assign the task to another node, which will proceed from the same place
the previous node stopped at.

There are two more general classes of issues that needs addressing. The first of which is when the relational database cannot be accessed for whatever reason.
If the relational database is down, too slow to respond or in accessible, that would halt the entire ETL process and cause RavenDB to keep retrying the process
until we have successfully connected to the relational database.

> **Handling deletes in ETL**
>
> In both RavenDB ETL and SQL ETL, dealing with documents changes is actually relatively easy. What is more complex is handling _deletes_. This is because we
> need to propogate the delete to the other side, but the delete is an absence of something, so how do we know what isn't there?
> 
> RavenDB needs to deal with this problem in many different forms. It is relevant for replciation between databases (both inside the same cluster and external
> replication between clusters), for ETL processes, for indexes and even for backups. In all cases, this is handled via the tombstone concept. A tombstone is
> an entry that marks the absence of a document. 
> 
> Usually you don't care about them, it is an internal implementation detail. For ETL, what you need to know is that a deletion of a document will trigger
> the ETL process as usual and it will remove the matching records on the destination side. 

Another type of error if when we fail to complete the ETL process for a single statement. For example, in Listing 8.5 we are sending a `Name` field that is 
composed of the `FirstName` and `LastName` of an employee. If the length of the employee's name in question will exceed 50 characters (as defined in the 
table creation script in Listing 8.4) then we'll not be able to process that document. In such a case, when there is an error that pertains only to a particular
document, RavenDB will _proceed_ with the ETL process, rather than halting it entirely. 

An alert will be raised and the admin will need to resolve the issue, either by limiting the size of the name we send or by extending the size of the column on
the relational table. The example of the column length is a simple one and illustrate the issue quite nicely. There is a non recoverable error in the document
that prevent us from completing the ETL process for that particular document but doesn't halt it for other documents. Other examples include a violations of 
not null constraints, violations of foreign keys, etc. 

Because they don't halt the ETL process, it can be easy to miss the fact that some documents haven't been sent over. RavenDB generates an alert when such an 
error occurs. You can see such an error by going to one of the `Employees` documents and increasing the size of the `FirstName` field until it is over 50
charcaters and saving. RavenDB will attempt to send the update over but fail and you'll get the error shown in Figure 8.11.

![Adding additional tables to the SQL ETL process](./Ch08/img11.png)

The error handling strategy RavenDB uses in such a scenario is based on the assumption that as long as we are making progress, it is better to alert than to 
fail completely. But if the nubmer of errors exceed a certain threshold, we consider the entire ETL process to be failing and stop processing it until an
admin tells us otherwise. The reasoning behind this behavior is that we want to ensure proper progress, but we don't want to limp along if we fail for all
the documents that we try to send. 

### Summary

In this chapter we went over the ETL processes supported by RavenDB. An ETL process allows you to take the documents residing in RavenDB and send them over
to another location. This can be another RavenDB node or even a relational database. Along the way, you have the chance to mutate and filter the data, which
opens up a lot of avenues for interesting use cases.

We looked a few such examples. Relying on RavenDB ETL to distribute a set of records to related services so they'll have their own copy of the data and can
access it without making a cross service call. In this case, the ETL process is part and parcel of the service public interface, and needs to be treated as 
such. Another option is to provide partial views of the data. It can be to give developers access to production grade data without exposing them to details that
should remain private, for example. 
We also looked at the reversed example, seeing how we can send data from multiple locations (the point of sales systems) to a central server that will aggregate
all that data. 

RavenDB also supports automatic ETL process to a relational database. You can use scripts to decide how RavenDB will transform the document model into the 
table model, including non trivial logic. This allows you to use RavenDB in a number of interesting ways:

- As an OLTP database while your reports are handled by the ETL target
- As a cache, for both reads and write, with RavenDB sending updated to the backend relational database.
- As part of a migration strategy, were you move different segments of your applications to RavenDB while others continue to operate with no change.

We looked at some of the details of the ETL processes. How they handle failure and recovery and what kind of response you should expect from them. With ETL
processes between RavenDB instances, you get failover on both the source destination, with each cluster able to route the ETL process to the right location
even if some of the nodes have failed. With SQL ETL, a RavenDB node going down will cause the cluster to move the task to another node while a problem with
the relational database will halt the ETL process until the remote database is up and running.

RavenDB employes a sophisticated error handling strategy with regards to errors writing to relational databases, trying to figure out whatever a particular
error is trasient and relevant for a single record or should impact the entire ETL process. In the first case, we'll alert about the error and continue
forward while in the later, after alerting, we'll stop the ETL process until an admin has resolved the issue.

We looked into some of the more common options for utilizing ETL processes in your system and I tried giving you a taste of the kind of deployments and 
topologies that are involved. It is important to note that this is just to give you some initial ideas. The ETL features are quiet powerful and they can
be used to great affect in your environment. I encourage you to think about them not just as a feature to be used in the tactical sense but how they play in 
the grand architecture of your system.

In the next part of the book we are going to start on an exciting topic, queries and indexes in RavenDB. This has been a long time in coming and I'm almost
more excited than you are at this point.