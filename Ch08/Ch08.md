
## Sharing data and making friends with ETL

[Sharing data and making friends with ETL]:(#integrations)

While talking about distributed work with RavenDB so far we have focused primarily on the work that RavenDB is doing to replicate data between different nodes
in the same cluster as part of the same database group or between different database in potentially different clusters. This mode of operation is simple, 
because you don't just setup the replication and RavenDB will take care of everything else. But there are other modes for distributing data in your systems.

Replication assumes that you have another RavenDB instance and that you want to replicate _everything_ to it. When replicating information, that is what you
want, but we also have a need to share a _part_ of the data. ETL^[Extract, Transform, Load] is the process in which we take data that resides in RavenDB
and push it to an exteranal source, potentionally transforming it along the way. That exteranal source can be another RavenDB instance or a relational database. 

Consider a micro service architecture and the Customer Benefits service. This service decides what kind of benefits the customer has. This can be anything
from free shipping to giving a discount on every 3rd jug of milk and the logic can be as simple as "this customer is in our membership club" to complex as 
trying to compute airline miles. Regardless of how the Customers Benefits service works, it need to let other parts of the system know about the 
benefits that this customer has. The Shipping service, the Helpdesk service and many others need to have that information. 

At the same time, we _really_ don't want them to poke their hands into the Customers Benefits database (or worse, have a shared database for everything)^[Doing
so is a great way to ensure that you'll have all the costs of a micro service architecture with none of the benefits.]. We could design an API between the 
systems, but then the Shipping service will be dependent on the Customer Benefits service to always be up. A better solution is to define an ETL process between
the two services and have the Customer Benefits service publish updates for the Shipping service to consume. Those updates are part of the publiccontract of
those services, mind. You shouldn't just copy the data between the databases.

Another example is the reporting database, RavenDB is a wonderful database for OTLP scenarios, but for reporting, your organization likely already have a 
solution, and there is really little need to replace that. But you can't just dump the data from RavenDB directly into a relational database and expect things
to work. We need to transform the data as we send it to match the relational model.

For all of those needs, RavenDB has the notion of ETL processes. RavenDB has builtin ETL to another RavenDB instance and to a relational database 
(such as MS SQL Server, Postgres, Oracle, MySQL, etc). 
Because RavenDB has native relational ETL, brown field systems will typically start using RavenDB by replacing a single component at a time, RavenDB is used
to speed up the behavior of high value targets, but instead of replacing the whole system, we use ETL processes to write the data to the existing relational
database. We'll cover that later in this chapter, discussing the deployment of RavenDB as a write-behind cache.

In most cases, the rest of the system doesn't even need to know that some parts are using RavenDB. This is using RavenDB as the write behind cache.
Some part of your application reads and writes to RavenDB directly, and RavenDB will make sure to update the relational system. 
We'll start talking about ETL processes between RavenDB instances, because we explore the whole ETL process without introducing another database instance.


### ETL processes to another RavenDB instance

Any non trivial system is going to have at least a few ETL processes and RavenDB has a good story on how to handle that. The simplest ETL process between
<<<<<<< HEAD
two RavenDB nodes is telling RavenDB that we want to send just a single collection to the other side. We first need to configure the connection string
we'll use. Because RavenDB ETL work between clusters, we may need to define multiple URLs, and it is easier to put it all in a single location. 
=======
two RavenDB nodes is telling RavenDB that we want to send just a single collection to the other side, without any transformations. Let us see how we can get
that working, shall we?

In the studio, go to `Settings`, `Manage Ongoing Tasks` and click the `Add Task` button and then select `RavenDB ETL`. You can see how this looks like 
in Figure 8.1
>>>>>>> cf8dd3aa3ef34be6f44c37e57b7d328b9caf82be

Go to `Settings` and then to `Connection Strings` and create a new RavenDB connection string. The easiest is to use create a new database in your cluster
and setup the connection string to it. As you can see in Listing 8.1, I've defined a connection string to the `helpdesk` database in the live test instance.

![Defining a connection string to another RavenDB instance.](./Ch08/img01.png)

With the connection string defined, we can now go ahead and build the actual ETL process to the remote instance. Go to `Settings` and then to 
`Manage Ongoing Tasks` and click the `Add Task` button and then select `RavenDB ETL`. You can see how this looks like in Figure 8.2.
Give it a name and select the previously defined connection string.

> **What about security?**
>
> We'll cover security in depth in the ["Securing your Ravens"](#security) chapter, but given that we have shown a connection string I wanted
> to address a not so minor issue that you probably noticed. We don't have a place here to define credentials. This is because we are talking
> to another RavenDB server, and for that we use x509 certificates. 
>
> In practice, this means that inside the same cluster ETL processes don't need any special configuration (nodes within the same cluster 
> already trust one another). Outside the cluster, you'll need to register the cluster's certificate with the remote cluster to allow the ETL
> process to work. You can read the full details of that in the security chapter.

![Defining an ETL process to another RavenDB instance](./Ch08/img02.png)

Now we need to let RavenDB know what will be ETL'ed to the other side. We do that by defining scripts that control the ETL proceess. Click on the 
`Add New Script` and give it a name such as "Employees to Helpdesk" and then select the Employees collection below, click on `Add Collection` and
then on `Save`. The result should look like Figure 8.3.

![Defining a simple "copy the whole collection" ETL script](./Ch08/img03.png)

Because we didn't specify anything in the ETL script, RavenDB will just copy the entire collection as is to the other side. This is useful on its own
because it allow us to share a single (or a few) collections with other parties with very little work. 

> **ETL is a database task, with bidirectional failover**
>
> In the previous chapter, we learned about database tasks and how the cluster will distribute such work among the different database instances. If a node
> fails, then the ETL task responsability will be assigned to another node. 
>
> It is important to note that in cases where we replicate to another RavenDB instance, we also have failvoer on the recieving end. Instead of specifying a 
> single URL, we can specify all the nodes in the cluster, and if one of the destination node is down, RavenDB will just run the ETL process against another
> node in the databse group topology.

It is _very_ important to remember that ETL is very different from replication. When a RavenDB node perform ETL to another, it is not replicating the data, it 
is _writing_ it. In other words, there are no conflicts and no attempt to handle such. Instead, we'll always overwrite whatever exists on the other side. 
As far as the destination node is concerned, the ETL process is just another client writing data to the database. This is done because the data is _not_
the same. This is important because of a concept that we didn't touch so far, data ownership. 

> **Data ownership in distributed systems** 
>
> One of the key differences between a centralized system and a distributed system is the fact that in a distributed environment, different parts of the system
> can act on their local information without coordination from other parts of the system. In a centralized system, you can take a look or use transactions to
> ensure consistency. But in a distributed system, that is not possible. Because of this limitation, the concept of data ownership is very important.
> 
> Ideally, you want every piece of data to have a single well defined owner and only mutate that data through that owner. That allows you to put all the 
> validation and business roles in a single place and ensure overall consistency. Everything else in the system will get updates to that data through its 
> owner. 
>
> A database group, for example, needs to handle the issue of data ownership. Conceptually, it is the database group as a whole that owns the data stored in 
> the document. However, difference database instances can mutate the data. RavenDB handles that using change vectors and conflict detection and resolution.
> That works for replication inside the database group, because all the nodes in the group share the ownership of the data. But it doesn't work for ETL.
>
> The ETL source is the owner the data and it is distributing the updates made to its data to interested parties. Given that it is the owner, it is expected
> that it can just update it to the latest version it has. That means that if you made any modification to the ETl'ed data, they will be lost. Instead of 
> modifyingg the ETL'ed data directly, you should create a compantion document that _you_ own. In other words, for ETL'ed data, the rule is that you can look,
> but not touch.

So far we have only done ETL at the collection level, but we can also modify the data as it is going out. Let's see how we can do that, and why would we want
to do this. 

#### ETL Scripts

Sometimes you don't want to send a full document in the ETL process. Sometimes you want to filter them or modify their shape. This is quite important  
since ETL processes compose a part of your public interface. Instead of just sending your documents to remote destination willy nilly, you'll typically only
send data that you are interested in sharing, and in a well defined format.

Consider the employees we sent over the wire, we sent them as is, potentially exposing our own internal document structure and making it harder to modify in 
the future. Let us create a new ETL process which will send just the relevant details and add a script for sending redacted employee information over the wire. 
You can see how this looks like in Figure 8.4.

![Using ETL with a script to redact the results](./Ch08/img04.png)

It is important to understand that when we are using an ETL script to modify what is sent, we need to take into account that RavenDB will send just what we told
it to. This seems obvious, but it can catch people unaware. If you don't have a script, the data sent to the other side will include attachments and will go to 
the same collection as the source data.

However, if you chose to use the 

### Document ids in replication

#### Multiple items from a single document

### Working with attachments

### Working with revisions

// TODO: this isn't there yet

### Required indexes
