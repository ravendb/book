
# Batch Processing with Subscriptions

[Subscriptions]: #batch-processing


RavenDB needs to handle some very different usecases in its day to day operations. On the one hand, we have transaction oriented processing, which typically
touch a very small number of documents as part of processing a single request. And on the other hand, we have batch processes, which usually operate on 
very large amount of data. Impleenting those kind of processes with the OLTP mode is possible, but it isn't really easy to do.

RavenDB supports a dedicated batch processing mode, using the notion of subscriptions. A subscription is simple a way to register a certain criteria on the
database and have the database send us all the documents that match this criteria. For example, "gimme all the support calls". So far, this sounds very much
like the streaming feature, which we covered in the previous chapter. However, subscriptions are _meant_ for batch processing. Instead of just getting all 
the data that match the criteria, the idea is that we'll keep the subscription open forever.

The first stage in the subscription is to send you all the existing data in the data that matches your query. Depending on your data size, this can take a 
while. This is done in batches, and RavenDB will only proceed to send the next batch when the current one has already been acknowledge as successuflly
processed. 

The second stage in the lifetime of a subscription is when it is done going through all the data that is already in the database. This is where things 
start to get really interesting. At this point, the subscription _isn't_ done. Instead, it is kept alive and wait around for a doucment that matches
it criteria to arrive. When that happens, the server will immediately send that document to the subscription. In other words, not only did we get batch
processing, but in many cases, we have _live_ batch processes. 

Instead of waiting for a nightly process to run, you can keep the subscription open and handle changes as they come in. This keep you from having to do
polling, remember the last items that you read, etc. RavenDB's Subscription also include error handling, recovery and retries, on the fly updates, etc. 

Working with subscriptions is divided into two distinct operations. First we need to create a thesubscription and then open it. Subscriptions aren't like
queries, they aren't ephemeral. A subscription is a persistent state of a particular business process. It indicates what documents that particular subscription
have processed, the criteria it is using, etc. 

In Listing 4.32 you can see how we create a subscription to process all the `Customer` documents in the database. 

```{caption="Creating a subscription to process customers" .cs}
var options = new SubscriptionCreationOptions<Customer>();
string subsId = store.Subscriptions.Create(options);
```

The result of the `Create` call in Listing 5.1 is a subscription id, which we can pass to the `Open` call. Listing 5.2 shows how we open the subscription and register our batch handling routine. 

```{caption="Opening and using a subscription to process customers" .cs}
var options = new SubscriptionConnectionOptions(subsId);
using(var subscription = store.Subscriptions.Open<Customer>(options)))
{
	// wait until the subscription is done
	// typically, a subscription lasts for very long time
	await subscription.Run(batch => /* redacted */ );
}
```

There isn't much in Listing 5.2. We open a subscription using the previous gotten `subsId`, then we `Run` the subscription, I redacted the actual batch processing
for now, we'll see what it is in the nexte section. After calling `Run` we wait on the returned task, why do we do that?
In general, a subscription will live for a very long time, typically, the lifetime of the process that is running it. In fact, typically you'll have a process
dedicated just to running subscriptions (or even a process per subscription). What we are doing in Listing 5.2 is basically wait until the subscription will exit.

However, there are very few reasons that a subscription _will_ exit. Problems with the network will simply cause it to retry or failover to another node, for
example. And if it processed all the data that the ddatabase had to offer, it will simply sit there and wait until it has something more to give it. However,
there are a few cases where the subscirption will exit. First, of course, if you want to close it by disposing the subscription. It is safe to dispose the 
subscription while it is running, and that is how you typically do orderly shutdown of a subscription. 


	// TODO: Image of subscription connections in the studio

Next, we might have been kick off the subscription for
some reason. The admin may have deleted the subscription or database, the credentials we use are invalid, or the subscription connection has been taken over
by another client. As you can see, typically the `Run` method will not just return (only if you manually disposed the subscription), it will typically throw 
when there is no way for it to recover. We'll cover more on this in the subpscription deployment section later on in this chapter. 

The actual batch processing of the documents is done by the lambda that is passed to the `Run` call  on a background thread. In Listing 4.33 we used the `subsId` 
from Listing 4.32. This is typical to how we are using subscriptions. The subscription id should be persistent and survive restat of the process or machine, 
becuase it is used to represent the state of the subscription and what doucments have already been processed by that particular subscription.

You can set your own subscription id during the `Create` call, which give you well known subscription name to use, or you can ask RavenDB to choose one for you, 
as we have done in Listing 4.32. Note that even if you use a hard coded subscription id, it still needs to be created before you can call `Open` on it. 
Why do we have all those moving parts? We have the creation of the subscription, open it, run it and wait on the resulting task and we haven't even gotten to
the part where we are actually do something using it. 

The reason for this is that subscriptions are very long lived processes, which are resilient to failure. Once a subscription is created, a client will open it 
and then keep a connection open to the server, getting fed all the documents that match the subscription criteria. This is true for all existing data in the
database. 

Once we have gone over all the documents currently in the database, the subscription will go to sleep, but will remain connected to the server. Whenever a new
or updated document will match the subscription criteria, it will be sent again. Errors during this process, either in the network, the server or the client are
tolerated and recoverable. Subscriptions will ensure that a client will recieve each matching document at least once^[Although errors may cause you to recieve 
the same document multiple times, you are guranteed to never miss a document]. 

> **Subscription in a cluster**
>
> The subscription will connect to _a_ server in the cluster, which may redirect the subscription to a more suitable server for that particular subscription. 
> Once the subscription found the appropriate server, it will open the subscription on that server and start getting documents from it. A failure of the client 
> will result in a retry (either from the same client or possibly another one that was waiting to take over). A failure of the server will cause the client to 
> transparently switch over to another server. 
>
> The entire process is highly available on both client and server. The idea is that once you setup your subscriptions, you just need to make sure that the
> processes that open and process the subscription is running, and the entire system will hum along, automatically recovering from any failures along the way.

Typically, on a subscription that already processed all existing documents in the database, the lag time between a new document coming in and the subscription 
recieving it is a few milliseconds. Under load, when there are many such documents, we'll batch documents and send them to the client for processing
as fast as it can process them. The entire model of subscription is based on the notion of batch processing. While it is true that subscriptions can remain
up constantly and get fed all the changes in the database as they come, that doesn't have to be the case. If a subscription is not opened, it isn't going to 
miss anything. Once it has been opened, it will get all the documents that have changed when it was gone.

This allow you to build business processes that can either run continiously or to run them at off peak times in your system. Your code doesn't change, nor does
it matter to RavenDB (a subscription that isn't opened consume no resources). In fact, a good administrator will know that it can reduce the system load by 
shutting down subscriptions that are handling non time critical information with the knowledge that once the load has passed, starting them up again will allow
them to catch from there last processed batch.

This is done by having the client acknowledge to the server that it successfully completed a batch once it has done so, at which point the next batch
will be sent. It is the process of acknowledging the processing of the batch that make the process reliable. Until the client confirmed reciept of the batch,
we'll not move forward and send the next one. 

Okay, that is about as much as we can talk about subscriptions without actually showing what they are _doing_. Let us go to handle the actual document processing.

## Processing a batch of documents in a subscriptions.

We previously seen the `Run` method, in Listing 5.2. But we haven't seen yet what is actually going on there. The `Run` method is simply taking a lambda that will
go over the batch of documents. Listing 5.3 shows the code to handle the subscription that we redacted from Listing 5.2. 

```{caption="Processing customers via subscription" .cs}
await subscription.Run( batch =>
{
	foreach(var item in batch.Items)
	{
		Customer customer = item.Result;
		// do something with this customer
	}
});
```

After all this build up, the actual code in Listing 5.3 is pretty boring. The lambda we sent get a batch instance, which has a list of `Items` that are contained
in this batch. And on each of those items, we have a `Result` property that contains the actual document instance that we were sent from the server. 
This code will first get batches of all the customers in the database. Once we have gone through all the customer documents, this subscription will wait, and 
whenever a new customer comes in, or an existing customer is modifed, we'll have a new batch with that document. If there are a lot of writes, we might get 
batches that will contain several documents that were changed in the time it took us to complete the last batch.

What can we do with this? Well, quite a lot, as it turns out. We can use this to run all sort of business processes. For example, we may want to check if this
customer have a valid address, and if so, record the GPS coordinates so we can run spatial queries on it. Because of the way subscriptions work, we get a full
batch of documents from the server, and we can run heavy processing on them, we aren't limited by the data streaming over the network, unlike streaming, we won't 
run out of time here. As long as the client remains connected, the server will be happy to keep waiting for batch to complete. Note that the server will ping
the client every now and then to see what its state is, to detect client disconnection at the network level. If that is detected, the connection on the server
will be aborted and all resources will be released.

> **Subscriptions are background tasks**
>
> It may be obvious, but I wanted to state this explicitly, subscriptions are background tasks for the server. There is no requirement that a subscription 
> will be opened at any given point in time, and a subscription that wasn't opened will simply get all the documents that it needs to since the last 
> acknowledged batch.
> 
> That means that if a document was modified multiple times, it is possible that the subscription will only be called upon it once. See the section about
> Versioned Subscriptions if you care about this scenario.

One of the things we can do here is to open a new session, modify the document we got from the subscription, `Store` the document and call `SaveChanges` on it, 
right from the subscription lambda itself. But note that doing so will also typically put that document right back on the path to be called again with this 
subscription, so you need to be aware of that and protect against infinite loops like that. There are also a few other subtle issues that we need to handle 
with regards to running in a cluster and failover, we'll discuss those issues later in this chapter. 

## The Subscription Script

Subscriptions so far are useful, but not really something to get excited about. But the fun part starts now. Subscirptions aren't limited to just fetching all
the documents in a particular collection. We can do much better than this. Let us say that we want to send a survey for all customers where we had a complex
support call. The first step for that is to created a subscription using the code in Listing 5.4.

```{caption="Creating a subscription for complex calls" .cs}
var options = new SubscriptionCreationOptions<SupportCall>(
		call => 
			call.Comments.Count > 25 && 
			call.Votes > 10 		 && 
			call.Survey == false
	);
string subsId = store.Subscriptions.Create(options);
```

We are registering for subscriptions on support calls that have more than 10 votes and over 25 comments, and we add a flag to denote that we already sent the
survey. It is important to note that this filtering is happening on the _server side_, not on the client. Internally we'll transform the conditional into a
JavaScript expression and send it to the server, to be evaluated on each doucment in turn. Any matching document will be sent for the client for processing.
Of course, this is just part of the work, we still need to handle the subscription itself. This is done in Listing 5.5.

```{caption="Taking surveys of complex calls" .cs}
await subscription.Run(batch =>
{
	foreach(var item in batch.Items)
	{
		SupportCall call = item.Document;
		
		var age = DateTime.Today - call.Started;
		if( age > DateTime.FromDays(14))
			return; // no need to send survey for old stuff

		// DON'T open a session from the store directly
		// using(var session = store.OpenSession())

		// INSTEAD, open a session from the batch
		using(var session = batch.OpenSession())
		{
			var customer = session.Load<Customer>(
				call.CustomerId);

			call.Survey = true;		

			session.Store(call, item.Etag, item.Id);

			try
			{
				session.SaveChanges();
			}
			catch(ConcurrenyException)
			{
				// will be retried by the subscription
				return;
			}	

			SendSurveyEmailTo(customer, call);
		}
	}

});
```

A lot of stuff is going on in Listing 5.5, including stuff that we need to explain _not_ to do. Even though we removed the code for actually opening the 
subscription (this is identical to Listing 5.4) there is still a lot going on. For each item in the batch, we'll create a new session load the customer for
this support call, then we mark the call as having sent the survey. Then we call `Store` and pass it not just the instance that we got from the subscription, 
but also the etag and id for this document. This ensures that when we call `SaveChanges`, if the document has changed in the meantime on the server side, we'll 
get an error.

> **Don't use the `store.OpenSession` in batch processing**
>
> The code in Listing 5.5 calls out the use of `store.OpenSession` as a bad idea. Why is that? This is where I need to skip ahead a bit and explain some 
> concepts that we haven't seen yet. When running in a cluster, RavenDB will divide the work between the various nodes in a database. That means that a
> subscription may run on node B while the cluster as a whole will consider node A as the preferred node to write data to. However, since the subscription
> is being served from node B, it means that the etag that we get from the subscription is also local to node B.
> 
> Remember our discussion on etags in [Document Modeling](#modeling), an etag is local to the database where it was created. When using the `store.OpenSession`
> the returned session will use the preferred node on the cluster to write to. That means that when you call `SaveChanges`, even though you read the
> value from node B, the `SaveChanges` call will go to node A. Since you are using optimistic concurrency and using the etag from node B, you are pretty
> much guranteed to always hit a concurrency exception. 
>
> In order to avoid that, you should use `batch.OpenSession` (or `batch.OpenAsyncSession`) to create the session. This will ensure that the session you have
> created will operate against the same node that you are reading from and thus allow us to use optimistic concurrency properly. 

In Listing 5.5, the concurrency exception is expected error, we can just ignore it and skip processing this document. There is a bit of a 
trickery involved here. Because the document have changed, the subscription will get it again anyway, so we'll skip sending an email about this call now, but 
we'll be sending the emaial later, when we run into the support call again.

Finally, we send the actual email. Note that in real production code, there is also the need to decide what to do if sending the email failed. In this case, 
the code is assuming that it cannot fail, and favor skipping sending the email rather then sending it twice. Typical mail systems have options to ignore 
duplicate emails in a certain time period, which is probably how you would solve this in production.

Instead of using explicit concurrency handling, you can also write the code in Listing 5.5 using a `Patch` command, as you can see in Listing 5.6.

```{caption="Taking surveys of complex calls, using patches" .cs}
await subscription.Run( batch =>
{
	foreach(var item in batch.Items)
	{
		SupportCall call = item.Document;
		var age = DateTime.Today - call.Started;
		if( age > DateTime.FromDays(14))
			return; // no need to send survey for old stuff

		using(var session = batch.OpenSession())
		{
			var customer = session.Load<Customer>(
				call.CustomerId);

			session.Advanced.Patch<SupportCall, bool>(
				result.Id,
				c => c.Survey,
				true);

			SendSurveyEmailTo(customer, call);

			session.SaveChanges();
		}
	}
});
```

In Listing 5.6 we are doing pretty much the same thing we did in Listing 5.5. The difference is that we are using a `Patch` command to do so, 
which saves us from having to check for concurrency violations. Even if the document have changed between the time the server has sent it to us
we will only set the `Survey` field on it. In Listing 5.6 we are also sending the survey email _before_ we set the `Survey` flag, so a failure
to send the email will be throw all the way to the calling code, which will typically retry the subscription. This is different from the code in
Listing 5.5, where we first set the flag and then send the email.

The main difference here is in what happens in the case of an error being raised when sending the survey email. In Listing 5.5, we have already
set the flag and sent it to the server, so the error will mean that we didn't send the email. The subscription will retry, of course, but the
document was already changed and will be filtered from us. In Listing 5.5, if there was an error in sending email, the email will not be sent.

> **Subscribing to documents on the database we are writing to**
> 
> There are a few things to rembmer if you are using the subscription to write back to the same database you are subscribing to:
>
> * Avoiding subscription/modify loop. When you modify a document that you are subscribing to, the server with send it to 
>   the subscription again. If you'll modify it every time that it is processed, you'll effectively create an infinite loop, with all the costs
>   that this entails. You can see in Listing 5.5 and Listing 5.6 as well that we are careful to avoid this by setting the `Survey` flag when we 
>   processed a document and filtering on that flag in Listing 5.4.
>
> * The document you got may have already been changed on the server. Typically, the lag time of between a document being modified and the 
>   subscription processing that document is very short. That can lead you to think that this happens instantaneously or even worse, as part of 
>   the same operation of modifying the document.
>   Nothing could be further from the truth. A document may be changed between the time the server has sent you the document and the time you 
>   finished processing and saving it. In Listing 5.5 we handled that explicitly using optimistic concurrency and in Listing 5.6 we used 
>   patching to avoid having to deal with the issue. 
>
> * If you are using subscriptions to integrate with other pieces of your infrastructure (such as sending emails, for example), you have to be
>   ready for failure on that end and have some meaningful strategy for handling it. Your options are to either propogate the error up the chain,
>   which will force the subscription to close (and retry from last susccessful batch) or you can catch the exception and handle it in some manner.

On the other hand, in Listing 5.6, we first send the email, then set the flag and save it. This means that if there is an error sending the email
we'll retry the document later on. However, if we had an error saving the flag to the server and we already sent the email, we might send the email
twice. You need to consider what scenario you are trying to prevent, double email send or no email sent at all.

Instead of relying on two phase commit and distributed transactions, a much better alternative is to use the facilities of each system 
on its own. That topic goes beyond the scope of this book, but idempotent operations or de-duplication for operations can both give you a 
safe path to follow in the precense of errors in a distributed system. If the email system will recognize that this email has already been 
sent, the code in Listing 5.6 will have no issue. We'll never skip sending an email and we'll never send the same email twice. 

> **Distributed transactions and RavendB**
>
> The main reason that we have to face this issue is that we are forced to integrate between two systems that do not share a transaction boundary.
> In other words, theoretically speaking, if we could share a transaction between the email sending and the write to RavenDB, the problem would be
> solved. 
>
> In practice, RavenDB had support for distributed transactions with multiple resources up until version 3.x, when we deprecated this feature and 
> version 4.0 removed it completely. Distributed transactiosn (also known as two phase commit or 2PC) _sound_ wonderful. Here you have a complex
> interaction between several different components in your system, and you can use a transaction to orchestrate it all in a nice and simple 
> manner. 
>
> Except it doesn't work like this. Any distributed transaction system that I had worked with had issues related to failure handling and partial
> success. A distributed transaction coordinator basically require all praticipants in the transaction to promise it that if it tells them to
> commit the transaction, it will be successful. In fact, the way a coordinator usually work is by having one round of promises, and if all
> participants have been able to make that promise a second round with confirmations. Hence, the two phase commit name.
>
> The problem starts when you have gotten a promise from all the participants, you already confirmed with a few of them that the transaction
> has been committed, and one of the particpants fail to commit for whatever reason (hardware failure, for example). In that case, the 
> transaction is in a funny, half committed state. 
>
> The coordinator will tell you that this is a bug in the participant, that it shouldn't have made a promise that it couldn't keep. And typically
> coordinators will retry such transactions (manually or automatically) and recover from trasients errors. But the problem with "it is an issue
> with this particular particpant, not the coordinator" line of thinking is that those kind of errors are happening in production. 
>
> In one particular project, we had to restart the coordinator and manually resolve hanging transactions on a bi-weekly basis, and it wasn't a 
> very large or busy website. [Joe Armstrong](https://en.wikipedia.org/wiki/Joe_Armstrong_(programming) ), Inventor of Erlang,
> described^[That particular lecture was over a decade ago, and I still vividly remember it, it was _that_ good.] the problem far better than I 
> could:
>
> > The Two Generals' Problem is reality, but the computer industry says, it doesn't believe 
> > in mathematics: Two phase commit^[There is also the _three_ phase commit, which just add to the fun and doesn't actually solve the issue.] 
> > always works!

There is another issue with the code in Listing 5.5 and Listing 5.6. They are incredibly wasteful in the number of remote calls that they are making. One of 
the key benefits of using batch processing is the fact that we can handle things, well, in a batch. However, both Listing 5.5 and Listing 5.6 will create a
session per document in the batch. The default batch size (assuming we have enough documents to send to fill a batch) is in the order of 4,096 items. That 
means that if we have a full batch, the code in either one of the previous listings will generate 8,192 remote calls. That is a _lot_ of work to send to the
server, all of which is handled in a serial fashion.

We can take advantage of the batch nature of subscriptions to do _much_ better. Turn you attention to Listing 5.7. 

```{caption="Efficeintly process the batch" .cs}
await subscription.Run( batch =>
{
	using(var session = batch.OpenSession())
	{
		var customerIds = batch.Items
			.Select(item=> item.Result.CustomerId)
			.Distinct()
			.ToList();
		// force load of all the customers in the batch
		// in a single request to the server 
		session.Load<Customer>(customerIds);

		foreach(var item in batch.Items)
		{
			SupportCall call = item.Document;
			var age = DateTime.Today - call.Started;
			if( age > DateTime.FromDays(14))
				return; // no need to send survey for old stuff

			// customer was already loaded into the session
			// no remote call will be made
			var customer = session.Load<Customer>(
				call.CustomerId);

			// register the change on the session
			// no remote call will be made
			session.Advanced.Patch<SupportCall, bool>(
				result.Id,
				c => c.Survey,
				true);

			SendSurveyEmailTo(customer, call);

		}

		// send a single request with all the 
		// changes registered on the seession
		session.SaveChanges();
	}
});
```

Listing 5.7 and Listing 5.6 are functionally identical. They have the same exact behavior, except that Listing 5.6 will generate 8,192 requests, and Listing 5.7
will generate just 2. Yep, the code in Listing 5.7 is always going to generate just two requests. First a bulk load of all the customers in the batch and then
a single `SaveChanges` with all the changes for all the support calls in the batch.

Note that we are relying on the `Unit of Work` nature of the session. Once we loaded a document into it, trying to load it again will give us the already loaded
version without going to the server. Without this feature, the amount of calls to `Load` would have probably forced us over the budget of remote calls allowed
for the session^[Remember, that budget is configurable, but it is there mostly to help you realize that generate so many requests is probably not healthy for 
you].

Listing 5.7 takes full advantage of the batch nature of subscriptions, in fact, the whole reason why the batch expose a `List` property instead of just being
an enumerable is to allow you to make those kinds of optimizations. By making it obvious that scanning the list of items per batch is effectively free, we are 
left with the option of traversing it multiple times and optimizing our behavior.

### Complex conditionals 

We already saw how we can create a subscription that filter documents on the server side, in Listing 5.4. The code there used a Linq expression and 
the client API was able to turn that into a JavaScript function that was sent to the server. Listing 5.4 was a pretty simple expression, but the code
that handles the translation between Linq expressions and JavaScript is quite smart and is able to handle much more complex conditions.

However, putting a complex condition in a single Linq expression is not a good recipe for good code. A better alternative is to skip the convienance of 
the Linq expression and go directly to the JavaScript. In Listing 5.8, we are going to see how we can subscribe to all the support calls that require 
special attention.

```{caption="Creating a subscription using JavaScript filtering" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	var watchwords = ['annoy', 'hard', 'silly'];

	var lastIndex = this['@metadata']['Last-Monitored-Index'] || 0;

	for(var i = lastIndex; i < this.Comments.length; i++)
	{
		for(var j = 0; j < watchwords.length; j++)
		{
			var comment = this.Comments[i].toLowerCase();
			if(comment.indexOf(watchowrd[i]) != -1)
				return true;
		}
	}

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

The interesting code in Listing 5.8 is in the `Script` property. We are defining a few words that we will watch for, and if we see them in the comments 
of a support call, we want to give it some special attention via this subscription. We that by simply scanning through the array of `Comments` and checking
if any of the comments contains any of the words that we are looking for.

The one interesting tidbit is the use of `this['@metadata']['Last-Monitored-Index']`, what is that for? Remember that a subscription will be sent all the 
documents that match its criteria. And whenever a document is changed, it will be chcked if it match this subscription. That means that if we didn't have
some sort of check to stop it, our subscription will process any support call that had a comment with one of the words we watch for _every single time
that call is processed_. 

In order to avoid that scenario, we set a metadata value named `Last-Monitored-Index` when we process the subscription. You can see how that work in
Listing 5.9.

```{caption="Escalating problematic calls" .cs}
await subscription.Run( batch =>
{
	const string script = @"
var existing = this['@metadata']['Last-Monitored-Index'] || 0;
this['@metadata']['Last-Monitored-Index'] = Math.max(idx, existing);
";
	using(var session = batch.OpenSession())
	{
		foreach(var item in batch.Items)
		{
			// mark the last index that we
			// already observed using Patch
			session.Advanced.Defer(
				new PatchCommandData(
					id: item.Id,
					etag: null, 
					patch: new PatchRequest
					{
						Script = script,
						Values = 
						{
							["idx"] = item.Result.Comments.Count
						}
					},
					patchIfMissing: null, 
				);

			// actually escalate the call
		}

		session.SaveChanges();
	}
});
```

We are simply setting the `Last-Monitored-Index` to the size of the `Comments` on the call and saving it back to the server. This will ensure that we'll only get the support call again only if there are _new_ comments with the words that we are watching for. The code in Listing 5.9 is going out of its way to be a good
citizen and not go to the server any more times than it needs to. This is also a good chance to demonstrate a real usecase for using `Defer` in production code. The use of `Defer` means that we both don't need to worry about the number of calls and that we have handled concurrency. 

Assuming that we don't have a lot of calls that requires escalation, just having a session per batch item (and the associated number of requests) is likely good
enough. And if we _do_ have a higher number of escalated calls, we probably have other, more serious issues. 

> **Maintaining per document subscription state**
>
> Subscription often need to maintain some sort of state on a per document basis. In the case of Listing 5.9, we needed to keep track of the last 
> monitored index, but other times you'll have much more complex state to track. For example, imagine that we need to kick off a workflow that will
> escalate a call once it passed a certain threshold. We might need to keep track of the state of the workflow and have that be account for in the
> subscription itself.
>
> Using the metadata to do it works quite nicely if we are talking about small and simple state. However, as the complexity grows, it isn't viable
> to keep it all in the document metadata and we'll typically introduce a separate document to maintain the state of the subscription. In we are 
> tracking support calls, then for `SupportCalls/238-B` will have a `SupportCalls/238-B/EscalationState` document that will conatain the relevant
> information for the subscription.

Listing 5.8 and Listing 5.9 together show us how a subscription can perform rather complex operations and open up some really interesting options for 
processing documents. But even so, we aren't done, we can do even more with subscriptions.

### Complex scripts

We have used conditional subscriptions to filter the documents that we want to process, and since this filtering is happening on the server side, it 
allows us to reduce the number of documents that we have to send to the subscription. This is awesome, but a really interesting feature of 
subscriptions is that we don't actually _need_ to send the full documents to the client, we can select just the relevant details to send. 

We want to do some processing on highly voted support calls, but we don't need to get the full document, we just need the actual issue and the number
of votes for that call. Instead of sending the full document over the wire, we can use the code in Listing 5.10 to efficeintly.

```{caption="Getting just the relevant details in the subscription" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	if (this.Votes < 10)
		return;

	return { 
		Issue: this.Issue, 
		Votes: this.Votes 
	};

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

What we are doing in Listing 5.10 is to filter the support calls. If the call has less then 10 votes, we'll just return from the script. RavenDB consider
a `return` or `return false` or `return null` to be an indication that we want to skip this document, and will not send that to the client. On the other
hand, with `return true`, the document will be sent to the client. However, we aren't limited to just returning `true`. We can also return an object of our 
own. That object can be built by us and contain just the stuff that we want to send to the client. RavenDB will send that object directly to the client.

There is an issue here, however. In Listing 5.10, we are creating a subscription on `SupportCall`. However, the value that will be sent to the client for
the subscription to process is _not_ a `SupportCall` document. It is our own object that we created. That means that on the client side, we need to know 
how to handle that. This requires a bit of a change in how we open the subscription, as you can see in Listing 5.11.

```{caption="Opening subscription with a different target" .cs}
public class SupportCallSubscriptionOutput
{
	public string Issue;
	public int Votes;
}

var subscription = store.Subscriptions
	.Open<SupportCallSubscriptionResult>(options));

await subscription.Run(batch =>
{
	foreach(var item in batch.Items)
	{
		SupportCallSubscriptionResult result = item.Result;
		// do something with the 
		// result.Issue, result.Votes
	}
});
```

In order to consume the subscription in a type safe way, we create a class that matches the output that we'll get from the subscription script, and we'll 
use that when we open the subscription. As you can see, there isn't actually a requirement that the type that you use to `Create` the subscription and the
type you use for `Open` will be the same and scenarios like the one outlined in Listing 5.10 and 5.11 make it very useful.

If this was all we could do with the subscription script, it would have been very useful in reducing the amount of data that is sent over the wire, but 
there is actually more options available to us that we haven't gotten around to yet. Consider Listing 5.6, there we get the support call and immediately
have to load the associated customer. That can lead to a remote call per item in the batch. We have already gone over why this can be a very bad idea in
term of overall performance. Even with the optimization we implemented in Listing 5.7, there is still another remote call to do. We can do better.

We can ask RavenDB to handle that as part of the subscription processing directly. Take a look at Listing 5.12, which does just that.

```{caption="Getting just the relevant details in the subscription" .cs}
var options = new SubscriptionCreationOptions<SupportCall>
{
	Criteria = 
	{
		Script = @"

	if (this.Votes < 10)
		return;

	var customer = LoadDocument(this.CustomerId);

	return { 
		Issue: this.Issue, 
		Votes: this.Votes,
		Customer: {
			Name: customer.Name,
			Email: customer.Email
		}
	};

		"
	}
};
string subsId = store.Subscriptions.Create(options);
```

In Listing 5.12, we are calling `LoadDocument` as part of the processing of the subscription on the server side. This allow us to get the customer instance
and send pieces of it back to the client. In order to consume it, we'll need to change the `SupportCallSubscriptionOutput` class that we introduced in 
Listing 5.11 to add the new fields. 

When processing the output of this subscription, we don't need to make any othe remote call, and can directly process the results without making any remote
calls to load the assoicated document. Listing 5.13 you can see how we can process such a subscription.

```{caption="Opening subscription with a different target" .cs}
public class SupportCallSubscriptionOutput
{
	public class Customer
	{
		public string Name;
		public string Email;
	}
	public string Issue;
	public int Votes;
	public Customer Customer;
}

var subscription = store.Subscriptions
	.Open<SupportCallSubscriptionResult>(options));

await subscription.Run(batch =>
{
	foreach(var item in batch.Items)
	{
		SupportCallSubscriptionResult result = item.Result;
		SendEscalationEmail(result.Customer, item.Id);
		// other stuff related to processing the call
	}
});
```

You can see that we use an inner class to scope the meaning of the `Customer` here. This isn't required, it is merely a convention we use to bring some order
for the client side types. 

## Error handling and recover with subscriptions

What happens when there is an error in the processing of a document? Imagine that we had code inside the lambda in the `Run` method and that code threw an 
exception. Unless you set `SubscriptionConnectionOptions.IgnoreSubscriberErrors`^[And you probably shouldn't do that.], we will abort processing of the 
subscription and the `Run` will raise an error. Typical handling in that scenario is to dispose of the subscription and immediately open it again. 

Assuming the error is transient, we'll start processing from the last batch we got and continue forward from there. If the error isn't transient, for example,
some `NullReferenceException` because of a `null` the code didn't check for, the error will repeat itself. You might want to set an upper limit to the number
of errors you'll try to recover from in a given time period, and just fail completely afterward. This depends heavily on the kind of error reporting and 
recovery you are using in your applications. 

Note that this applies only to errors that came from the code processing the document. All other errors (connection to server, failover between servers, etc) 
are already handled by RavenDB. The reason that we abort the subscription in the case of subscriber error is that there really isn't any good way to recover
from it in a safe manner.
We don't want to skip processing the document, and just logging the error is possible (in fact, that is exactly what we do if `IgnoreSubscriberErrors` is set)
but no one ever reads the log until the problem was already discovered, which is typically very late in the game. 

In other words, RavenDB will take care of all the issues related to talking to the database but the error handling related to your code is on you. In 
practice, you generally don't have to worry about it. An error thrown during document processing will kill your subscription. We saw in Listing 5.3 that after 
we call to `Run`, we need to wait on the resulting task, if an error is raised during document processing, that error will close the subscription and the 
error will be raised to the caller of the `Run` method. 

The typical manner in which you will handle errors with subscriptions is just to retry the whole subscription, as shown in Listing 5.14. There is a lot of 
things going on in there, so take the time to carefully read through the error handling there. 

```{caption="Retrying subscription on error" .cs}
var errorTimings = new Queue<DateTime>();
while(true)
{
	var options = new SubscriptionConnectionOptions(subsId);
	var subscription = store.Subscriptions.Open<SupportCall>(
		options));

	try
	{
		await subscription.Run(/* redacted */);
		// Run will complete normally if you have disposed
		// the subscription
		return;
	}
	catch(Exception e)
	{
		Logger.Error("Failure in subscription " + subsId,e);

		if (e is DatabaseDoesNotExistException || 
			e is SubscriptionDoesNotExistException || 
			e is SubscriptionInvalidStateException || 
			e is AuthorizationException)
			throw; // not recoverable

		if (e is SubscriptionClosedException)
			// closed explicitly by admin, probably
			return;

		// handle this depending on subscription
		// open strategy (discussed later)
		if (e is SubscriptionInUseException)
			continue;

		
		// user code has thrown an exception
		// handle according to your policy
		
		errorTimings.Enqueue(DateTime.UtcNow);
		if(errorTimings.Count < 5)
			continue;

		var firstErrorTime = errorTimings.Dequeue();
		var timeSinceErr = DateTime.UtcNow - firstErrorTime;
		if (timeSinceErr < TimeSpan.FromMinutes(5))
			continue;

		// log subscription shut down for constant errors
		return;
	}
	finally
	{
		subscription.Dispose();
	}
}

```

Listing 5.14 shows a typical way to handle errors in subscriptions. For completion's sake, I included all the common error conditions that can be raised from
the `Run` method. The first few items involve non recovable scenarios. The database doesn't exists, the subscription doesn't exists or is misconfigured or the
credentials we have are invalid. There is no way to recover from those kinds of errors without administrator involvement, so we should just raise this up the stack until we catch the attention of someone who can actually fix this.

The next part handle errors when the subscription was closed explicitly by an administrator. RavenDB will automatically recover from failures and accidental
disconnects, but the administrator can also choose to explicitly kill a subscription connection. In this case, we'll report this to the caller, who can dedide
what to do about this. Just retrying is probably not a good in this case. A subscription can also fail because another client is already holding to the subscription, or a client came in and kicked our subscription from the subscription explicitly. Both of those cases are strongly tied to the deployment mode you have and will be discussed in the next section.

> **What about subscription script errors?**
>
> We talked a lot about what happens when there are error on the server side (automatically recover) on the client side (thrown to caller and handled by
> your code) etc. However, there is one class of errors that we didn't consider, what happen if there is an error in the JavaScript code that we use to
> evaluate each docuemnt. 
>
> Consider the following snippet: `return (10 / this.Votes) > 0`. It isn't meant to be meaningful, just to generate an error when the `Votes` property is
> set to zero. In this case, when evaluating the script on a support call with no votes, we'll get a error attempting to divide by zero. That error will
> be send to the client and be part of the batch.
>
> When the client access the `item.Result` property on that particular document, the exception will be raised on the client side. In this way, you can 
> select how to handle such errors. You can handle this on the client side in some manner and continue forward normally, or you can let it buble up as
> an error in the subscription, which should trigger your error handling policy as we saw in Listing 5.15.

All other exceptions would typically be from an exception raised by the batch processing code. In that case, we use the `errorTimings` queue to check if we 
got more than 5 errors in the space of the last 5 minutes. If we didn't, we continue normally and usually just this is enough to handle most transient errors. 
However, if we got more than 5 errors in the space of 5 minutes, we'll abort the subscription and typically alert an admin that something strange is going on.

The policy shown in Listing 5.14 (max 5 errors in 5 minutes) is a trivially simple one. I have seen production system that just kep blindly retrying 
and others that had far more sophisticated process for recovery using exponential backoff to try to avoid long running but eventually transient failure.
Exactly how you'll handle error recovery in your system is up to you and the operations teams that will maintain the application in production and it is tied
very closely to how you are using subscriptions and how you are deploying them.

## Subscription deployment patterns

Talking about _a_ subscription in isolation helps us understand how it works, but we also need to consider how subscriptions are deployed and manage in your
systems. Subscriptions are used to handle most batch processing needs in RavenDB, which puts them in interesting place regarding their use. On the one hand,
they are often performing critical business functions, but on the other, they aren't visible (a failing subscription will not cause an error page to be shown
to the user).

Subscriptions are usually deployed in either a batch & stop mode or in continious run. The continious run is a process (or a set of processes) that are running
your subscriptions constantly. The database will feed any new data to them as soon as it is able, and all processing happens live. That isn't required, mind. 
Under load, the administrator is perfectly able to shut down a subcription (or even all of them) to free up processing capacity. When they are started up again
they will pick up from where they left.

In batch mode, the subscription are run on some schedule and when they run out of work to do, they will shut themselves down until being run again. 
Listing 5.15 shows how you can write a self stopping subscription.

```{caption="Stop the subscription after 15 minutes of inactivity" .cs}
var options = new SubscriptionConnectionOptions(subsId);
using(var subscription = store
		.Subscriptions.Open<SupportCall>(options)))
{
	var waitForWork = new ManualResetEvent();
	var task = subscription.Run(batch =>
	{
		waitForWork.Set(); // we have work
		// process batch
	});

	while(waitForWork.Wait(TimeSpan.FromMinutes(15)))
	{
		waitForWork.Reset();
	}

	subscirption.Dispose(); // ask for disposal
	await task; // wait for it to complete
}

```

What we are doing in Listing 5.15 is pretty simple. Whenever we start processing a batch, we set an event, and in the calling code, we wait on that event.
If more than 15 minutes have passed since a batch arrived, we will dispose the subscription and exit. 

Another aspect of subscription deployment is the notion of high availability. On the server side, the cluster will ensure that if there was a failure, we will
transparently failover to another node, but who takes care of high availability on the client? Typically you'll want to have at least a couple of machines 
running subscriptions, so if one of them goes down, the other one will carry on. However, if the same subscription is running concurrently on two machines 
that can lead to duplicate processing. Now, your code need to handle that anyway, since subscriptions gurantee at least once and various error conditions can
cause a batch to be resent. However, there is a major difference between handling that once in a blue moon and having to deal with it constantly, leaving aside
the issue that we will also have higher resource usage because we need to process the subscription on multiple nodes.

Luckily, RavenDB Subscriptions aren't working like that. Instead, a subscription is always opened by one and only one client. By default, if two clients 
attempt to open the same subscription, one of them will succeed and the other one will raise an error because it couldn't take hold of the subscription.
This is controlled by the `SubscriptionOpeningStrategy` option set on the `SubscriptionConnectionOptions.Strategy`. The various options of this property are:

* `OpenIfFree` - the default. The client will attempt to open the subscription, but will fail if another client is already holding it. This is suitable for
  cases where you are running subscriptions in batch / schedule mode and if another client is already processing it, there is no need to process anything
  yourself. 

* `TakeOver` - the client will attempt to open the subscription even if a client is already connected to it. If there is a connected client, its connection
  will be closed. This is used to force the subscription open to succeed by a particular client. It is recommend to only enable this with an eye toward the
  global behavior across all your clients and subscriptions, since two clients that attempt to connect with this mode will kick one another off the 
  subscription as they each try to hold it.

* `WaitForFree` - the client will attempt to open the subscription, but if there is already a connected client, it will keep trying to acquire the connection.
  This is suitable for high avaialability mode, when you have multiple clients attempting to acquire the same subscription, and you want one of them to succeed
  and the rest to stand in readiness for that client to fail and replace it as the one processing the subscription.

For high availability processing on the client, you'll setup multiple machines that will run your subscriptions and open them using the `WaitForFree` option. 
In this way, those clients will compete for the subscriptions and if one of them fail will take over and continue processing them. The use of multiple machines
for handling subscriptions also allow you to split your processing between the machines. 

> **Failure conditions in distributed environment**
>
> Failure handling in a distributed environment is _hard_. When both clients and servers are distributed, this make for some really interesting failure modes.
> In particular, while we promise that a subscription will only be opened by a single client at a time, it is possible for the network to split in such a way
> that two clients have successfully connected to two different servers and are rying to process the same subscription from it.
>
> That scenario should ber rare, and will only last for the duration of a single batch, until the next syncronization point for the subscription (which ensure > global consistency for the subscription). One of those servers will fail to process the batch acknowledgement and return that error to the client, eventually
> aborting the connection. 

You may decide that you want certain operations to be handled on one node, and configure those subscriptions with the `TakeOver` on that node and `WaitForFree`
on another node. This way, that particular subscription has a preferred node that it will run on, but if that node fail, it will run on the other node. This 
way each machine will usually run only its own preferred subscriptions unless there is a failure. 

When your deployment contains mutliple nodes that are all processing subscriptions, you need to be even more aware to the fact that you are running in a 
distributed system, and that concurrency in a distributed environment should always be a consideration. We already talked about optimistic concurrency in 
previous chapters, but with distributed database we also need to take into account eventual consistency and conflicts between machines. We'll talk in [Your first RavenDB Cluster](#clustering-intro) about such topics in depth.

For subscriptions, we typically don't care about this. The batch processing mode means that we already see the world differently and it doesn't mather if 
we get a document in a particular batch or in the next. One thing to be aware of is that different subscriptions may be running on different nodes and get
the documents in different order (all of them will end up getting all the documents, there is just no guranteed order for that). 

## Using subscription for queuing tasks

In this section I want to utilize subscriptions as a processing queue. Both because this is a very common scenario for using subscription and because it
lets us explore a lot of the RavenDB functionality and how different pieces are held together. What we want to do is to be able to write an operation to 
the database and when we have successfully processed it, automatically delete it.

Going back to the email sending sample, our code will write `EmailToSend` documents that will be picked up by a subscription and handled. The code for 
actually doing this is shown in Listing 5.16. 

```{caption="Delete the emails to send after successful send" .cs}
using (var subscription = 
	store.Subscriptions.Open<EmailToSend>(options))
{
    await subscription.Run(async batch =>
    {
        using (var session = batch.OpenAsyncSession())
        {
        	foreach (var item in batch.Items)
            {
		        try
	            {
               		SendEmail(item.Document);
            	}
            	catch
            	{
            		// logging / warning / etc
            		continue;
            	}
                session.Delete(item.Id);
        	}
            await session.SaveChangesAsync();
        }
    });
}
```

There is actually a lot of stuff that is going on in Listing 5.16 that may not be obvious. First, we are processing a batch of documents, trying to send each
one in turn. If there is an error in sending one of those emails, we will skip further processing.

However, if we were able to successfully send the email, we'll register the document to be deleted. At the end of the batch, we'll delete all the documents 
that we have successfully sent. However, we won't touch the documents that we didn't send. The idea is that we dedicate the `EmailToSend` collection for this
task only. That means that in the `EmailToSend` collection we are going to only ever have one of two types of documents.

* `EmailToSend` documents that haven't been processed yet by the subscription and will be sent (and deleted) soon.
* Documents that we tried to send by failed to do so.

Those documents that we fail to send are really interesting. Since the subscription has already processed them, we will not be seeing them again. However, we
didn't lose any data, and an administrator can compare the current state of the subscription to the state of the `EmailToSend` collection and get all the 
failed documents.

At that point, the admin can either fix whatever is wrong with those documents (which will cause them to be sent to the subscription again) or they can reset 
the position of the subscription and have it re-process all those documents again (for example, if there was some environmental problem). This mode of 
operation is really nice for processing queues and for simple cases you can just rely on RavenDB to do it all for you. In fact, given that RavenDB 
subscriptions can work with failover of both clients and servers this give you a robust solution to handle task queues that are local to your application.

One thing to note, RavenDB isn't a queuing solution, and this code doesn't pretend that it is. The code in Listing 5.16 is a great way to handle tasks queues, 
but proper queuing system typically offer additional features (monitoring, builtin error handling, etc) that you might want to consider. For most simple tasks,
the kind of fire & forget operations, you can use RavenDB in this mode. But for more complex stuff, you should at least look whatever a proper queue can offer
a better solution.

## Versioned Subscriptions

The subscriptions we used so far are always operating on the current state of the document. Consider the following case:

* Create a subscription on the `Customer` collection.
* Create a customer document and modify it 3 times.
* Open & Run the subscription and observe the incoming items in the batch.

What will the subscription get? It will get the _last_ version of the customer document. The same can happen when the subscription is already running, if you
have made multiple modifications to a document, when the next batch will start, we'll just grab the current version of the document and send it to the 
subscription.  This can be a problem if your subscription code assume that you'll get not only the document when it changes, but you'll get the document once 
on each change.

The good news is that we can utilize another RavenDB feature, Versioning (we looked at it in [Advanced client API](#advanced-client-api)) to allow us to see all the changes that 
were made to a document. 

The first step to exploring this feature is to enable versioning on the collection you want, in this case, the `Customer` collection. Set the minimum retention
time for 2 weeks, and again make a few modifications to a customer document. When you'll run your subscription again, note that this again, you are getting 
just the latest version of the `Customer` document. 

The reason you only get the latest version of the document is that the subscription, too, need to let the server know that it is versioned. This is because the
data model for versioned subscription is different. Let us take a look at Listing 5.17 for an example of a very simply versioned subscription that can detect
when a customer changed its name. 

```{caption="Subscribing to versioned customers" .cs}
string subsId = store.Subscriptions.Create(
	new SubscriptionCreationOptions<Versioned<Customer>>()
);

using(var subscription = 
	store.Subscriptions.Open<Versioned<Customer>>(
		new SubscriptionConnectionOptions(subsId)
	)))
{

	await subscription.Run( batch =>
	{
		foreach(var item in batch.Items)
		{
			Versioned<Customer> customer = item.Result;

			if(customer.Previous.Name != 
				customer.Current.Name)
			{
				// report customer name change
			}
		}
	});
}
```

The key parts to the code in Listing 5.17 is the use of `Versioned<Customer>` as the type that we are subscribing on. RavenDB recognize that as a versioned
subscription and will feed us each and every revision for the `Customer` collection.

Note that the data model, as well, has changed. Instead of getting the document itself, we get an object that has `Previous` and `Current` properties, 
representing the changes that happened to the document. In other words, you are able to inspect the current and previous versions and make decisions
based on the changes that happened to the entity. 

This feature opens up a _lot_ of options regarding analytics because you aren't seeing just a snapshot of the state, but all the intermediate steps along the 
way. This has utilizations in business analytics, fraud and outliers detection, foresnics and being able to reconstruct the flow of data through your system. 
The way versioned subscriptions work, we are going to get all the changes that match our criteria, at the same order they happened in the database^[Again, 
different nodes may have observed the events in a different order, but it should roughly match across the cluster].

If I wrote to `customers/8243-C`, created a new `customers/13252-B` and then `customers/8243-C` again. I will get the changes (either in a single batch 
or across batches) in the following order:

* `customers/8243-C` - (`Previous` -> `Current`)
* `customers/13252-B` - (`null` & `Current)
* `customers/8243-C` - (`Previous` -> `Current`)

This can be useful when running foresnics or just when you want to inspect what is going on in your system.

> **Handling versioned document creation and deletion**
> 
> When a new document is created, we'll get it in the verisoned subscription with the `Previous` property set to `null` and the `Current` set to the newly
> created version. Conversely, if the document is deleted we'll get it with the `Current` set to `null` and the `Previous` set to the last version of the
> document. 
> 
> If a document is deleted and re-created, we'll have one entry with (`Previous`, `null`) and then another with (`null`, `Current`). This feature is also
> useful if you just want to detect deletion of documents in a subscription.

Just like regular subscriptions, we aren't limited to just getting all the data from the server. We can filter it, as you can see in Listing 5.18.

```{caption="Setting filters on a versioned subscription" .cs}
string subsId = store.Subscriptions.Create(
	new SubscriptionCreationOptions<Versioned<Customer>>
	{
		Criteria = new SubscriptionCriteria<Versioned<Customer>>
			(
				ver => ver.Previous.Name != ver.Current.Name
			)
	}
);
```

The code in Listing 5.18 will only send you the `Customer` documents whose name have been changed. The rest of the code in Listing 5.17 can remain the same
but we now don't need to check if the name have changed on the client side, we have done that already on the server.

> **Versioned subscriptions use the versioning store**
> 
> This might be an obvious statement, but it needs to be explicitly stated. The backing store for the versioned subscription is all the revisions that are 
> stored for the documents as they are changed. That means that it is important to know what is the minimum retention time that has been configured.
> If you have a weekly subscription run and the versioning is configured to keep data for a day, you are going to miss out of revisions that have already
> been deleted. 
>
> The administrator need to make sure that when using a versioned subscription, the retention time matches when we are actually going to call to the 
> subscription.

A natural extension of this behavior is to not send the full data to the client, but just the data we need. An example of this can be shown in Listing 5.19.

```{caption="Getting changed names using versioned subscription" .cs}
string subsId = store.Subscriptions.Create(
	new SubscriptionCreationOptions<Versioned<Customer>>
	{
		Criteria = 
		{
			Script = @"

	if (this.Previous.Name !== this.Current.Name)
	{
		return {
			OldName: this.Previous.Name, 
			NewName: this.Current.Name
		}
	}
	"
		}
	}
);
```

The code in Listing 5.19 isn't actully that interesting, except for that it is actually doing. It is filtering the data on the server side and only sending 
us the old and new names. The subscription handling code on the client side just need to take the old name and new name and notify whoever needs to know 
about the name change.

Versioned subscriptions give you a lot of power to work with your data (and the changes in your data) in a very easy manner. The ability is of particular 
inteest in reversed event sourcing^[event projectioning? I'm not sure that this is a word] because you can go over your data, detect a pattern that match
a particular event you want to raise and publish it. 

## Subscriptions in Python

Quick scripts to enhance your sytem 

## Subscription administration

	// TODO: UI of subscription view
	// TODO: UI of subscription edit
	// TODO: UI of test subscription output

## Summary

When RavenDB got subscriptions for the first time, it was just a small section. A minor feature that was meant to handle an esoteric scenario that only a 
single customer has run into. From that modest beginning this feature has blown up to completely replace most batch processing handling in RavenDB. It is 
not the first thing you'll use in any situation, but for anything that isn't about responding, it is usually a match.

In this chapter we looked into what subscriptions are, how to do batch processes with RavenDB and subscription, filter and modify the data on the server, 
accept and process the data on the client side and write it back to RavenDB in an optiomal fashion. We looked at integrating with external systems via the 
email sending example, including how to handle failures and partial failures both in RavenDB and in the external systems.

> **Using subscription for ETL work**
>
> It is tempting to use subscriptions to handle ETL^[Extract, Transform, Load - the process of moving data around between different different data storage 
> systems.] tasks, such as writing to a reporting database. While it is possible, RavenDB have better options to  handle ETL. See the discussion on this 
> topic in the [Sharing data and making friends with ETL](#integrations) chapter.

We spent a lot of time discussing how to handle errors in subscriptions, how to deploy them into production and how we can ensure failover and load balancing
of the work among both client and server using subscriptions. The subscription open strategies allow us to have a hot standby client for a subscription, ready
to take over from a failed node and continue processing. On the server side, RavenDB will automatically failover subscriptions from a failed database node to
a healthy one in a completely transparent manner.

Another great usecase for subscriptions is implementing task queues, and we looked at that, including error handling and recovery and the ability to fix an 
issue with a document that failed and have it automatically reprocessed by the subcsription. 

We then looked at versioned subscription, which asks RavenDB to give us a before / after snapshot of each and every change for the documents that we care 
about. This is a great help when producing timeline, tracking changes and handling foresnics. This feature relies on the versioning configuration on the 
database and expose them directly for batch processing needs. We can even write a subscription script that would filter and send just the relevant details
from both old and new versions of the document to the client side.

Finally, we looked at how we can manage subscriptions in the RavenDB Studio, looking at subscriptions states, editing them and in general verifying that the
all systems are green and that the data is flowing through the system properly.

The next part of the book is going to be exciting, we are going to learn how RavenDB is working in a distributed envrionment and can finally figure out what
all those cryptic references to working in a cluster _mean_.