
# Map/Reduce Indexes

Map/Reduce is a programming concept that was made popular after Google started talking about doing their Page Rank algorithm using it. The idea isn't new, but the scale in which Google applied it was.

You've probably heard about Map/Reduce in the past, at least in passing. But my experience have shown that this is a subject most people have marked with "There Be Dragons" in their head and consider this something that is only relevant for Big Data or special use cases.

The terminology used is a strong part of that. The terms Map/Reduce actually refer to function name in Lisp and to a common usage pattern in functional languages. All of which result in even technical people assuming that this is nerd stuff that they don't need to care about.

Another way to talk about Map/Reduce is to talk about aggregation. In particular, aggregation of a data set to a smaller summary. I assume that you are familiar with SQL, so you shouldn't have a problem understanding the SQL statement in Listing 10.1.

```{caption="{Aggregation using GROUP BY in SQL}" .sql}
SELECT o.CategoryId, COUNT(*) TotalProducts
FROM Products p
GROUP BY o.CategoryId
```
If you aren't familiar with SQL, don't worry. What this statement is doing is to get the total amount of all purchases for each customer. Nothing surprising or scary about this, right?

The statement in Listing 10.1 is something that most developers encounter on a regular basis. We are just doing a bit of aggregation, nothing as fancy or hard as Map/Reduce. But the interesting thing about Map/Reduce is that it is best when it is doing exactly what Listing 10.1 is doing. Aggregation over a data source to get the summary. In Listing 10.2, the SQL statement was translated into a Linq expression.

```{caption="{Aggregation using GroupBy in Linq}" .cs}
from o in products
group o by o.Category into g
select new 
{
	Category = g.Key,
	Count = g.Count()
}
```

So far, it is perfectly obvious what is going on here. Let us take Listing 10.2 and turn that into a Map/Reduce definition. You can see that in Listing 10.3.

```{caption="{Aggregation using Map/Reduce}" .cs}
var mapResults = // map operation
	from o in products
	select new 
	{
		o.Category,
		Count = 1
	};

var reduceResults = // reduce operation
	from result in mapResults
	group result by result.Category into g
	select new
	{
		Category = g.Key,
		Count = g.Sum(x=>x.Count)
	};
```

Listing 10.3 isn't actually all that different from Listing 10.2. Instead of doing everything in a single expression, we have simply split the operation into two expressions. One that just gather the details, and another that will perform the actual aggregation. But even though Listing 10.2 and Listing 10.3 are very similar, Listing 10.3 is the more complex one. So why go with that approach?

The reason for splitting an aggregation operation like that is very simple, it allow us to break apart the actual execution into distinct parts, that can run independently from one another, and _that_ in turn, allow us to do some really interesting things with aggregation computation.

## The magic of Map/Reduce

So Map/Reduce is basically a complex way to do a Group By, nothing more. Why add this complexity? Because having the steps of the aggregation process split into an explicit map and reduce steps allow us to treat each of them in isolation. And that lead to an interesting observation. If we can treat each step in isolation, that means that we aren't required to run them sequentially and at the same time.

The common case for running Map/Reduce is that it allows us to easily parallelize the operation. And we can do that on multiple machines to quickly aggregate very large amount of information. That was the original usage of Google's Map/Reduce framework.

Let us see how we can process a Map/Reduce operation on the data showing in Table 10.1.

Id 		  	Name 		 					Category
--------- 	------     						---------
products/37 Gravad lax  					categories/8
products/38 CÃ´te de Blaye   				categories/1
products/39 Chartreuse verte   				categories/1
products/40 Boston Crab Meat   				categories/8
products/41 Jack's New England Clam Chowder categories/8

: Sample data for Map/Reduce test run 

First, we need to run the map. We want to see how Map/Reduce allow us to work with large amount of data, so we'll limit ourselves to 3 rows per run. So we run the map twice, on the first three rows and then on the last two rows. The results are shown in Table 10.2 and table 10.3.

Category 		Count
------------ 	------
categories/8	1
categories/1 	1
categories/1 	1

: Running the Map operation on the first three rows of Table 10.1

Category 		Count
------------ 	------
categories/8 	1
categories/8 	1

: Running the Map operation on the last two rows of Table 10.1

We have run the Map operation, now it is time to run the Reduce operation. The results are shown in Table 10.4 and Table 10.5.

Category 		Count
------------ 	------
categories/8	1
categories/1 	2

: Running the Reduce operation on Table 10.2

Category 		Count
------------ 	------
categories/8 	2

: Running the Reduce operation on Table 10.3

We are almost done, but we still have two separate tables, and no final results. In order to get those, we now need to run the Reduce again. But this time (and this is crucial for understanding), we are going to run the Reduce on its own output. Take a look at Figure 10.1 to see how the results are processed.

![Parallel Execution path of Map/Reduce](./Ch10/map-reduce.png)

Map/Reduce seems to add a lot of extra complexity. We could have just aggregated everything in a single step, and it would be much easier to understand, faster and easier all around. But Figure 10.1 is the key for understanding why Map/Reduce plays such a role in Big Data and NoSQL systems. Map/Reduce allows us to process the data in parallel, and there is no requirement that all the data would be present on a single machine.

### The Map process

The first stage in the Map/Reduce process is the Map, obviously. The map is responsible for getting just the relevant information for the aggregation that we are performing from the source data. Usually, the map select just a few fields. In the case of the Map/Reduce in Listing 10.3, the Map will select the `Category` and the `Count` properties.

Actually, `Count` isn't even a real property, we are setting that value to 1. That seems strange, can't we just do a `Count()` in the Reduce instead of creating this fake property?

The answer is that we cannot, and the reason we cannot is that while the Map function is only applies once on the source input, the Reduce function can be applied multiple times. See Figure 10.1 and the Reduce being operated on the results in the Map Threads and the Reduce Threads.

Because the Reduce function is applied incrementally and recursively, it needs to be able to process its own output. The easiest way to ensure that it can do that is to have both the Map and Reduce functions output the data in the same shape. 

Map isn't really all that interesting, to be fair. It just gets the relevant data, the interesting bits happen in the Reduce.

### The Reduce process

After we mapped the source data, we start aggregating it. In the case of the Map/Reduce process in Listing 10.3, we are aggregating the results by the `Category`. A common mistake when writing Reduce functions is assuming that they only run once, and on the full data set. As you can see in Figure 10.1, that is certainly not the case.

![Distributed process of Map/Reduce process](./Ch10/distibuted-map-reduce.png)

As the name implies, Reduce is generally meant to accept large input and have a much smaller output. Most aggregation tasks do this naturally (sum, count, etc). Note that any Reduce function has the notion of the `reduce key`, the value on which we are aggregating. In the case of the Map/Reduce process in Listing 10.3, the `reduce key` is the `Category` property. 

Right now, it doesn't seem very meaningful, because we are processing the entire thing inside a single machine as a single batch (even if Figure 10.1 showed it running with multiple concurrent steps). Take a look at Figure 10.2, which shows how we can split the process among multiple machines.

In this case, we process half the source data on each machine, and send to the other machine the _reduced results_ for the `reduce keys` that this machine handles. In Figure 10.2, Machine #1 run Map/Reduce on the data as part of initial stage. Then send the partial results is has for the `reduce key` with the value `categories/8` to Machine #2, where it is combined with that machine's partial results to compose the final results. We don't have any results for the `categories/1` value, so we don't send anything back to Machine #1.

The major benefit that we gained here is that we only had to send the reduced results. And as the name implies, that is far smaller amount of data to send, and most of the hard part part was already done in the initial Map/Reduce on the origin machine. In this manner, we are able to aggregate large amount of data without having to shuffle the entire dataset through the network.

## Map/Reduce in RavenDB

RavenDB uses Map/Reduce extensively, but it isn't usually used to handle distributed Map/Reduce tasks^[Those are possible, using the Sharding features.]. Instead, RavenDB uses Map/Reduce to handle aggregation tasks inside a single node. In other words, it is close to Figure 10.1 in operation than Figure 10.2.

Why? We already established that Map/Reduce is more complex than simple aggregation. The reason for going with Map/Reduce was to parallelize the work and allow to distribute it, but if RavenDB doesn't take advantage of data distribution for aggregation (unless you use Sharding, discussed in Chapter ???), why is it using Map/Reduce at all?

![Incremental Map/Reduce execution ](./Ch10/map-reduce.png)

The reason is quite simple. RavenDB isn't distributing the aggregation operation between different machines. At least not in the traditional sense. What it does instead is distribute the aggregation operation _over time_. RavenDB is one of the few databases that supports incremental and resumable Map/Reduce operations.

Consider the process outlined so far, we gather the source data (in RavenDB's case, the documents) and run them through the Map function. Then we run the results of the map through the Reduce function to get the final result. We are done. Almost. Because there is one thing that we haven't talked about.

What happen when users modify the data? For example, by adding a new product? You can see what RavenDB will do in Figure 10.3. Instead of re-executing the entire Map/Reduce operation, which may require us to go over the entire dataset, we can run the Map and Reduce functions on the new product, get the `Category` that it belongs to, and then load the results from the previous Map/Reduce run. Now we can simple run the Reduce over the new results and persisted results, getting to the final answer.

> **Single step & multi step reduces**
> 
> The process shown in Figure 10.3 isn't actually how this works. We can't just apply the reduce over the 
> values and the new ones blindly. We might have an update (maybe the product 
> categories changed?), so it isn't that simple.
>
> RavenDB handles this by automatically selecting the appropriate strategy. If there aren't many values for 
> a specific `reduce key` (in our case, not many products per category), we'll just reduce all of the saved
> Map outputs together with the new map value. This still saves a lot of work, because we don't have to load
> the relevant documents and run the map again, and we store the saved map data in a way that make it cheap to
> load all the saved Map results for a particular `reduce key`. This is called the single step 
> reduce strategy.
>
> However, if there are a lot of values for a `reduce key`, we choose the multi step reduce strategy instead. 
> "A lot" in this case is over 1,024 values for a single `reduce key` (more than 1,024 products per category).
> In that case, we'll save not only the map results, but also the intermediate reduce results. The actual 
> process is complex and not really interesting externally. We split the data into buckets and steps. In the 
> first step (`reduce0`), we have 1,048,576 buckets, and map results are spread across those buckets in 
> an even manner. The next step (`reduce1`) have 1,024 buckets, and in the last step (`reduce2`) we only 
> have a single bucket.
>
> When we need to run the Map/Reduce again (a new product in a big category), we don't need to re-reduce 
> the whole thing. We can get away with running the Map on the new product, then reducing the result with 
> the relevant buckets. We need to go through the full three reduce steps, but the advantage here is that we
> actually work on drastically smaller data set, and still are able to update the aggregated value quickly.

Note that the amount of work that we actually had to do is fixed. Run Map/Reduce on the new data, reduce it with the old data. That means that the action of updating the aggregation results is very cheap. Because we are only maintaining the running tally, instead of having to go and start the whole process from scratch.

### Querying a Map/Reduce index

A Map/Reduce index in RavenDB is still an index, and the final results of the index are going to be stored to the Lucene index. That means that we can search on those results, to find the already aggregated values very quickly. 

If we have a very large product catalog, asking how many products are in a specific category is very cheap. All we need to do is check the index for that category's value, where we'll find the aggregated result. Instead of having to go through all the products, RavenDB just hand you the right result, previously computed and at near to zero cost.

The fact that we put the Map/Reduce results inside a Lucene index means that we can apply all of the usual suspects on the index. We can make complex queries, facets or even suggestions. 

The fact that the aggregation already happened by the time you query the database has a huge impact on the way we structure our applications. Consider for example a business requirement that we'll show for each customer how many orders they have with us and for what amount. If you are using a relational database, issuing such a query on every page load is expensive. Not only that, but the more important the user is (the more orders they have), the slower the system will respond.

That is the point where you start looking at caching, invalidation, keeping track of the totals manually, etc. A very simple feature escalate to a very complex one, because we can't just hit the database with a complex group by query on large customers at each page view.

With RavenDB, you define the Map/Reduce index, and you query it as you need. Because the computation doesn't happen during the query, the actual query execute in a constant amount of time and you get the results immediately. Caching, invalidation and re-running the computation are all handled by RavenDB internally. 

A simple feature _remain_ a simple feature.

## Practical aspects of Map/Reduce indexes

So far in this chapter we mostly talked about the theory of Map/Reduce, how it works, and how RavenDB make use of it to provide lightning fast aggregation queries over large data sets. But after the theory, we need to sit down and understand what RavenDB actually does with it and the implications of that.

One very obvious fact is that a Map/Reduce index needs to keep track of a lot more information than a simple Map (or a Multi Map) index. That means that the process of indexing a Map/Reduce index is more costly, and involved multiple stages.

Even if we fall into the single reduce step mode, we still need to persist the Map results, and then run the reduce. That means that we have to do more I/O. The Map stage in the Map/Reduce goes through the same exact code path as the simple indexes, which means that RavenDB may decide to parallelize Map run across as many cores as it is able. 

The Reduce stage, however, is a bit more complex. Here, we still have parallelism, but it is expressed a bit differently. We'll run multiple Reduce indexes at the same time, but we also have parallelism inside the same index. Multiple reduce actions can be run concurrently for the same index, for different `reduce keys`. 
You can see what exactly is going on with the indexing by looking at the Status > Indexing page in the Management Studio (see more in Chapter ???, Administration).

So Map/Reduce is more costly when it comes to indexing. But the cost of _querying_ a Map/Reduce index is the same as querying any other index (indeed, at that stage, they use the same code paths and the exact same behavior).

### Impure functions and other sins 

RavenDB has a few requirements from a Map/Reduce index. We already noticed that the Map and Reduce has to have the same output. Or, to be more exact, the _shape_ of the output from Map and Reduce must be the same. This is required because the input of Reduce is both the output of the Map and the output of the Reduce itself.

Another requirement for both the Map and Reduce functions must be _pure_. What does it mean, a pure function? A pure function will always have the same output for identical input. It cannot rely on any external state or make any observed side effect. Why is that?

Consider the following _non_ pure Map function:

	from user in docs.Users
	select new { Age = DateTime.Today - user.Birthday };

What is the problem here? Look closely at the `DateTime.Today` call. It means that whenever we run this function, it will give different results, just because the clock changed. But RavenDB is _relying_ on the fact that it can save the intermediate results of Map/Reduce operations to allow for incremental aggregation. If the value of the Map can change, RavenDB wouldn't be able to use that.

> **What about `LoadDocument`?**
>
> Calling `LoadDocument` in Map/Reduce is possible, it is actually quite useful. But `LoadDocument`
>  is also pretty much the _definition_ of relying on external state. 
> RavenDB allows this because it is aware of those dependencies
> and will reindex the referencing documents when the referenced document changes.

In practice, most of the time you won't even notice that this is a hard requirement. And RavenDB is smart enough to detect most external state violations (such as relying on the current time) and we'll error when you try to create the index if you call them.

Map functions only need to be pure, but the requirements for Reduce are a bit higher than that. A Reduce function needs to be pure, commutative and associative. Basically, it means that given a set of inputs to the reduce, we can send the inputs to Reduce in any order, or split it and merge the results in any orders, and still arrive at the same final result. Given how we are actually processing Map/Reduce, the reason behind that should be pretty obvious.

For the most part, that is really easy. A lot of aggregation operations are already naturally pure, commutative and associative. Count, sum, min and max are great matches, but calculating average is not. Average is neither commutative or associative. Listing 10.4 shows how we can calculate the average number of items purchased per order for each product.

```{caption="{Calculating averages number of products purchased per order using Map/Reduce}" .cs}
//map
from o in docs.Orders
from l in o.Lines
select new 
{
	l.Quantity,
	l.Product,
	Count = 1,
	Average = -1
}

//reduce
from r in results
group r by r.Product into g
let sum = g.Sum(x=>x.Quantity)
let count = g.Sum(x=>x.Count)
select new 
{
	Quantity = sum,
	Product = g.Key,
	Count = count,
	Average = sum / count
}
```

In Listing 10.4, we are calculating the average on each reduce by also taking care to have all the information we need to actually calculate it, the `Quantity` and `Count` fields. Similar methods exists for more complex operations (like standard deviation or medians, for example).

One final restriction that we need to remember for Map/Reduce indexes is that we cannot play limits on the output based on the result on the aggregation. What does _that_ means?

Let us say that we want to create a Map/Reduce index to gather all of our active customers. An active customer has a lot of orders, so we could try to add a clause to the reduce to discard all customers with less than 5 orders from the index. Such an index would look like Listing 10.5.

```{caption="{Bad Map/Reduce index with a where in the reduce}" .cs}
//map
from o in docs.Orders
select new 
{
	o.Customer,
	Count = 1,
}

//reduce
from r in results
group r by r.Customer into g
let count = g.Sum(x=>x.Count)
where count >= 5
select new 
{
	Customer = g.Key,
	Count = count
}
```

The problem with the index in Listing 10.5 is that it would have no results. When you reason about it, the answer why is obvious. We process the index in an incremental fashion, so at some point, we are processing a single order, and then run the reduce on that. The count is obviously 1, so it is discarded. We never get to group enough orders for a customer to pass the where clause. In practice, this means that you shouldn't use a where clause in the Reduce function. Instead, you do the filter in query time.

## Map/Reduce from the client API

Enough with learning how things work, let us roll up our sleeves and actually _write_ something end to end. In Northwind, we have employees and orders. Let us create an index that will let us select the Employee of the Month, which for our purposes will be the employee with the most sales in a particular month. You can see the index in Listing 10.6.

```{caption="{Using Map/Reduce to calculate Employee of the Month}" .cs}
public class EmployeeOfTheMonth : 
	AbstractIndexCreationTask<Order, EmployeeOfTheMonth.Result>
{
	public class Result
	{
		public string Employee;
		public string Month;
		public int TotalSales;
	}

	public EmployeeOfTheMonth()
	{
		Map = orders =>
			from order in orders
			select new
			{
				order.Employee,
				Month = order.OrderedAt.ToString("yyyy-MM"),
				TotalSales = 1
			};

		Reduce = results =>
			from result in results
			group result by new {result.Employee, result.Month}
			into g
			select new
			{
				g.Key.Employee,
				g.Key.Month,
				TotalSales = g.Sum(x => x.TotalSales)
			};
		Sort(x => x.TotalSales, SortOptions.Int);
	}
}
```

In Listing 10.6, you can see that we define an index in just the same way that we would have if we were using a Map only index. We inherit from `AbstractIndexCreationTask` and specify two generic parameters. The source collection (`Order`) and the output (`EmployeeOfTheMonth.Result`). In this case, the output of the index is also the input for the Reduce (as well as being the _output_ of the Reduce, obviously).

You should note that we are grouping by two fields, the `Month` and the `Employee`. And we are summing the total number of orders per month. Nothing really special going on there. You can see the results of querying this index in the studio in Figure 10.4.

![Results of querying the EmployeeOfTheMonth index in the Northwind db.](./Ch10/Figure-04.png)

Now that we have this index, let us find who was the top selling employee in a particular month^[The Northwind dataset is pretty old, we actually have to go all the way to 1998 to find a good month to check]. We do that in Listing 10.7.

```{caption="{Finding the Employee of the Month for Mar 1998}" .cs}
var empOfMon = session.Query<EmployeeOfTheMonth.Result, EmployeeOfTheMonth>()
	.Include(x => x.Employee)
	.OrderByDescending(x => x.TotalSales)
	.Where(x => x.Month == "1998-03")
	.First();

var emp = session.Load<Employee>(empOfMon.Employee);

Console.WriteLine(emp.FirstName +" " + emp.LastName);
``` 

The result of the code in Listing 10.7 is that Margaret Peacock is the Employee of the Month for March of 1998. Let us break down the query to see what we are doing. We start by specifying that we want to get `EmployeeOfTheMonth.Result` from the `EmployeeOfTheMonth` index. We include the employee in the query, like we learn in Chapter 3, so we won't have to go to the database to get the Employee of the Month's document. We sort the results and ask to get highest ranking result in March '98. Then we are pretty much done.

Note that even though we grouped on both `Employee` and `Month`, we are able to query them independently. Using the code in Listing 10.7, it is easy to see how we can ask an employee, what is your best month^[Just changed the condition in the where from a match on the `Month` to a match on the `Employee` property].

## Complex aggregation with Map/Reduce

The index shown in Listing 10.6 is pretty simple. We are aggregating by employee and month, and summing the total orders. Map/Reduce is capable of doing so much more. In this case, we want to show the user a purchase history for a specific company. Over time, how many products this company purchased? You can see an example of that in Figure 10.5.

![Showing product purchases over time](./Ch10/Figure-05.png)

We are still aggregating over orders, like in Listing 10.6, but we need to more than just sum value by employee and month. In fact, we _can_ probably solve this using just the techniques we have seen so far, by grouping by Company, Year and Product. But that would mean that after querying the index, we'll need to stitch the whole thing up to be able to show that to the user. Instead, we can do it all in a single (complex) Map/Reduce index.

Because this is pretty complex, we'll split this into several steps. Listing 10.8 will show the skeletal structure of the index.

```{caption="{Skeleton for getting product purchases over time per company}" .cs}
public class ProductPurchasesByCompany : 
	AbstractIndexCreationTask<Order, ProductPurchasesByCompany.Result>
{
	public class Result
	{
		public string Company;
		public IEnumerable<ProductPurchaseHistory> History;
	}

	public class ProductPurchaseHistory
	{
		public string Product;
		public int TotalPurchases;
		public IEnumerable<ProductPurchases> Purchases;
	}

	public class ProductPurchases
	{
		public int Year;
		public int Quantity;
	}

	public ProductPurchasesByCompany()
	{
		Map = orders => ...

		Reduce = results => ...
	}
}
```

In Listing 10.8 you can see that we created the usual `Result` nested class, but unlike our usual suspects, we don't have simple properties. In fact, we have a full blown object graph just sitting there and grinning at us mischievously. What is it going to do? Listing 10.9 shows the Map function for this index.

```{caption="{Map function for product purchases history}" .cs}
from order in orders
select new Result
{
	Company = order.Company,
	History =
		from line in order.Lines
		select new ProductPurchaseHistory
		{
			TotalPurchases = line.Quantity,
			Product = line.Product,
			Purchases = new[]
			{
				new ProductPurchases
				{
					Year = order.OrderedAt.Year,
					Quantity = line.Quantity
				}
			}
		}
};
```

One obvious new thing in Listing 10.9 is that we are now using full types, not anonymous types, as the result of a Map operation. That is done so we'll have type safety, but behind the scenes, before we send this index to RavenDB, those types are removed. We can't have them, because they don't exist on the server side.

Beyond the use of `Result`, `ProductPurchaseHistory` and `ProductPurchases` this Map function is special only in the fact that it creates a relatively complex object, instead of the usual flat outline. But there is nothing wrong with that, we just haven't done this before. It is in the Reduce function that we are really getting crazy. You can see that function in Listing 10.10.

```{caption="{Complex Reduce function for product purchases history}" .cs}
from result in results
group result by result.Company into g
select new Result
{
  Company = g.Key,
  History = 
    from p in g.SelectMany(x => x.History)
	group p by p.Product into pg
	let productPurchaseHistory = new ProductPurchaseHistory
	{
		Product = pg.Key,
		TotalPurchases = pg.Sum(x => x.TotalPurchases),
		Purchases = 
			from prod in pg.SelectMany(x => x.Purchases)
			group prod by prod.Year into tg
			let productPurchases = new ProductPurchases
			{
				Year = tg.Key,
				Quantity = tg.Sum(x => x.Quantity)
			}
			orderby productPurchases.Year descending
			select productPurchases

	}
	orderby productPurchaseHistory.TotalPurchases descending
	select productPurchaseHistory
};
```

Take your time reading through Listing 10.10, it ain't easy. I'll admit to a few cackles when writing this index, because of the amount of work that we can get out of it.

The key part in the Reduce in Listing 10.10 is that RavenDB will only apply Map/Reduce on the first level `group by`. So as far as RavenDB is concerned, we are simply grouping everything by the `Company`. After the results has been grouped by the company, we are settling down to the aggregation per company. And here is when things get interesting.

In Listing 10.6, we just summed the values, but in Listing 10.10, we are going through quite a bit of work. We have all the history of this company purchases, and we aggregate it, first by the product that was purchased, then by the year it was purchased. Finally, we output the results in the appropriate order. Most popular product first, and for each product, we output the values sorted by year.

It is much easier to understand what is going on when we see the output of this index. You can see this in Listing 10.11.

```{caption="{Output from the product purchases history index}" .json}
{
  "Company": "companies/52",
  "History": [
    {
      "Product": "products/62",
      "TotalPurchases": 32,
      "Purchases": [
        {"Year": 1997, "Quantity": 20 },
        {"Year": 1996, "Quantity": 12 }
      ]
    },
    {
      "Product": "products/72",
      "TotalPurchases": 30,
      "Purchases": [
        {"Year": 1997, "Quantity": 30 }
      ]
    }
  ]
}
```
The data in Listing 10.11 is redacted for space reasons, but it should give you a good idea about what kind of output it generates, and what you can do with it. This can be the source data for a chart like Figure 10.5, or you can show the customer what the trends are. The point in this index wasn't so much to show the actual technique as to demonstrate that you are only limited by what you want to do with Map/Reduce.

The really nice thing about this index is that regardless of how complex it is, it is still a RavenDB Map/Reduce index. That means that all the complexity happens at indexing time, and we are able to query over this index with lightning speed.

> **Querying nested objects**
>
> RavenDB only index the first level of properties in an object. That means that looking at 
> the object in Listing 10.11, we will only index the `Company` property^[We actually index the `History` property as well, but as a json string, which isn't really useful for our purposes here].
> We can't ask for the result for a company that has purchases for a specific product, or made
> a specific amount of purchases. If we want to do that, we need to make sure that the data
> is indexed as a first level property.

With this, we conclude our Map/Reduce discussion. You have all the tools and knowledge you need to start writing Map/Reduce indexes and query them. Almost. Because there is still one important aspect of Map/Reduce that we haven't touched. The Multi Map/Reduce index!

## Multi Map/Reduce index

### Join

### Join vs. Map/Reduce

## Dynamic aggregation