
# Map/Reduce Indexes

Map/Reduce is a programming concept that was made popular after Google started talking about doing their Page Rank algorithm using it. The idea isn't new, but the scale in which Google applied it was.

You've probably heard about Map/Reduce in the past, at least in passing. But my experience have shown that this is a subject most people have marked with "There Be Dragons" in their head and consider this something that is only relevant for Big Data or special use cases.

The terminology used is a strong part of that. The terms Map/Reduce actually refer to function name in Lisp and to a common usage pattern in functional languages. All of which result in even technical people assuming that this is nerd stuff that they don't need to care about.

Another way to talk about Map/Reduce is to talk about aggregation. In particular, aggregation of a data set to a smaller summary. I assume that you are familiar with SQL, so you shouldn't have a problem understanding the SQL statement in Listing 10.1.

```{caption="{Aggregation using GROUP BY in SQL}" .sql}
SELECT o.CategoryId, COUNT(*) TotalProducts
FROM Products p
GROUP BY o.CategoryId
```
If you aren't familiar with SQL, don't worry. What this statement is doing is to get the total amount of all purchases for each customer. Nothing surprising or scary about this, right?

The statement in Listing 10.1 is something that most developers encounter on a regular basis. We are just doing a bit of aggregation, nothing as fancy or hard as Map/Reduce. But the interesting thing about Map/Reduce is that it is best when it is doing exactly what Listing 10.1 is doing. Aggregation over a data source to get the summary. In Listing 10.2, the SQL statement was translated into a Linq expression.

```{caption="{Aggregation using GroupBy in Linq}" .cs}
from o in products
group o by o.Category into g
select new 
{
	Category = g.Key,
	Count = g.Count()
}
```

So far, it is perfectly obvious what is going on here. Let us take Listing 10.2 and turn that into a Map/Reduce definition. You can see that in Listing 10.3.

```{caption="{Aggregation using Map/Reduce}" .cs}
var mapResults = // map operation
	from o in products
	select new 
	{
		o.Category,
		Count = 1
	};

var reduceResults = // reduce operation
	from result in mapResults
	group result by result.Category into g
	select new
	{
		Category = g.Key,
		Count = g.Sum(x=>x.Count)
	};
```

Listing 10.3 isn't actually all that different from Listing 10.2. Instead of doing everything in a single expression, we have simply split the operation into two expressions. One that just gather the details, and another that will perform the actual aggregation. But even though Listing 10.2 and Listing 10.3 are very similar, Listing 10.3 is the more complex one. So why go with that approach?

The reason for splitting an aggregation operation like that is very simple, it allow us to break apart the actual execution into distinct parts, that can run independently from one another, and _that_ in turn, allow us to do some really interesting things with aggregation computation.

## The magic of Map/Reduce

So Map/Reduce is basically a complex way to do a Group By, nothing more. Why add this complexity? Because having the steps of the aggregation process split into an explicit map and reduce steps allow us to treat each of them in isolation. And that lead to an interesting observation. If we can treat each step in isolation, that means that we aren't required to run them sequentially and at the same time.

The common case for running Map/Reduce is that it allows us to easily parallelize the operation. And we can do that on multiple machines to quickly aggregate very large amount of information. That was the original usage of Google's Map/Reduce framework.

Let us see how we can process a Map/Reduce operation on the data showing in Table 10.1.

Id 		  	Name 		 					Category
--------- 	------     						---------
products/37 Gravad lax  					categories/8
products/38 CÃ´te de Blaye   				categories/1
products/39 Chartreuse verte   				categories/1
products/40 Boston Crab Meat   				categories/8
products/41 Jack's New England Clam Chowder categories/8

: Sample data for Map/Reduce test run 

First, we need to run the map. We want to see how Map/Reduce allow us to work with large amount of data, so we'll limit ourselves to 3 rows per run. So we run the map twice, on the first three rows and then on the last two rows. The results are shown in Table 10.2 and table 10.3.

Category 		Count
------------ 	------
categories/8	1
categories/1 	1
categories/1 	1

: Running the Map operation on the first three rows of Table 10.1

Category 		Count
------------ 	------
categories/8 	1
categories/8 	1

: Running the Map operation on the last two rows of Table 10.1

We have run the Map operation, now it is time to run the Reduce operation. The results are shown in Table 10.4 and Table 10.5.

Category 		Count
------------ 	------
categories/8	1
categories/1 	2

: Running the Reduce operation on Table 10.2

Category 		Count
------------ 	------
categories/8 	2

: Running the Reduce operation on Table 10.3

We are almost done, but we still have two separate tables, and no final results. In order to get those, we now need to run the Reduce again. But this time (and this is crucial for understanding), we are going to run the Reduce on its own output. Take a look at Figure 10.1 to see how the results are processed.

![Parallel Execution path of Map/Reduce](./Ch10/map-reduce.png)

Map/Reduce seems to add a lot of extra complexity. We could have just aggregated everything in a single step, and it would be much easier to understand, faster and easier all around. But Figure 10.1 is the key for understanding why Map/Reduce plays such a role in Big Data and NoSQL systems. Map/Reduce allows us to process the data in parallel, and there is no requirement that all the data would be present on a single machine.

### The Map process

The first stage in the Map/Reduce process is the Map, obviously. The map is responsible for getting just the relevant information for the aggregation that we are performing from the source data. Usually, the map select just a few fields. In the case of the Map/Reduce in Listing 10.3, the Map will select the `Category` and the `Count` properties.

Actually, `Count` isn't even a real property, we are setting that value to 1. That seems strange, can't we just do a `Count()` in the Reduce instead of creating this fake property?

The answer is that we cannot, and the reason we cannot is that while the Map function is only applies once on the source input, the Reduce function can be applied multiple times. See Figure 10.1 and the Reduce being operated on the results in the Map Threads and the Reduce Threads.

Because the Reduce function is applied incrementally and recursively, it needs to be able to process its own output. The easiest way to ensure that it can do that is to have both the Map and Reduce functions output the data in the same shape. 

Map isn't really all that interesting, to be fair. It just gets the relevant data, the interesting bits happen in the Reduce.

### The Reduce process

After we mapped the source data, we start aggregating it. In the case of the Map/Reduce process in Listing 10.3, we are aggregating the results by the `Category`. A common mistake when writing Reduce functions is assuming that they only run once, and on the full data set. As you can see in Figure 10.1, that is certainly not the case.

![Distributed process of Map/Reduce process](./Ch10/distibuted-map-reduce.png)

As the name implies, Reduce is generally meant to accept large input and have a much smaller output. Most aggregation tasks do this naturally (sum, count, etc). Note that any Reduce function has the notion of the `reduce key`, the value on which we are aggregating. In the case of the Map/Reduce process in Listing 10.3, the `reduce key` is the `Category` property. 

Right now, it doesn't seem very meaningful, because we are processing the entire thing inside a single machine as a single batch (even if Figure 10.1 showed it running with multiple concurrent steps). Take a look at Figure 10.2, which shows how we can split the process among multiple machines.

In this case, we process half the source data on each machine, and send to the other machine the _reduced results_ for the `reduce keys` that this machine handles. In Figure 10.2, Machine #1 run Map/Reduce on the data as part of initial stage. Then send the partial results is has for the `reduce key` with the value `categories/8` to Machine #2, where it is combined with that machine's partial results to compose the final results. We don't have any results for the `categories/1` value, so we don't send anything back to Machine #1.

The major benefit that we gained here is that we only had to send the reduced results. And as the name implies, that is far smaller amount of data to send, and most of the hard part part was already done in the initial Map/Reduce on the origin machine. In this manner, we are able to aggregate large amount of data without having to shuffle the entire dataset through the network.

## Map/Reduce in RavenDB

RavenDB uses Map/Reduce extensively, but it isn't usually used to handle distributed Map/Reduce tasks^[Those are possible, using the Sharding features.]. Instead, RavenDB uses Map/Reduce to handle aggregation tasks inside a single node. In other words, it is close to Figure 10.1 in operation than Figure 10.2.

Why? We already established that Map/Reduce is more complex than simple aggregation. The reason for going with Map/Reduce was to parallelize the work and allow to distribute it, but if RavenDB doesn't take advantage of data distribution for aggregation (unless you use Sharding, discussed in Chapter ???), why is it using Map/Reduce at all?

![Incremental Map/Reduce execution ](./Ch10/map-reduce.png)

The reason is quite simple. RavenDB isn't distributing the aggregation operation between different machines. At least not in the traditional sense. What it does instead is distribute the aggregation operation _over time_. RavenDB is one of the few databases that supports incremental and resumable Map/Reduce operations.

Consider the process outlined so far, we gather the source data (in RavenDB's case, the documents) and run them through the Map function. Then we run the results of the map through the Reduce function to get the final result. We are done. Almost. Because there is one thing that we haven't talked about.

What happen when users modify the data? For example, by adding a new product? You can see what RavenDB will do in Figure 10.3. Instead of re-executing the entire Map/Reduce operation, which may require us to go over the entire dataset, we can run the Map and Reduce functions on the new product, get the `Category` that it belongs to, and then load the results from the previous Map/Reduce run. Now we can simple run the Reduce over the new results and persisted results, getting to the final answer.

Note that the amount of work that we actually had to do is fixed. Run Map/Reduce on the new data, reduce it with the old data. That means that the action of updating the aggregation results is very cheap. Because we are only maintaining the running tally, instead of having to go and start the whole process from scratch.

> **Single step & multi step reduces**
> 
> The process shown in Figure 10.3 isn't actually how this works. We can't just apply the reduce over the 
> values and the new ones blindly. We might have an update (maybe the product 
> categories changed?), so it isn't that simple.
>
> RavenDB handles this by automatically selecting the appropriate strategy. If there aren't many values for 
> a specific `reduce key` (in our case, not many products per category), we'll just reduce all of the saved
> Map outputs together with the new map value. This still saves a lot of work, because we don't have to load
> the relevant documents and run the map again, and we store the saved map data in a way that make it cheap to
> load all the saved Map results for a particular `reduce key`. This is called the single step 
> reduce strategy.
>
> However, if there are a lot of values for a `reduce key`, we choose the multi step reduce strategy instead. 
> "A lot" in this case is over 1,024 values for a single `reduce key` (more than 1,024 products per category).
> In that case, we'll save not only the map results, but also the intermediate reduce results. The actual 
> process is complex and not really interesting externally. We split the data into buckets and steps. In the 
> first step (`reduce0`), we have 1,048,576 buckets, and map results are spread across those buckets in 
> an even manner. The next step (`reduce1`) have 1,024 buckets, and in the last step (`reduce2`) we only 
> have a single bucket.
>
> When we need to run the Map/Reduce again (a new product in a big category), we don't need to re-reduce 
> the whole thing. We can get away with running the Map on the new product, then reducing the result with 
> the relevant buckets. We need to go through the full three reduce steps, but the advantage here is that we
> actually work on drastically smaller data set, and still are able to update the aggregated value quickly.

## After Map/Reduce

A Map/Reduce index in RavenDB is still an index, and the final results of the index are going to be stored to the Lucene index. That means that we can search on those results, to find the already aggregated values very quickly. 

If we have a very large product catalog, asking how many products are in a specific category is very cheap. All we need to do is check the index for that category's value, where we'll find the aggregated result. Instead of having to go through all the products, RavenDB just hand you the right result, previously computed and at near to zero cost.

The fact that we put the Map/Reduce results inside a Lucene index means that we can apply all of the usual suspects on the index. We can make complex queries, facets or even suggestions. 

The fact that the aggregation already happened by the time you query the database has a huge impact on the way we structure our applications. Consider for example a business requirement that we'll show for each customer how many orders they have with us and for what amount. If you are using a relational database, issuing such a query on every page load is expensive. Not only that, but the more important the user is (the more orders they have), the slower the system will respond.

That is the point where you start looking at caching, invalidation, keeping track of the totals manually, etc. A very simple feature escalate to a very complex one, because we can't just hit the database with a complex group by query on large customers at each page view.

With RavenDB, you define the Map/Reduce index, and you query it as you need. Because the computation doesn't happen during the query, the actual query execute in a constant amount of time and you get the results immediately. Caching, invalidation and re-running the computation are all handled by RavenDB internally. 

A simple feature _remain_ a simple feature.


## The costs of Map/Reduce indexes

## Impure functions and other sins 