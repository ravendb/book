
## Static Indexes and other advanced options

[Static indexes]:(#advanced-indexing)

In the previous chapter we looked at how we can query RavenDB and we mostly let the query optimizer to handle indexing things for us.
In the next chapter, we'll dive into the actual details about how indexes are implemented in RavenDB and the wealth of metrics into how 
indexes work and behave, which is very useful for performance monitoring and troubleshooting issues. This chapter, however, will discuss
primarily the how you can define your own indexes, why you would want to do that and what sort of options this provides you.

Indexes in RavenDB are split across multiple axes:

* Auto (dynamic) indexes vs. Static indexes.
* Map only indexes vs. Map/reduce indexes.
* Single collection indexes vs. Multi collection indexes. 

In addition to the types of indexes, they also offer quite a bit of features and capabilities, so it is a big topic to cover but also
one that give you tremendous amount of power and flexibility.


### What are indexes?

Indexes are how RavenDB can answer questions about your documents without contiously scanning the entire data set each and every time. 
An index can be created by the query optimizer or by the user directly. An index work by iterating over the documents and building a map 
between the terms that are indexed and the actual documents that contain them. After the first indexing run, the index will keep update
that map as updates and deletes happen on the database.

Listing 10.1 shows a very simple way to construct an index. The code in Listing 10.1 has nothing to do with RavenDB but is provided so
we'll have a baseline from which to discuss how indexes work.

```{caption="Creating an index over users' names" .cs}
Func<string, List<User>> BuildIndexOnUsers(List<User> users)
{
    var index = new Dictionary<string, List<int>>();
    for (var i = 0; i < users.Count; i++)
    {
        if (index.TryGetValue(users[i].Name, out var list) == false)
        {
            list = new List<int>();
            index[users[i].Name] = list;
        }
        list.Add(i);
    }

    return username =>
    {
        var results = new List<User>();
        if (index.TryGetValue(username, out var matches))
        {
            foreach (var match in matches)
                results.Add(users[match]);
        }
        return results;
    };
}
```

The code in Listing 10.1 is meant to convey the sense of what is going on. We are given a list of users, and we iterate over the list
building a dictionary that would allow fast access to user by name. This is all that an index is, effectively. It trade off the cost 
of building the index with significant reduction in the cost of the query. 

If there is a query that you only want to perform once, you are probably better off just scanning through the entire dataset, since that
is what creating an index will have to do anyway. But if you intend to query more than once, an index is a fine investment. Consider the 
two options shown in Listing 10.2.

```{caption="The right and wrong way to use an index" .cs}
// the right way
Func<string, List<User>> findUserByName = BuildIndexOnUsers(largeNumberOfUsers);
List<User> usersNamedOren = findUserByName("Oren");
List<User> usersNamedArava = findUserByName("Arava");

// the wrong way
List<User> usersNamedOren = BuildIndexOnUsers(largeNumberOfUsers)("Oren");
List<User> usersNamedArava = BuildIndexOnUsers(largeNumberOfUsers)("Arava");
```

In the first section in Listing 10.2, we generate the index and then use is multiple times. In the second section, we create the index 
for each query, dramatically increasing the overall cost of making the query.

The code in Listing 10.1 and Listing 10.2 is about as primitive an index as you can imagine. Real world indexes are quite a bit more
complex, but there is a surprising amount of details that are actually unchanged between the toy index we have here and real world 
indexes used in RavenDB.

Indexes in RavenDB are implemented via the Lucene search library, hosted inside Voron, the RavenDB's storage engine. An index in RavenDB
can contain multiple fields and a query can be composed of any number of clauses that operate on these fields. But in the end, we end up
with a simple search from the queried term to the list of documents that contain the term in the specified field, just like our 
dictionary usage in Listing 10.1.

#### Indexes come in layers, and with an identity crisis

I'll be the first to admit that it can be quite confusing, but RavenDB actually have several different things internally that are all 
called index. At the lowest level, we have Voron indexes, which is how RavenDB organize the data on persistent storage. As a user, you
don't have any control over Voron indexes. A Voron index is used to find a document by id, for example, or to return a list of documents
that belong to a particular collection, without any further filters. 

A Voron index is updated by the stroage engine directly as part of the transaction and is always kept in sync with the data. Unlike the 
async nature of higher level indexes in RavenDB, these indexes gurantee full consistency to the reader. They are also called storage 
indexes sometimes. You'll rarely be able to see them in use or affect them in any way, but they are crucial for the well being and 
performance of RavenDB.

I did mention that this is confusing, right? Because while we call these Voron indexes, our regular (exposed to the user) indexes are
also stored in Voron, but instead of being part of the storage engine, they are stored _inside_ the storage engine, to ensure that 
the index data is transactionally safe and can recover from an error such an abrupt shutdown.

Even for the user visible indexes (what you'll generally mean when you are talking about indexes) there are several different levels. 
At the outer edge of the system, you have the index that was defined for you by the query optimizer or manually created by you. This,
however, is not the actual index, but rather the index _definition_. It merely tell RavenDB how you want to index the data. 

Then, we have the actual process of indexing the data (which reside in the `Index` class and its derived classes) and the actual output
of the indexing process, which is also called an index. 

The good new about all of this naming mess is that you are very rarely going to need to think about any of that. As far as the outside
world can tell, RavenDB allow you to define indexes and that is pretty much it. But if you are reading the code or interested n the 
implementation details, you need to remember that when we are talking about an index, you might want to verify _what_ we are talking 
about. 

For this chapter, we'll use the term `index definition` for the definition of what is going to be indexed and the term `index` for the 
actual indexed data that is generated from the indexing process.

#### The first index

In the studio, go to the `Indexes` section and then to `List of Indexes`. Click on the `New index` button and give the index the name
"MyFirstIndex". This screen allows you to define an index by writing the transformation function from the document format to the 
actual indexed data. 

RavenDB uses C# and Linq to define indexes and Figure 10.1 shows the simplest possible index. Indexing the `FirstName` and `LastName`
from the `Employees` collection.

![A simple index over the `Employees` collection](./Ch10/img01.png)

An index is just a C# Linq expression on a collection that outputs the values that we want to index. The index in Figure 10.1 is not
a really interesting one and usually will not be a good candidate for a static index. There is nothing there that cannot be done 
using an auto index that the query optimizer will create for us. Nevertheless, let us see how we use such an index. 

Listing 10.3 shows how we can query this index. Instead of specifying that we'll use a collection, which will cause the query optimizer
to take over and select the best index to use, we are explicitly specifying that we want to use "MyFirstCollection" index.

```{caption="RQL query using an explicit index" .sql}
from index 'MyFirstIndex' where FirstName = 'Andrew'
```

Except for the explicit index, the rest of the query looks very familiar. Indeed, everything that we have gone over in the previous 
chapter still apply, the difference is that in this case, we have an index that define the shape of the result in a strict fashion.
That doesn't seem like such a good idea, until you realize that the shape of the index and the shape of the source document does not
have to be the same. Consider Listing 10.4, which shows an updated definition for "MyFirstIndex" which index a computed value rather
than the actual values on the document and Listing 10.5, which shows how to query the new index.


```{caption="Indexing a computation allow us to query over the computed value" .cs}
from emp in docs.Employees
select new 
{
    Name = emp.FirstName + " " + emp.LastName
}
```

```{caption="Querying over the computed field" .sql}
from index 'MyFirstIndex' where Name = "Nancy Davolio"
```

The result of Listing 10.4 and Listing 10.5 is quite interesting, so it is worth examining further. In Listing 10.4 we define an index
whose output is a computed field which is the result of a concatenation of two values. In Listing 10.5 you can see that we are querying
over the index and finding a result, even though the source document never contained such a field. 

The example here is quite silly, I'll be the first to admit, but it show off nicely a very important feature. You can run computation 
during the indexing process, and they query over the result of said computation. This is quite powerful, because it allows you to 
do some pretty cool things. For example, consider Listing 10.6 which shows an index ("Orders/Total" in the sample data set) that does
a more interesting computation.

```{caption="Computation during indexing can be arbitrarily complex" .cs}
from order in docs.Orders
select new { 
    order.Employee,  
    order.Company, 
    Total = order.Lines.Sum(l=>(l.Quantity * l.PricePerUnit) *  ( 1 - l.Discount)) 
}
```

In Listing 10.6 we can see that we compute the total value of an order. The formula we use is not too complex, but it is also not 
trivial. What makes this interesting is that this allow us to run a query such as the one in Listing 10.7.


```{caption="Querying over computed field as well as sorting by it" .cs}
from index 'Orders/Totals'
where Total > 100
order by Total as double desc
```

The query in Listing 10.7 is an important example because it demonstrate a few key concepts. Once the indexing process is done, the 
computed field is just a field. It means that you can filter using this field as well as sort by it. The computation has already 
happened at indexing time, and the cost of the query in Listing 10.7 is an index seek to the relevant location in the index and then 
sorting the results according to the indexed value. 

> **Index definition must be a pure function**
>
> RavenDB places several limitations on the index definition. Primarily, it requires that the index definition will be a pure function.
> That means that for the same input, the index definition will always produce the same output. One of the reasons that RavenDB uses
> Linq for defining indexes is that it is quite easy to define a pure function using Linq. In fact, you need to go out of your way to
> get non deterministic output from a Linq expression. And the syntax is quite nice, too, of course.
>
> In particular, usage of date time functions, `random` or trying to access external resources is not allowed. This allow RavenDB
> to assume that identical inputs will produce identical outputs and is important for re-indexing and updates. 

Most importantly, this query involves _no_ computation during its execution, only index operations. In contrast to a comparable query
in SQL, which would have to sum all of the order lines for each of `Orders` table, we can take an enormous shortcut by
running the computation once during indexing and reusing it in our queries. 
We are going to spend quite some time exploring what kind of fun we can have with this feature. 

> **Computation during indexing vs. computation during query**
>
> In the previous chapter, we mentioned that RavenDB does not allow you to perform queries that will require computation during the
> query, such as: `from Employees where FirstName = LastName`. In order to answer such a query RavenDB will need to check each individual document,
> which is incredibly expensive. 
>
> You can get an answer to the question you actually want to ask: "Do I have users whose first and last name match?", though. You 
> do that using a static index, such as:
>
> ``` // Employees/FirstAndLastNameMatch index definition
> from employee in docs.Employees
> select new
> {
>    FirstAndLastNameMatch = employee.FirstName = employee.LastName    
> }
> ```
>
> And you can query this index using the following query: `from index 'Employees/FirstAndLastNameMatch' where FirstAndLastNameMatch = true`.
>
> This query can be performed as a simple indexing operation instead of an expensive full scan. This is a common thing to do with RavenDB,
> shift the cost of the computation to indexing time as much as possible. Queries are far more frequent than updates, so this kind of cost
> shifting make a lot of sense. Of course, even so, if you have several operations you want to do on a specific collections you are better
> off with having them all on a single index rather than having a separate index for each.

Doing computation during indexing is a neat trick, but how does RavenDB handle the case where a document was updated? That is quite 
simple, as it turns out. All RavenDB needs to do is to simply run the udpated document through the indexing function again and index
the resulting values. 

#### How the index actually works

There are a lot of moving parts here, so we need to clearly define what the terms we use mean:

* Document - the RavenDB JSON document.
* Index entry - all of fields and values that has been indexed from a particular document. Frequently it will be a subset
  of the fields from the document that is being indexed, but it can be some computed fields as well.
* Term - the actual indexed value that is stored in the index. This is usually the same as the value of the field being
  indexed, but it can be different if you are applying full text search.

For example, let us assume that we have the JSON document in Listing 10.8 and we query using `from Docs where search(Name, 'Arava')`. 

```{caption="Sample document that is about to be indexed" .json}
{
	'Name': 'Arava Eini', 
	'Nick': 'Dawg', 
	'@metadata': {
		'@id': 'dogs/1',
        '@collection': 'Dogs'
	} 
}
```
What will happen happen is that RavenDB will produce an index entry from this document that
will have the following structure: `{'Name': 'Arava Eini'}` and mark the `Name` field as using full text search. This require
additional processing and the actual terms that will be indexed are shown in Listing 10.9.
 	
```{caption="Index terms after the analyzing for full text search" .json}
index = {	
	'Name': {
		'arava': ['dogs/1'],
		'eini': ['dogs/1']
	}
}
```

The `search(Name, 'Arava')` will be then be translated into what is effectively an search on: `index.Name['arava']` to get the proper
matches. 

This isn't how the index work _at all_. But it is a very good lie, because it allows you to reason about what is going on and make intuative
guesses about the behavior of the system without actually having to deal with the full complexity of managing an index. For example, you can 
see from the data we keep in the index in Listing 10.9 that we aren't storing the full document in the index, instead, we only store the 
document id.

This means that the query pipeline first need to run the query on the index and get the list of document ids that are a match for this query 
and then go to the documents storage to load the actual documents from there. 

> **Queries, stale indexes and ACID documents, oh my!**
>
> In the previous chapter we talked about async indexes and the possibily that a query will read from an index before it is done indexing 
> new or modified documents. An interesting wrinkle here is that the index does not contain the actual document data, so after the index
> is done giving us the document ids, we need to go to the documents storage and load the documents.
> 
> One of the promises that RavenDB provides is that document reading is always ACID and must be consistent (within a single node, at least).
> That means that even if the index itself hasn't caught up to changes, the data it pull from the document store will have everything up to date.

Another important aspect of how queries work in RavenDB that you can see in Listing 10.9 is that the `Name` field was not indexed as a single 
term. In other words, if we would look for `index.Name['Arava Eini']`, we'll not find anything in the index. This is because we search on the
terms on the index. And during indexing, we broke the name to its constituents parts and lower cased it. At query time, we can apply the same
transformation and be able to find the individual terms.

If we were indexing the name _without_ full text search, we would index the term `arava eini`. So the only thing this will allow is to query
using case insensitive manner. Using `exact()`, of course, will store the term in the index as is and will require a case sensitive match.

We already saw, in Figure 9.4 in the previous chapter, that you can pull the terms that are actualled indexed from RavenDB and inspect them, 
which can help you understand why your queries return the results they do. 

All of this explanation is here to hammer home the fact that at the index level, we aren't querying on your documents' properties, we are 
querying on the output of the indexing function, and that may bear little resemblence to how you index looks like, the `Total` field in 
Listing 10.6 serve as a good example for this feature. The documents do not have a `Total` property, but we can compute it during indexing
and query on it.

> **Security considerations**
> 
> It is worth noting that a static index is just a C# Linq statement, which means that you have a lot of power in your hands. An index can 
> transform a document into an index entry in some pretty interesting ways. Combined with the fact that the shape of the index entry and the shape of 
> the data can be completely disconnected from one another and it's easy to understand why we'll spend most of this chapter just skimming over all 
> that you can do with indexes.
> 
> This power has a downside. Static indexes _can_ do everything, including running arbitrary code. In fact, they _are_ arbitrary code. Defining static 
> indexes is an operation that is limited to database administrators, for that reason. Auto indexes defined by the query optimizer do not have this
> issue, obviously and will be defined for users of all permissions levels.
> 
> You can also use the `Additional Sources` feature in indexes to expose additional classes and methods to the index definition. This goes beyond having
> a simple Linq expression and allow you to run any code whatsoever on the server. This is mostly meant to allow you to perform complex logic in the 
> indexing and enable advanced scenarios (using custom data types on client and server, with their own logic, like `NodaTime`).
> You can real more about the `Additional Sources` feature in the online documentation.

We can also _filter_ data out during indexing. The indexing function takes a collection of documents and return a collection of index 
entries. It can also return _no_ index entries, in which case the document will not be indexed. This can be done using a `where` in the 
Linq express that compose the index definition. Listing 10.10 shows an example of filtering out employees without a manager for the
"Employees/ByManager" index^[The general recommendation is that you'll have a single index per collection with all the fields that you 
want to search on defined on that index. It is better to have fewer and bigger indexes than smaller and more indexes.].

```{caption="Filtering documents during indexing" .cs}
from employee in docs.Employees
where employee.ReportsTo != null
select new
{
    employee.ReportsTo
}
```

Only employees who has someone to report to will be included in this index. There isn't usually a good reason to filter things during
the indexing process, the reduction in the index data size isn't meaningful and you'll usually better to have this in the query itself
where you can make such decisions on a per query basis. We'll see another usecase for filtering the raw data to the index in a bit, when
we'll dicuss mutli map indexes.

#### Storing data in the index

A query in RavenDB will go to the index to find the results (and the order in which to return them) and then usually grab the document ids
from the results and load them from the document store. Since the typical query will return full documents back, that is usually what you'll 
want to do.

Sometimes, such as with the `Total` field in Listing 10.6, you want to compute a value during indexing and use it in your projection. By 
default, RavenDB will store in the index only enoguh information to handle the query, and not to get data out of the index. So as it stands,
we'll need to recompute the `Total` after the query. 

We can ask RavenDB to store the field. Go to "Indexes" and then "List of Indexes" and clikc on "Orders/Totals" index. Thie will take you to 
the index edit screen, click on "Add Field" and then set `Total` as the field name and set "Store" to `Yes`. You can now save the index.
This setting tells RavenDB that it need to store the value itself (and not just the parts that it indexed) in such as way that we can later
retrieve it.

We can project the `Total` field from the query, as you can see in Listing 10.11.

```{caption="Projection of a stored computed field from " .sql}
from index 'Orders/Totals'
where Total > 10.0
select Total, Company, OrderedAt
```

Listing 10.11 also shows that we can project a standard field, `Compnay`, without storing it. This works because if the value isn't stored in the index
we'll try to get it from the document. Last, we also project the `OrderedAt`, which follow the same logic. It isn't stored in the index, so it is 
fetched directly from the document. 

Stored fields are used primarily to store the result of such computations, but there is a small perfromance advantage in projecting everything
from the index. We don't need to do the document load, and in some very specific circumstances, that might be helpful. Document load is a _very_ 
cheap process in RavenDB. It happens so frequently it has been heavily optimized, so there is usually not much point trying to store fields in the 
index for that reason.

Storing data in the index will also increase its size and the time it take to actually index, since it needs to do more work. Unless you actually need
to get the projection out, it is usually not worth it.

### Querying many source at once with Mutli Map indexes

Sometimes, just querying a single source is not enough. In the sample database, we have several types of users. We have `Employees`, the `Contact`
person on `Companies` and the `Contact` person for `Suppliers`. If an application needed to allow a free form search over all of these users, how
would that work? Will we need to perform three separate queries? We _could_ do that, but that would be pretty complex. Instead, we can define an 
index that will use more than a single source of data. 

Go to `Indexes` and then to `List of Indexes` and click on `New Index`. Name the new index "People/Search" and click on `Add map` twice. You can
see the content of the map functions in Liting 10.12.

```{caption="These map functions allow us to query over multiple sources with ease" .cs}
from e in docs.Employees
select new
{
    Name = e.FirstName + " " + e.LastName
}

from c in docs.Companies
select new 
{
    c.Contact.Name
}

from s in docs.Suppliers
select new 
{
    s.Contact.Name
}
```

The three map functions in Listing 10.12 each points to a different collection. RavenDB will use the information from all three map functions to 
generate a single index. This means that you can now query on all of these collections as a single unit. There are a few things to notice, though.
First, multi map indexes require that all the maps in the index will have the same output. Note that we indexed a `Name` field in all three maps
even though the `Employees` collection have no such field. 

The other important factor is that it is usually awkward to have to deal with heterogeneous result set. When you are querying, it is often really
nice to know what shape of data to expect. A query on a multi map can return any of the collections that the multi map index covers. Because of 
that, it is usually best to project the data out into a common shape, as you can see in Listing 10.13.

```{caption="Projecting data from multiple collections into a common shape" .sql}
from index 'People/Search' as p 
where Name in ('Mary Saveley', 'Nancy Davolio', 'Wendy Mackenzie')
select
{
    Collection: p["@metadata"]["@collection"],
    ContactName: (p.Contact || { Name: p.FirstName + " " + p.LastName }).Name
}
```

The output of the query in Listing 10.13 can be seen in Figure 10.2. You can also remove the `select` clause for this query to see how the query
results will change when you get three different types of documents back from a single query.

![Common output shape for results from mutli map index](./Ch10/img01.png)

In a way, the multi map feature is similar to a union in relational database. It allows you to query over multiple source and get the results back
from any of them. However, there is no limit on the shape of the results like there would be with a union, although that is quite convenient. 

### Full text indexes

The query in Listing 10.13 is nice, but it awkward, we wouldn't want to ask our users to specify the full name of the person that they are trying to 
find, we'll typically want to do a smarter search. A... full text search, one might even say.

In the previous chapter we looked at some full text search queries. They look like `search(Name, 'Nancy')` and allowed us to efficeintly search for
results without the expense of scanning all of them. Listing 10.9 is a good example of how RavenDB breaks up the terms during indexing for quick 
lookups and speedy searches. But so far we only looked at it as dynamic queries. How do I make use of the full text search capabilities of RavenDB 
using static indexes?

Full text search in RavenDB is composed of the following elements:

* The analyzer that you have selected.
* The values and manner in which you are querying.
* The field or fields that you are indexing.

The analyzer for full text search is defined on a field or fields on the index and determine how RavenDB will break apart the text to individual 
terms. We have seen such example in Listing 10.9, but let us consider the following string: `"The white knight is running to the princess' tower
to slay the dragon"`. How would full text search operate on such a string?

#### Full text search analysis

RavenDB will hand over this string to the analyzer and get a list of terms back. This is extremely simplistic view of what is actually going on
but it is important to understand it so you'll have the proper mental model of what is actually going on under the covers. The simplest analyzer
will do nothing to the text provided to it, and you will get it back as is. This is what `exact()` does with dynamic queries, it let RavenDB 
know that we should use the noop analyzer which does case sensitive matches.

The default analyzer that RavenDB uses is not doing much more than that, merely allowing us to query in a case insenstive manner. This is done 
by lowering the case the input string (with all the usual case required for Unicode aware programs, of course). It is important to note that 
the analyzer is run during indexing and during query time. In this way, what ends up querying the actual index data structures is a value that
has been passed through the analyzer during the query and compared to a value that was passed through the analyzer during the indexing.

Just lowering the case of strings isn't that interesting, I'll admit, but analyzers can do much more. By default, when you use `search()` you'll
use an analyzer that is called the standard analyzer, and that is when things start to get interesting. The analyzer will break the input string
into individual terms, on a word boundary. So the previous string will be broken up to the following terms:

* the x 3
* white
* knight
* is
* running
* to x 2
* princess'
* tower
* slay
* dragon

Note that we have lower cased the terms and that the term `the` appears three times and `to` appears twice. 
In many languages, there are certain words that appear so frequently that they are meaningless noise in most cases. 
In English, those would be words like: `a`, `the`, `to`, `from`, `is`, `are`, etc. 
These are called stop words and are stripped from the terms that the analyzers will return because they add no semantic value to the search results.

The terms we end up with are:

* white
* knight
* running
* princess
* tower
* slay
* dragon


Note the possesive s on the `princess'`, that is also something that the standard analyzer has removed. We could also reduce words to their stems, 
such as turning `running` in `run`. The standard analyzer doesn't do this, but you can select an analyzer that would do that. Analyzers are usually
specific for the language (and sometimes even the specific business domain) that you are working on. The standard analyzer is a good default for 
English (and most latin based) languages, but sometimes you'll need more. In that case, you can look at available analyzers and use them. A full 
list of the analyzers available by default with RavenDB can be found on the online documentation.
Because RavenDB uses Lucene under the covers, it is quite easy to find analyzers for most needs available that can be readily used by RavenDB.
You can also define you own custom analyzers quite easily as well.

Let us look at how analyzers modify how RavenDB index documents. Figure 10.3 shows a sample documents that we'll focus on.

![Sample documents for playing with full text search](./Ch10/img03.png)

Go ahead and create this document and then create an index named `test/search` with the index definition in Listing 10.14.

```{caption="This index definition uses the same field three times, to allow different indexing" .cs}
from n in docs.Notes
select new 
{
    Plain = n.Description,
    Exact = n.Description,
    Search = n.Description
}
```

After adding the index definition, you'll need to customize the way RavenDB will index these fields. Click on the `Add field` button and enter "Search"
as the field name, then click on `Advanced` and select `Search` as the value for the `Indexing` drop down. Add another another field and enter "Exact"
in the name field, then click on `Advanced` and select `Exact` as the value for the `Indexing` drop down. You can see how this should look like in 
Figure 10.4.

![Configuring the `test/search` index fields with different analyzers](./Ch10/img04.png)

With that done, you can click on `Save` and we are done. You can now go to the index `Terms` to see the different indexing methods that were used 
on each of these fields. The results are shown on Figure 10.5.

![The indexed terms for the same values using different analyzers](./Ch10/img05.png)

The `Plain` field was indexed as is, but in lower case (note the first `the` in the string). The `Exact` field is almost the same, but preserving the
casing (again, notice the `The` at the beginning). And the `Search` field is probably the most interesting one. There, we can see the whole process
of breaking it up into individual terms, filtering out stop common words and stripping the posessive s out. 

Now, let us do some queries on this index. 

#### Full text search queries

Querying a full text search field is an interesting experience, because what we think we are doing and what is actually happening is so often drastically 
different, yet the end result is the same. Consider the following query: `from index 'test/search' as n where search(n.Search, "princess flower tower")`.

If you'll run this query, you'll find that this actually matches the document, even though the word `flower` is no where to be found. This is because
of the way RavenDB will process queries on full text search, it is enough that we have _any_ match to be considered enough to return the result. More 
advanced options, such as phrase queries are also available when using Lucene directly, such as the following queries:

* `where lucene(n.Search, ' "princess tower" ')` - phrase query match (note the `"` in the query), because there is a `princess` followed by a `tower` in 
  the text, even though we don't have the `'` in the query.
* `where lucene(n.Search, ' "running princess" ')` - also a match, because the word running is followed by a princess (with the stop words removed).
* `where lucene(n.Search, ' "running knight" ')` - not a match, there is no `running` followed by `knight` in the text.

As you probably remember the `lucene()` method allow you to drop down directly into the Lucene syntax and compose complex full text search queries. I'm
using it here primarily because it allow to demonstrate that the way full text search matches work are not quite so simple. You can read more about the
full power of Lucene in the online docs, but I want to focus on understanding how the queries work. Let us take for example the `search()` method and
how it operates.

The `search()` method accept the query string that you are looking for, and pass it to the analyzer for the specified field. It then compares the terms
that the analyzer returned with the terms already in the index, and if there is a match on any of them, this is considered to be a match for the query. 
There is also the ranking of the results to take into account. The more terms are matched by a particular document from the query, the higher it will
be in the results. This is affected by such things as the term frequency and the size of the document and a lot of other things that I'm not going to
cover but are quite interesting to read about^[See the Lucene in Action and Managing Gigabytes books, recommened in the previous chapter.].

What happens when we are making a query on a full text field (one with an analyzer defined) without using `search()`? Let us see:

* `where n.Search = "princess tower"` - no match, there is no term `princess tower` for this field.
* `where n.Search = "dragon"` - match, there is a term `dragon` for this field.

This is really strange, isn't it? But take a closer look at Figure 10.5 and it will be clear what is going on. There is a term `dragon` there, and when
we use equality comparison, we compare against the terms directly, so we find a `dragon` but we don't find a single term `princess tower`. When we use
`searc()` or `lucene()` we are perfoming more complex operations, which allow us to do more interesting queries.

For the same reason, it is not meaningful to talk about _sorting_ on a full text field. The value you are sorting on can be any of the terms that were
generated by the analyzer for this field. If you want to sort on such a field, you need to index it twice, once as full text field and one as a normal
field. You'll search on the full text field and sort on the normal field. 

This leads us nicely to an important disucssion, how to work with fields in the index.

#### Full text search fields

RavenDB doesn't require anything to match between the source document and its index entry. We saw that previously in Listing 10.14, when we indexed a 
single field from the document three different times, with different analyzers for each field. In many cases, you'll use the same field names, but there
is no requirement to do that. This behavior is intentional, because it gives you a lot of freedom with regards to how you are able to build your index
and perform queries.

Listing 10.14 showed an example of how we can index a single field multiple times. Listing 10.15 shows the index definition for `Compaines/Search`, 
which show the reverse scenario, where we are searching over multiple fields in the document using a single field in the index entry.

```{caption="Combining severl document fields in a single index field" .cs}
from c in docs.Companies
select new
{
    Query = new[] {
        c.ExternalId,
        c.Name,
        c.Contact.Name
    }
}
```

The index definition in Listing 10.15 add three different fields to the `Query` field on the index entry. When creating this index definition, you 
also need to register the `Query` field as full text search (as we have done with the `Search` field in the previous example). The question is, what
does this gives us? 

This type of index is typically used to serve search pages directly. For example, we can run the following queries on this index:

* `from index 'Companies/Search' where search(Query, "ALFKI")` - Search companies using the external id.
* `from index 'Companies/Search' where search(Query, "Alfreds")` - Search companies by full text search on the company name.
* `from index 'Companies/Search' where search(Query, "Anders")` - Search companies by full text search on the contact person's name.

Note that in all cases, we get the same result (`companies/1-A`). A user can type any of the above into the search text box and get the result they 
are looking for. Gone are the days of search pages with dozens of fields and extra long waiting times. You can search on any of the interesting fields
that a client may remember without any hassle.

This is something that is quite easy to do, but can significantly improve the life of our users, because now they have much greater freedom in querying
and they don't need to limit themselves to knowing exactly what value to search in what field. One of the things I recommend you'll do in such pages
is to directly go into the details page if there is just one result. This gives the users the impression that they can type just enough for your 
system to recognize who they are talking about and take them to the right place. It may seems like a small thing, but these are the kind of things that
make a user really appreciate a system. 

Of course, things aren't always this easy. What happens if I know what I'm searching for, but I can't quite get it right enough for the index to find it?
There is alot more that you can do with indexes in RavenDB.

### Getting the most out of your indexes

Indexes are typically used to answer very specific questions such as: "Give me all the documents that match this criteria in this order". But RavenDB
indexes are actually capable of doing much more. In this section, I want to highlight three of the more interesting capabilities of the indexes in 
RavenDB.

* Suggestions - allowing you to ask RavenDB what the user probably _meant_ to ask about.
* More like this - suggesting similar documents to an existing one. 
* Facets - slicing and dicing of the data to provide you with detailed insights into what is going on in large result sets.

I'm going to cover them briefly here, mostly to introduce them and explain where and how they should be used. I'll leave the details on all the myriad
of options and advanced features they have to the online documentation.

#### Suggetsions

In the sample data, we have the `companies/8-A` document, it is for a company from Spain, whose owner name is: `Martín Sommer`. Note that diatric over 
the `í`. It is reasonable for someone to not notice that and search for `Martin`. In which case, they'll find nothing. This can be frustrating for the 
user. We do have a few ways in which we can help the user find what they are looking for.

In the same way we will automatically go to the details page if there is only a single result, we'll also ask RavenDB if it can think of what the user
meant to ask for. This can be done using the Suggestions feature. Before we can use it, though, we need to enable it in the index, as shown in 
Figure 10.6.

![Marking a field as having the suggestions feature](./Ch10/img06.png)

With this change, we let RavenDB know that we will asking it to suggest options to queries that the user have made. This typically require RavenDB to
spend more time in indexing, preparing all the options that a user can misspell a search query, but the results can be astounding to users. Consider
the query in Listing 10.16.

```{caption="Querying for suggestions for a misspelled term" .sql}
from index 'Companies/Search' 
select suggest(Query, "Martin")
```

We are asking RavenDB, what could the user have meant with "Martin" on the `Query` field? RavenDB will try to look at the data in the index for this
field and infer the intent of the user. If you care to know the details, RavenDB breaks the terms into pieces during the indexing process and scramble
them to simulate common errors, that all goes into an index that is queried during query. This does increase the cost of indexing, but the cost of 
querying suggestions is typically very low. I wouldn't suggest^[pun intended.] to apply this globally, but for user facing searches, the dataset it 
typically pretty stable, so that works out great.

The result of the query in Listing 10.16 can be seen in Figure 10.7. 

![Suggestions for alternatives for "Martin"](./Ch10/img07.png)

You can use the results of the suggestion query to show results to the user or you can ask them what they meant, similar to how Google does it. 
This feature tend to get enthusiastic response when users run into it. 

I gave an example in Unicode, because that is easy to see how it would be hard to use, but the same is possible using any type of misspelling, such as 
listing 10.17.

```{caption="Asking RavenDB to suggest other options for 'Summer' from the terms in the index" .sql}
from index 'Companies/Search' 
select suggest(Query, "Summer")
```

The query in Listing 10.17 will give `sommer` and `cramer` as the possible suggestions for `summer`. 

I focused heavily on finding what the user meant when they misspelled something and didn't find anything, but suggestions can also be very useful when 
you did find something, but want to let the user know that there are additional alternatives that they might want to explore. Sometimes this can give
the user a reason to go exploring, although the More Like This feature is more appropraite there.

#### More like this

My toddler likes playing "what looks like this" and it is a lot of fun. In a more serious setting, there are a lot of use cases of "find me more stuff
that looks like this". In bug tracking, it might be finding a previous occurance of a bug. With a product catalog, that might be finding another items
that is roughly the same. 

> **What the more like this feature is and isn't**
>
> The way more like this works is quite simple, under the covers. You mark a field as full text search and define it to have a term vector. These two
> together provide RavenDB the ability to build a query to find similar documents. There is a bit of smarts around how we decide how the query should
> actually look like, but that is the basis of how it works.
>
> In many cases, this simple approach works very well to find similar and related documents. Especially if your data set is large and the documents
> and data you are indexing are complex. In this case, the more data you have, the more information RavenDB has to figure out what is a similar 
> document and what are just random noises, common in your domain.
>
> This isn't the basis of a recommendation engine, though. It is a good start and allow you to get off the ground running and demonstrate a feature
> quickly, but while there are quite a lot of options and tuning that you can do (see the online documentation for the full details), it is 
> a feature that was developed to find documents based on shared indexed terms, nothing beyond that. True recommendations engines can do much more.

To make true use of the more like this feature, we typically use a large dataset and utilize large text fields that give us enough information to 
distinguish between noise and what is of real value. This is especially true when we are talking about user generated content, such as comments, 
posts, emails, etc. It is of somewhat less use for static fields, but there are still quite a few interesting usecases that we can use this feature
for.

We'll start by defining the `Orders/Search` index, as shown in Listing 10.18.

```{caption="Index definition to use as more like this target to find similar orders"}
from o in docs.Orders
select new{
    Address = new[]{o.ShipTo.City, o.ShipTo.Country},
    Products = o.Lines.Select(x=>x.Product)
}
```

Before we can start performing more like this query, we need to configure a term vector for these fields, as shown in Figure 10.8. 

![Defining term vectors (for use with more like this) in the `Orders/Search` index](./Ch10/img08.png)

Setting a field to use a term vector tells RavenDB that we need to store enough information about each index entry to be able to tell not just from a
term what index entries contained it, but also to be able to go from an index entry to all the terms that it contained. A term may appear multiple
times in an index entry (common when we are using full text search). For example, the word `term` has appeared multiple times in this paragraph.
A term vector for the paragraph will contain a list of all the unique words and how often they appeared. 

> **Why didn't we define full text search on the fields in `Orders/Search`?***
>
> You might have noticed in Figure 10.8 that we _didn't_ set the fields to use full text search. Why is that? The values that we are going to
> put into these fields (a city, a country or a product id) are all terms that we don't wish to break up. In this case, using full text search
> on those fields would break them apart too much. For example, if we have a city named: "New York", that _is_ the city name and we don't want
> to break it to "New" and "York". 
>
> Another thing to note is that the `Address` field in the index is actually using an array to store multiple values in the same field. This is
> similar to using an analyzer, but for one off operations, this is often easier. You can look at the index terms for the index to see what got
> indexed.

With all of the preperations complete, we can now actually make our first more like this query, which is shown in Listing 10.19.

```{caption="Finding similar orders to `orders/535-A` based on products ordered and the city and country it was shipped to"}
from index 'Orders/Search' 
where morelikethis(id() = 'orders/535-A')
```

The query ask RavenDB to find similar orders to `orders/535-A` based on the `Orders/Search` index. This index has two fields marked with term 
vectors, which are used for this purpose. The `orders/535-A` document has a single purchased product (`products/31-A`) and was shipped to 
Buenos Aires, Argentina. Based on this, RavenDB will construct a query similar to this one: 
`where Address = 'argentina' or Address  =  'buenos aires' or Products = 'products/31-a'`

Note that the casing on the query parameters is intentional, because the data comes directly from the index terms, which were lowered cased
by the default analyzer. I'm doing this to explicitly show the source of the data.

The result of the query in Listing 10.19 is 66 results. The same as if we would replace the `morelikethis()` call with the equivalent query
that we figured out mantually. Note that issuing the query on `orders/830-A`, for example, will have 25 items in the query that will be generated.
Why go into all this detail on how this is actually implemented? Well, consider the implications. As we know, in such queries, the more matches
we have in the query, the higher the rank of a result, so this explains how we find get the more like this aspect. We query on the matches, and
the higher the number of matches, the more we'll consider it similar to the target and return it.
Magic, demystified, but hopefully still pretty cool and a very nice way to give the user something to follow up on. 

I'm not going to cover all the options of `morelikethis()` here, but I wanted to point out that you have a great deal of control on exactly
RavenDB it going to match these documents. Take a look at Listing 10.20, where we want to apply the `morelikethis()` only on the `Address` field.

```{caption="More like this query limited to a single field" .sql}
from index 'Orders/Search' 
where morelikethis(id() = 'orders/535-a', '{"Fields": ["Address"] }')
```

The results of the query in Listing 10.20 is just 15 similar orders, those that were sent to the same city and country. Other options allow to 
decide what terms will be considered for the `morelikethis()` query based on their frequency of use and overall popularity. You can find the
full details in the online documentation.

We are now going to look at another facet of the RavenDB querying capabilities, facets and how we can use them to get all _sort_ of information
about the result set for your queries.

#### Facets

To query is to ask a question about something. Usually the questions we ask are some variant of "give me all the documents matching this pattern", 
such as when we ask to get the last 50 orders from a particular customer. This is well understood and easy to reason about. We run into problems
when the size of the result set is so big that it is not meaningful for the user.

Consider the case of a support engineer fielding a phone call. The customer reported an error, so the enigneer search the knowledge base for all
articles with "error" in them. As you can imagine, there are likely going to be quite a few of these. The poor engineer is unlikely to find 
something meaningful using such a strategy. On the other hand, very often we have a pretty good idea about the general query we want, but not a
clue on how to narrow it down. 

Facets are quite widely used, but they are the sort of feature that you don't really pay attention to. A good example of that would be YouTube.
As you can see in Figure 10.9, searching YouTube for "Dancing" is an interesting experience. How would I be able to choose from over 280
million different videos? 

![Faceted search in YouTube](./Ch10/img09.png)

Facets allow me to narrow down the search quite easily, by exposing the inner structure of the data. There are two searches shown in Figure 10.9,
the first without any filters and the second with applying the 4K and uploaded today filters. This reduced the number of results to a far more 
managable number. It also gives me additional information, such as the fact that there are no matches with the type Show in the results. Another
good example of facets is with commerce. If I want to buy a new phone, I have way too many options. Searchign eBay for "phone" gives me over 
300,000 results. Figure 10.10 shows how eBay uses facet to help you narrow down the selection to just the right one.

![Facets can help a customer to narrow down to the exact product they want](./Ch10/img10.png)

In some cases, this is just there to help you feed the system the exact query it needs. In many others, this is actively assisting the user in
figuring out what kind of questions they need to ask. The feedback from the numbers in Figure 10.10, in contrast to the match / no match 
indication in Figure 10.9 is another factor, giving the user the ability to guide their searches.

Facets are a cool feature indeed, let us see how you can use them in RavenDB. Facets require that you'll define an index for the fields that you
want to query and apply facets on. In this case, we'll use the `Product/Search` index in the sample data set. We'll start with the simple faceted 
query shown in Listing 10.21.

```{caption="Range and field facets on product search" .sql}
from index 'Product/Search'
select 
    facet(
        PricePerUnit < 10, 
        PricePerUnit between 10 and 50, 
        PricePerUnit between 51 and 100, 
        PricePerUnit  > 100
    ) as Price,
    facet(Category),
    facet(Supplier)
```

The results of the query shown in Listing 10.21 can be seen in Figure 10.11. The query itself is composed of three facets. A range facet on the 
`PricePerUnit` field and two field facets on `Category` and `Supplier`. As you can see, in the case of the range facets, we grouped all the 
matches in each particular range, and in the case of the field facet, we group by each individual value.

![Faceted query results](./Ch10/img11.png)

The query in Listing 10.21 is simple, since it has no `where` clause, this is where you'll typically start. Just giving the user some indication
on the options they have for queries. Let's say that the user picked suppliers 11 and 12 as the ones that they want to drill down into. The query
will then look like the one in Listing 10.22.

```{caption="Faceted search over particular suppliers" .sql}
from index 'Product/Search'
where Supplier in ('suppliers/11-a', 'suppliers/12-a')
select 
    facet(
        PricePerUnit < 10, 
        PricePerUnit between 10 and 50, 
        PricePerUnit between 51 and 100, 
        PricePerUnit  > 100
    ) as Price,
    facet(Category),
    facet(Supplier)
```

In Listing 10.22 we are querying over the same facets, but we are now also limiting to just particular suppliers. The output of the `Price` facet
will change, as shown in Listing 10.23.

```{caption="`Price` facet output from the query in Listing 10.22" .json}
{
    "Name": "Price",
    "Values": [
        {
            "Count": 1,
            "Range": "PricePerUnit < 10"
        },
        {
            "Count": 6,
            "Range": "PricePerUnit between 10 and 50"
        },
        {
            "Count": 0,
            "Range": "PricePerUnit between 51 and 100"
        },
        {
            "Count": 1,
            "Range": "PricePerUnit > 100"
        }
    ]
}
``` 

As you can see, the number of results per each range has changed, to reflect the new filtering done on the query. 

The facets portion of the query is the very last thing that happens, after the entire query has been processed. This means that you can use 
any `where` clause you want and filter the results accordingly. However, any query that uses `facet()` must return _only_ `facet()` 
results and cannot use clauses such as `include` or `load`. 

Faceted queries are typically combined with the same query, sans the facets, to show the user the first page of the results as they keep
narrowing down their selections. You'll typically use the `Lazy` feature to combine such multiple queries, as was discussed in 
[Chapter 4](#advanced-client-api). 

### Spatial indexes

In the [previous chapter](#query-engine) we discussed spatial queries and used them with automatic indexes. Using a static index gives 
you a high degree of control over the spatial indexing that RavenDB will perform on your data. Let's create a new index, called 
`Companies/Spatial`, as shown in Listing 10.24.

```{caption="Defining an index with spatial support" .cs}
from c in docs.Companies
select new
{
    Name = c.Name,
    Coordinates = CreateSpatialField(
            c.Address.Location.Latitude, 
            c.Address.Location.Longitude)
}
```

The `CreateSpatialField` method instructs RavenDB to use the provided latitude and longitude to create a spatial field named `Coordinates`.
As usual, even though there are some companies with a null `Address.Location`, we can safely ignore that. RavenDB will handle null propagation
during the indexes and save us from all those null checks.

With this index, we can now perform spatial queries, you can see such a query in Listing 10.25, querying for companies withing a mile of
605 5th Ave S, Seattle.

```{caption="Spatial querying for companies using a static index"}
from index 'Companies/Spatial' 
where spatial.within(Coordinates, 
    spatial.circle(1, 47.5970, -122.3286, 'miles'))
```

The query in Listing 10.25 has a single result, the "White Clover Markets" company. You can also use the `CreateSpatialField` to pass a `WKT`
string represent any arbitrary shape that will be indexed by RavenDB. So far, this seems to be pretty much the same as we have previously 
done with auto indexes. Static indexes allow you to customize the spatial indexing behavior, so let's see how. 

Go to the `Companies/Spatial` edit index page and click on the `Add field` button, set the `Field name` to `Coordinates` and click on the 
`Spatial` toggle. The result should be similar to Figure 10.12. 

![Spatial field indexing options](./Ch10/img12.png)

You can use these options to have fine grained control over exactly how RavenDB will index your spatial data and process spatial queries.
Of particular interest is probably the `Max Tree Level` field, which control how precise the spatial queries are going to be and directly
relates to the cost of spatial indexing.

#### How RavenDB handles spatial indexing and queries

This section isn't going to be an exhuastive study of how spatial indexing work, nor dive too deeply into the actual implementation. The 
first is too broad a topic for this book and there are excellent resources online and the second is unlikely to be of much interest to
anyone who isn't actually implementing the querying support. Full details on the spatial behaviors and options are available in the online
documentation.
What this section _is_ going to do is to give you a good idea on how RavenDB perform spatial queries, enough so you'll be able to reason
about the impact of the decisions you are making. 

RavenDB offers three different spatial indexing strategies:

* Bounding box
* Geohash prefix tree
* Quad prefix tree

To demonstrate the difference between these strategies, we'll use the `Terms` feature to see what is actually going on under the covers.
Go into `Companies/Spatial` and edit the `Coordinates` field to the `BoundingBox` strategy. Click on `Save` and then go into the index 
terms. You should see something similar to Figure 10.13.

![Bounding box indexing behind the scenes](./Ch10/img13.png)

The bounding box strategy is the simplest one. Given a spatial shape, such as a point, a circle or a polygon it computes that shape's 
bounding box and index its location. These are the `Coordinates__minX`, `Coordinates__minY`, `Coordinates__maxX` and `Coordinates__maxY`
fields that you can see in Figure 10.13.

As for the actual values of these fields, these are the spatial coordinates that match the bounding box. Whenever you make a query 
RavenDB will translate the query to the same bounding box system. You can see a sample of this query translation in Listing 10.26.

```{caption="Translated spatial query using bounding box" .sql}
// actual query
from index 'Companies/Spatial' 
where spatial.within(Coordinates, spatial.circle(1, 47.5970, -122.3286, 'miles'))

// what actually gets executed
from index 'Companies/Spatial' 
where Coordinates__minX >= -125.3341 and
    Coordinates__maxX <= -119.3230 and 
    Coordinates__minY >= 45.5707 and
    Coordinates__maxY <= 49.623237136915
```

As you can imagine, this is a pretty cheap way to handle spatial queries, both during the indexing portion and at the time of the query.
However, it suffer from a number of issues related to the accuracy of the solution. In many cases, you wing it and get by with bounding
box but in truth it is limited in the kind of queries it can perform and how well it can answer them. In particular, the bounding box
assumes that the world is flat (or at least that the bounding box is small enough that it can ignore the curvature of the earth).

Moving to the next option, let's look at the geohash strategy. Go to the index, update the spatial option to `GeohashPrefixTree` and 
save the index. Now, go to the index terms, and you'll find something similar to Figure 10.14.

![Geo hash indexing behind the scenes](./Ch10/img14.png)

The pyramid that you see are the actual geohashes, but before we can start talking about how RavenDB uses them, we need to explain what
they _are_. The way geohashes work is by dividing the world into a grid with 32 buckets. Then dividing each bucket in the grid further
into another 32 buckets, etc. You can play with geo hasing in an interactive manner using the following sites:

* [http://geohash.gofreerange.com/](http://geohash.gofreerange.com/)
* [https://rawgit.com/rzanato/geohashgrid/master/geohashgrid.html](https://rawgit.com/rzanato/geohashgrid/master/geohashgrid.html)

![A map showing the top level of geohash](./Ch10/img15.png)

By looking at the map in Figure 10.15 and the terms from Figure 10.14 we can see that the prefix `6` covers most of South America. The
next level we have, `69` is mostly Argentina and `69y` is Buenus Aires. In other words, the longer the geohash, the more precise it is.

![A map showing multiple level of geohash](./Ch10/img16.png)

Look at the spatial indexing options in Figure 10.12, you can see the `Max Tree Level` there, which determine the accuracy of the 
spatial indexing. This determine the length of the geohash. A tree level of 9 (the default) gives us a resolution of about 2.5 meters.
That somewhat depend on the exact location on the earth that you are searching. 

When indexing a shape, RavenDB will pixelate it to the required resolution and enter the geohashes that cover it. The more irregular
the shape and the higher the precision required, the more work is needed to generate the terms that match the query. At query time,
we do the reverse and find the matches. Note that with the geohash strategy, the geohash is used to find the relevant shapes in a 
rough fashion, afterwhich we check on all the shapes inside the geo hash whatever they matched our actual query. 

In other words, with spatial queries using geohash (or quad), there are two stages to the query. First do a rough match on the shapes, 
this is what you can see in the index terms. Second, you have the actual spatial geomtery check to see if the shape matches the query.

The quad tree strategy is similar in many respects to the geohash but uses a different coordinate system (a grid of 4 buckets, hence
the name quad). Quad buckets are always squares, while geohash buckets can be rectangular. This might make a difference if you are 
using heatmaps and want a more predicatable zoom in / out. 

![Coordinates terms when indexing using quad prefix tree](./Ch10/img17.png)

Geohash is more widely used and supported by many platforms and tools. The selection of geohash or quad typically don't have any 
major affects on indexing speed or queries, it primarily depends on what you are going to be using this for. If you are going to 
dispaly spatial data, you'll probably want to select a mode that works best with whatever mapping component you are using. 

Bounding box strategy is much cheaper at indexing time and can produce very quick queries, at the expense of the level of accurancy
that you can get. This is mostly good if what you typically care about is rough matches. For example, if your query is something like:
"find me the nearest restaurants", you don't typically care too much about the shape. On the other hand, if you are asking for: "give 
me all the schools in this district", that is something quite different and you'll care about the shape that the query is 
processing and be willing to pay the cost for an exact answer.

The performance cost of spatial indexing is directly related to the tree level you chose, and a very granular level with complex 
shapes can cause long indexing times. You'll usually not notice that, because indexing is async and doesn't hold up other word, but
it can impact operations when you are creating a new index or storing large number of entities to the database all at once. In 
particular, if you have expensive spatial index, it is usually better to avoid indexing documents that has very high rate of change 
since RavenDB will need to re-index them every time, even if the spatial fields didn't change.

In such cases, it might be better to move the spatial data to a dedicated collection containing just that, so the documents with the
spatial data will only be indexed if they change. This is typically not required, since even with spatial indexing, the indexing is
fast enough for most cases, but it can be a valid approach if you need to speed up indexing times and are getting held up by the 
spatial indexing level you require.

Splitting the spatial data to a separate collection brings up an interesting issue, though. How do we deal with indexing of data that
is located on multiple documents?

### Indexing referenced data

The rules for document modeling we have gone over call for documents to be independent, isolated and coherent. Documents should be 
able to stand on their own without referencing other documents. Sometimes, however, you need data from a related document to create a 
good search experience. 




### Dynamic data
