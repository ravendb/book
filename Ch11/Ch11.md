
## Map-Reduce in RavenDB

Map-Reduce is an old term. It came from Lisp and was used as early as the 1960s. For a long time, it was primarily known only for
functional language afficandos and very rarely if ever seen outside their circles. In 2004, the Google paper
[MapReduce: Simplified Data Processing on Large Clusters](https://research.google.com/archive/mapreduce-osdi04.pdf) was released 
and Map-Reduce was instantly a big hit in the distributed programming circles. Everyone had to have a distributed map-reduce
implemention.

RavenDB is a distributed database, and it uses Map-Reduce. However, it does not do so in the context of distributed computing. 
Instead, RavenDB is using Map-Reduce for aggregation of data on each node independently. If you are used to Map-Reduce jobs that
are running on large clusters and processing terrabytes of data, that might look very strange. 
But RavenDB isn't using Map-Reduce to break apart large computation across different machines, instead, it uses Map-Reduce to 
break apart computation across _time_.

It is usually easier to explain with an example, so let's jump into that.

### Executing simple aggregations

Listing 11.1 shows a simple aggregation query, giving us the total number of orders and items purchased by a particular company.

```{caption="A simple aggregation query in RavenDB" .sql}
from Orders as o
group by o.Company
where o.Company  = 'companies/1-A'
select count() as NumberOfOrders, 
	   sum(o.Lines[].Quantity) as ItemsPurchased, 
	   o.Company
```

The query in Listing 11.1 should be very familiar to anyone who has used SQL before. Now, let us analyze what RavenDB must do in
order to answer this kind of query:

1. Find all the `Orders` documents where the `Company` field is set to `'companies/1-A'`.
2. Iterate over all of those, count their number and sum the number of line items in each.
3. Return the results to the client.

This seems quite straightforward, but it has a couple of issues. In particular, the first two steps. The sample data we are using
has just over a thousand documents in the database. There isn't much that we can do to make any query expensive over that dataset.
However, with real world datasets, we will typically deal with collections that contain hundreds of thousands to many milliions 
of documents. 

Consider the first step, finding all the `Orders` documents with the `Company` field set to a value. If I have a few millions 
documents to scan, that alone can be quite expensive, both in I/O (to read the data from disk) and computation (to check equality
so often). This protion is quite obviously something that we can optimize with an index. It is the next portion that is much 
harder to work with.

If a company has a _lot_ of orders, then the process of iterating over each of these orders can be extraordinarily expensive. 
Consider the case where you want to show the number of orders and the number of items purchased on the company page. This is a 
small amount of information which can tell how important this particular customer is. 

However, if we would need to repeat the second step each time that we run the query, that can get expensive very quickly. What is 
worse is that the more important the company, the more orders and items this particular customer purchased from us, the slower 
things will become. This method won't work, it is punishing success, and that isn't something we want.

Typically you will not run such queries directly, because of their cost and time to run.
Instead, you'll write a job that will run these queries at idle times and cache the results. Then you only have to deal with cache invalidation, making sure that this job is never run under load, explaining to users that the results are delayed and...

It's complex. All we wanted was to put a couple of numbers on a page, and suddenly you need to deploy a background job, monitor
its cost and execution and wait until the daily or weekly run to get updated numbers. Such high cost of aggregation will usually
cause such features to be dropped.

And even if you don't have enough data to require such measures, aggregation queries are still typically very costly for the 
database. Enough so that they are used sparingly in most systems.

RavenDB's aggregation doesn't work like this. 

#### The gory details of aggregation in RavenDB

Instead of gather all the data and then aggregating it all in place, RavenDB uses Map-Reduce to break apart the aggregation 
computation into discrete steps, the Map and the Reduce. Let's look at how RavenDB is _actually_ processing the query in 
Listing 11.1. Run the query in Listing 11.1 and then click on the index button in the results, as shown in Figure 11.1.

We are going to get deep into how RavenDB is handling aggregations, you'll not typically need to know that level of details, 
and can feel free to just skip this section. I'm including this here because it is important to understand the implication of
how things work, that aggregation queries in RavenDB are _very_ cheap.

![Getting to the index details on the query page](./Ch11/img01.png)

You can already see that an aggregation query in RavenDB is also using an index. In addition to the usual options, such as the 
index terms and performance statistics, there is also the Map-Reduce Visualizer, which we'll look in more detail later. For now
click on the `View Index` option, which should open the index details page, shown in Figure 11.2.

![A Map-Reduce index aggregation `Orders` by `Company`](./Ch11/img02.png)

Figure 11.2 shows the structure of the index. Operating over `Orders`, it is grouping by the `Company` and getting then 
aggregating over them. One thing to note here is that there is no mention anywhere of `companies/1-A`. Even though the query in
Listing 11.1 mentioned it, the index is not operating on that particular value, but on the generic concept of aggregating by
the `Company` field.

In other words, as usual, RavenDB has looked at the query and generalized the operation to answer any such question using any 
`Company`. But what about the aggregation?

Aggregation is actually handled via two separate actions, `map` and `reduce`. The first stage is running a `map` operation on
each of the documents, grabbing just `Company`, `sum(Lines[].Quantity)` and `Count = 1` from each of the `Orders` documents, the 
second stage is to group them all by the `Company` and run the `reduce` operation to get the final result by each of the 
`Company` field values.

If this doesn't make sense to you, don't worry, RavenDB contains a few tools specifically to help you understand how the 
`map-reduce` process work.  

In the `Indexes` tab, go to `Map-Reduce Visualizer` and then select the `Auto/Orders/ByCountAndLines[].QuantityReducedByCompany`
index from the drop down. Then search for the following document ids: `orders/396-A` and `orders/445-A`. The result should look
similar to Figure 11.3.

![The Map-Reduce Visualizer allows us to inspect the internal structure of the index](./Ch11/img03.png)

Clicking on the `{"Company":"companies/1-A"}` rectangle in Figure 11.3 will give us more details about that particular value, as
you can see in Figure 11.4.

![A single reduce result and the total reduced value for a map-reduce index](./Ch11/img04.png)

With the details in Figures 11.3 and 11.4, we can now see exactly what we mean when we talk about `map` output and the resulting
aggregation. The documents we selected (`orders/396-A` and `orders/445-A`) both belong to `companies/1-A`, and we can see that 
for `orders/396-A` the `map` output was `{"Company": "companies/1-A","Count": 1,"Lines[].Quantity": 38}`. Indeed, if we'll go 
and inspect the document, we'll see three line itmes, with quantities of 15, 21 and 2, totalling 38. For `orders/445-A` we can
see that the total quantity is 20, with a single line item. 

This is interesting, but what is even more interesting is the aggregated value. For `companies/1-A`, you can see the aggrated 
values of a total of 6 orders for this company with a final quantity of 174 items ordered. Clicking on the aggregation summary
will take us even further down, into the inidivual page entries, as you can see in Figure 11.5.

![Individual mapped entries for `companies/1-A`](./Ch11/img05.png)

In Figure 11.5 you can see all the details for each of the entries for `companies/1-A`. This is the lowest level we need to 
inspect as well as the reason why RavenDB implements aggregation in such a manner. I already spent several pages just explaining
what is going on, why is the aggregation implementation for RavenDB so _complex_?

The reason is quite simple, actually. The reason for the complexity is that we don't run aggregation queries once, instead, we
compute the aggregation result once, and then we store it. When we issued the query in Listing 11.1 we queried only for results
for `companies/1-A`, but the index that the query optimizer generated for us applies to all companies.

In fact, if we would now run the same query, but for `companies/2-A`, we'll be able to reuse the same index, and in fact, we'll 
have to do very little work. The query will use the index and fetch the already pre-computed results for `companies/2-A` and
won't have to actually perform any aggregation whatsoever. All the work has already been done. 

As great as that is, you might be asking yourself why this level of complexity. After all, surely we could have done the same
without so many moving parts, right? This is correct, but there is one additional item that we need to consider. How are we 
going to handle updates?

The map-reduce indexes in RavenDB aren't simply a cache of the already computed results, instead, we store the data in such a 
way that make it cheap to also update the results. Consider what will happen inside RavenDB when a new order comes in. We'll run
the `map` portion of the index, getting the `Company`, `sum(Lines[].Quantity)` and `Count = 1` from the newly created document.

The easiest way to visualize that is to just add another row to Figure 11.5. At this point, RavenDB can then just aggregate the
new results alongside the already existing result and get to the final tally. In other words, the complexity here exists in order
to allow RavenDB to efficently update map-reduce results when documents are created or updated.

This works great when we have a small number of items to aggregate, such in the case with `companies/1-A`, but what happens when
the number of items grow? Let's increase the number of documents that we are aggregating by a hundred fold and see where that 
takes us. Go do `Documents` and then `Patch` and run the update script in Listing 11.2.

```{caption="Increase the number of Orders documents by a hundred" .sql}
from Orders 
update {
    for(var i = 0; i < 100; i++ ){
        put("orders/", this);
    }
}
```

After running this script, we should have about 83,000 Orders documents in the database. Where previously we had 6 entries for
`companies/1-A`, we now have 600. Let's look at how that work. Go back to the Map-Reduce Visualizer and select the
`Auto/Orders/ByCountAndLines[].QuantityReducedByCompany` index and then add the `orders/396-A` and `orders/445-A` documents.
The result is shown in Figure 11.6

![Showing 600 mapped entries for `companies/1-A`](./Ch11/img06.png)

This is very similar to how it looked before, and indeed, the structure of the data is exactly the same. This is because the
amount of entires for `companies/1-A` is still quite small. Let's select another pair of documents, this time belonging to
`companies/77-A` and see what kind of structure we have there. Add `orders/77-A` and `orders/146-A` to the visualizer and 
see what the result looks like. You can see the results in Figure 11.7.

![Map-reduce works as a tree once we have enough entries for a particular value](./Ch11/img07.png)

On the left in Figure 11.7 you can see that unlike for `companies/1-A` in Figure 11.6, were we had a flat list, Figure 11.7
shows a tree structure. Indeed, once we are past a certain size, RavenDB will start processing map-reduce entries in a tree
like fashion.

Consider an update to `orders/77-A` and how RavenDB will apply it. First, we'll run the `map` on the updated document, giving
us the map entry to write to `#1028`. Then, we'll run the reduce on that page, giving us the final tally for this page. We'll
then recurse upward, toward `#1437`, where we'll also run the reduce.

> **Using aggregation in RavenDB**
>
> Aggregation operations in RavenDB are cheap, both to compute and to query. This is in stark contrast to the usual behavior of 
> aggregation queries in other databases. The more data you have to go through, the more efficently will RavenDB be able to 
> actually process and aggregate it. 
>
> While typically you'll run aggregation as a daily or weekly report (usually during off hours) and store these results for use 
> later, RavenDB allow you to just query the aggregated data and RavenDB will provide the answers you need as well as keep 
> everything up to date. 

The end result is that an update operation will take us a single `map` invocation and two `reduce` calls to update the final
result for `companies/77-A`. In other words, if we have a lot of data we have for a particular key, RavenDB will start segmenting
this data and apply aggregation operations in such a way to reduce the number of operations required to a minimum. This ensures
that not only are we able to answer queired efficently, but we are also able to update the map-reduce results with very little 
costs.

### Defining your own map/reduce indexes

In Listing 11.1 we have seen how we can query using `group by` and RavenDB will generate a map-reduce index for us behind the 
scenes. This works quite nicely, but there is only so much that you can do with a dynamic query before the query optimizer will
give up.

The query optimizer can do quite a lot, but in order to make sure that it is predictable, it is currently limited to recognizing 
and being able to generate map-reduce indexes from a fairly small list of predictable patterns. For more complex things, you'll 
need to create your own map-reduce index. Luckily, this is quite easy to do.

Let's say that we want to get the total number of sales and the total amount we made on each product. We cannot express this as 
a simple query, so we'll need to create an map-reduce index of our own for that. Go to the `Indexes` page and click on 
`New Index`. Name the index `Products/Sales` then click on `Add Reduction` button. Listing 11.3 has the contents of the `Map` 
and `Reduce` fields. After you have done filling these in, click on `Save` to create the new index.

```{caption="Compute total number of sales and revenue per product" .cs}
// map 
from o in docs.Orders
from l in o.Lines
select new
{
    l.Product,
    l.Quantity,
    Total = (l.Quantity * l.PricePerUnit) * (1 - l.Discount)
}


// reduce
from r in results
group r by r.Product into g
select new
{
    Product = g.Key,
    Quantity = g.Sum(x=>x.Quantity),
    Total = g.Sum(x=>x.Total)
}
```

Before we get to investigating how this index work, let's talk about what it does. The map portion runs over all the orders and 
all the line items for each order. Then, for each of the line items, we output an entry with the quantity sold and the amount
of money we made on this product. The reduce will group all these results based on the `Product` field and then sum up all the 
final numbers for the `Total` and `Quantity` sold. 

Now we can really see the separate operations of `map` and `reduce`, it might make more sense how they are truly two separate
operations. It is also important to understand that we aren't actually running the `map` or the `reduce` on the full results 
all the time. Instead, we break it apart internally and apply them to portions of the data each time. This leads to several 
important restrictions on map-reduce indexes:

* Both the `map` and `reduce` functions must be _pure_ functions. In other words, they should have no external input, and 
  calling them with the same input must always return the same output. In particular, usage of `Random` or `DateTime.Now` and
  similar calls is not allowed. 
* The output of the `map` is fed into the `reduce`, this is quite obvious, but what may not be obvious is that the output of 
  the `reduce` is _also_ fed into the `reduce`, recursively. In particular, you should make no assumptions in the `reduce` about
  the number of calls or the amount of data that you have to process in each invocation to `reduce`. 
* The output of the `reduce` must match the output of the `map` (because both are being fed back into the `reduce`, they must
  have the same structure). RavenDB will error if you have a different shape for each of the functions.

Because of these restrictions, RavenDB can apply the `map` and `reduce` functions in an incremental fashion and generate the 
results that we have already seen. This is the key for RavenDB's ability to compute aggregation cheaply over time.

We can now run the query in Listing 11.4 to find out the top grossing products:

```{caption="Finding the top grossing products across all orders" .sql}
from index 'Products/Sales'
order by Total as double desc
```

You'll note that the results of this query are effectively instantaneous, even though we have tens of thousands of records, 
RavenDB only need to look through 77 precomputed results. We can also sort by `Quantity` to find the most popular products. 

Go in to the `Map-Reduce Visualizer` and select the `Products/Sales` index and then enter `orders/6-A` and `orders/10-A` to
see the internal structure of the map-reduce index, as shown in Figure 11.8.

![Internal sturcutre of the `Products/Sales` index in the map-reduce visualizer](./Ch11/img08.png)

As you can see in Figure 11.8, all of the entries we have here are big enough to require a tree structure. If we'll look at
`products/39-A` you can see that this is holding just over 80,000 entries, with Figure 11.9 zooming into a single page inside
that tree.

![A single page holding over 25,000 map entries, due to high compression rate](./Ch11/img09.png)

Page `#578` holds over 25,000 entries for this product. How can a single page hold so many? The answer is that RavenDB will 
apply compression to the mapped entries. Since they are mostly very similar, they have a very high compression rate, allowing
us to pack a _lot_ of entries in a very small amount of space. This also adds to performance of the updating entries in the tree,
since we don't have to do as much work and the depth of the tree is much smaller.

This is about as low level as we'll get when discussing the map-reduce implementation. You know have sufficent information to 
have a good feel about the relative costs of using aggregation in general and map-reduce in particular in RavenDB. With that 
knowledge under your belt, let's explore some of the more interesting things that we can do with map-reduce indexes in RavenDB.

#### Common pitfalls with map-reduce indexes

Certain operations are not easy to perform using the limits on the `map` and `reduce` functions that RavenDB has. Probably the 
most obvious example here is the problem with calculating an average. Let's edit the `Products/Sales` index to add an average.
Listing 11.5 shows the updated definition with the most obvious (and wrong) way to do so:

```{caption="Computing average (wrongly) in map-reduce index" .cs}
// map 
from o in docs.Orders
from l in o.Lines
select new
{
    l.Product,
    l.Quantity,
    Count = 1,
    Total = (l.Quantity * l.PricePerUnit) * (1 - l.Discount),
    Average = 0, // so we'll have the same shape as the reduce
    Debug = new object[0] // same shape as reduce
}


// reduce
from r in results
group r by r.Product into g
select new
{
    Product = g.Key,
    Quantity = g.Sum(x=>x.Quantity),
    Count = g.Sum(x=>x.Count),
    Total = g.Sum(x=>x.Total),
    Average = g.Average(x=>x.Total),
    Debug = g.Select(x=>x.Total).ToArray()
}
```

Save the updated index and then run the following query: `from index 'Products/Sales' where Product = 'products/1-A'`, 
the result is showing in Figure 11.10.

![The wrong results shown for an average computation](./Ch11/img10.png)

The results shown in Figure 11.10 are wrong. The average price for a box of Chai (which is what `products/1-A` is) is not 
$184,514.014. So why are we getting this result? The answer is simple. The `Average` call is pretty simple, it simply sum
the elements that it got and then divide by the number of elements. But remember that `map` and `reduce` calls are not called
on each individual values, instead, they are called in batches. This means that by the the time the final call to `reduce` 
happened, it didn't get a flat list of all the results, but an aggregated list. Figure 11.11 shows the `Debug` field from
the index, which can allow us to better understand what is going on.

![The debug field from the `Products/Sales` index demonstrating the recursive nature of `reduce` operations](./Ch11/img11.png)

As you can see, the computation of the `Average` is correct, for the input that it got. Because we execute the `reduce` multiple
times, by the time the final round came, we fed it seven results, each of them aggregating about 500 documents.
In other words, when we write a map-reduce index, we need to take into account the fact that we never have access to the total
result set. This make sense, when you think about it, you don't want to have a single function invocation that has to go through
many tens of thousands of documents (or much more, on realistic data sets).

> **Do not assume that the order is fixed either**
>
> If your map-reduce index does work that rely on the order of operations or the order of items passed to the `map` or 
> `reduce` function, it is going to fail in a similar manner. This is common when you have calls to `First` or uses
> arrays in the map-reduce index. If you care about the order of operations, you'll need to ensure that as part of
> the map-reduce index and remember that the calls are made in a recursive manner. 

So how _can_ we compute an average? The answer is that we can do this quite simply. You can see it in Listing 11.6.

```{caption="Properly computing average in a map-reduce index" .cs}
// reduce
from r in results
group r by r.Product into g
let qty = g.Sum(x=>x.Quantity)
let total = g.Sum(x=>x.Total)
select new
{
    Product = g.Key,
    Quantity = qty,
    Count = g.Sum(x=>x.Count),
    Total = total,
    Average = total / qty
}
```

All we need to do for the proper computation is to make sure that we aren't assuming that the total dataset is passed to the
`reduce` call. In this case, we already have all the data that we need and we can just compute the final average. Note that we'll
still need to have a field called `Average` in the `map` function, because the shape must match. But because we are never reading
this field, we don't have to give it any special value and can just initialize it to any value we want.

Similarly to `Average`, we cannot use `Count` either in the `reduce`, for the exact same reason. That is why we use `Count = 1` 
in the `map`, to allow us the start the recursive computation of the total amount of items.

#### Complex map-reduce indexes

So far the map-reduce indexes we build were pretty simple, mostly doing trivial aggregation on numbers. To be fair, that is 
usually what you'll do with map-reduce indexes, but you can also do more with them. 
A good example would be to compute the total sales per company. This sounds like just taking the `Products/Sales` index and 
changing what we are grouping on, so let's make this more interesting. What we want to do is to get, per company, the total 
sales per product. 

Listing 11.7 shows the `Companies/Purchases` index, which does just that.

```{caption="Computing the total products sold for each company" .cs}
//map
from o in docs.Orders
from l in o.Lines
select new
{
    o.Company,
    Products = new []{ new { l.Product, l.Quantity } },
    Total = l.Quantity
}

//reduce
from r in results
group r by r.Company into g
select new
{
    Company = g.Key,
    Products = g.SelectMany(x=>x.Products)
        .GroupBy(x=>x.Product)
        .Select(p => new
        {
            Product = p.Key,
            Quantity = p.Sum(x=>x.Quantity)
        }),
    Total = g.Sum(x=>x.Total)
}
```

Taking a look at Listing 11.7, we can see that the `map` function looks pretty normal, but there is considerable amount of 
work that is going on in the `reduce`. Let's break it apart into its component parts and examine each independently.

First, the `map` generate an entry per line item, and the entry contains a single element `Products` array with the product's 
id and the amount sold. The reason we create the array in the map is that we need to match the shape of the `reduce`. In the
`reduce` function, we group the results by the `Company` and then generate the results for that particular company. The most
interesting tidbit happens when building the `Products` field. 

There, we apply _another_ `GroupBy` call to aggregate the data inside the `Company` once again. We use this to get the total
numbers of items purchased for each product, and this mechanism allow us to aggregate the data across mutliple invocations to
`reduce` safely and in a very natural manner. 

> **Reduce should... _reduce_ the size of the data**
>
> A somewhat overlooked requirement for the `reduce` function is that the amount of data going out of the `reduce` function
> should be smaller than the amount of data going in. All the examples we have seen so far had this properly, mostly because
> they aggregated information
>
> Let's look at an example where the amount of information that passed through `reduce` does not go down and discuss its 
> implications. In the `Companies/Puchases` index, if the `Products` field was defined just as 
> `Products = g.SelectMany(x=>x.Products)` we'll have a problem. Calling `reduce` will not actually reduce the amount of data
> that we are working and as time goes by the `reduce` function will have to operate on larger and larger values.
>
> This is no an efficent way to do things, and since we need to keep track of intermediate values, it will lead to a marked 
> increased in memory utilization, disk space usage and overall be quite expensive. 

Now, let's see what this index actually generates. Execute the following query: 
`from index 'Companies/Purchases' where Company ='companies/13-A'` and observe its results. You can also see that in 
Figure 11.12.

![Complex output form the index, showing aggregation inside the reduced result](./Ch11/img12.png)

This kind of index can be used to prepare complex computations ahead of time, leaning on RavenDB's ability to compute this 
ahead of time. RavenDB will then be in charge of keeping such details up to date, and your client code can simply query the 
output of the index. 

### Querying map-reduce indexes

When talking about map-reduce indexes we typically focus on the map-reduce portion of them, the aggregation and computation that
is being done. This, however, is only part of the story. While a lot of stuff is going on in the map-reduce portion of the index,
in the end, the data is still being written to the index in exactly the same way as we have seen in the previous chapter. 

This means that you can execute any of the usual operations that you would normally run. As a good example, let's look at 
Figure 11.12, the output of the query is pretty bare bone, what _is_ `products/21-A`, for example. Listing 11.8 shows a nice 
way to get more information, at very little cost.

```{caption="Using functions to enrich the results from a map-reduce index" .sql}
declare function addProductName(result){
    for(var i = 0; i < result.Products.length; i++){
        var p = load(result.Products[i].Product);
        result.Products[i].ProductName = p.Name;
    }
    return result;
}
from index 'Companies/Purchases' as result
where result.Company ='companies/13-A'
select addProductName(result)
```

The result of the query in Listing 11.8 is shown in Listing 11.9.

```{caption="Output of the query in Listing 11.8, showing enriched result set of map-reduce index" .js}
{
    "Company": "companies/13-A",
    "Products": [
        {
            "Product": "products/21-A",
            "Quantity": 1010,
            "ProductName": "Sir Rodney's Scones"
        },
        {
            "Product": "products/37-A",
            "Quantity": 101,
            "ProductName": "Gravad lax"
        }
    ],
    "Total": 1111
}
```

The query in Listing 11.8 should serve as a nice example for the kind of things we can do, but it just the first taste. In 
addition to using projections in this manner, you can also define map-reduce fields to support Full Text Search, apply 
suggestions and even More Like This. 

You can even query using facets on map-reduce indexes, an example of which you can see in Listing 11.10.

```{caption="Facets also apply to map-reduce indexes" .sql}
from index 'Companies/Purchases'
select facet(
    Total < 5000, 
    Total between 5_000 and 9_999,
    Total between 10_000 and 24_999,
    Total between 25_000 and 49_999,
    Total between 50_000 and 74_999,
    Total > 75_000,
    avg(Total)
    )
```

Applying facets, which can do their own aggregation, to map-reduce is an interesting experience. It allow us to do some 
interesting things, such as building a map-reduce operation to aggregate some of the data in a static fashion, then allow
faceted queries to slice and dice it further. If you'll look at the results of the query in Listing 11.10 you'll note that
the faceted results also include an average value for each facet of the results, and we have support for the usual (`min`,
`max`, `sum`, `avg`) aggregation methods. 

In fact, that is not the only way to aggregate the data in multiple ways, there is also recursive map-reduce operations...

#### Recursive map-reduce

#### Multi map reduce

### Disadvantages of Map-Reduce indexes in RavenDB