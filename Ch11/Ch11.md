
## MapReduce and Aggregations in RavenDB

[Map Reduce]:(#map-reduce)

MapReduce is an old term. It came from Lisp and was used as early as the 1960s. For a long time, it was primarily known only by
functional language aficionados, and it was rarely seen outside their circles. In 2004, the Google paper
["MapReduce: Simplified Data Processing on Large Clusters"](https://research.google.com/archive/mapreduce-osdi04.pdf) was released, 
and MapReduce was instantly a big hit in the distributed programming circles. Everyone had to have a distributed MapReduceÂ­
implementation.

RavenDB is a distributed database, and it uses MapReduce. However, it doesn't do so in the context of distributed computing. 
Instead, RavenDB uses MapReduce for aggregation of data on each node independently. If you're used to MapReduce jobs that run on large clusters processing terrabytes of data, this might look very strange. 
But RavenDB isn't using MapReduce to break apart large computations across different machines. Instead, it uses MapReduce to 
break apart computations across _time_.

It's usually easier to explain with an example, so let's jump into that.

### Executing simple aggregations

Listing 11.1 shows a simple aggregation query, giving us the total number of orders and items purchased by a particular company.

```{caption="A simple aggregation query in RavenDB" .sql}
from Orders as o
group by o.company
where o.company  = 'companies/1-A'
select count() as NumberOfOrders, 
	   sum(o.Lines[].Quantity) as ItemsPurchased, 
	   o.company
```

The query in Listing 11.1 should be familiar to anyone who's used SQL before. Now, let's analyze what RavenDB must do in
order to answer this kind of query:

1. Find all the `Orders` documents where the `company` field is set to `'companies/1-A'`.
2. Iterate over all of those, count their number and sum the number of line items in each.
3. Return the results to the client.

This seems quite straightforward, but it has a couple of issues&mdash;in particular, the first two steps. The sample data we're using
has just over a thousand documents in the database. There isn't much we can do to make any query expensive over that dataset.
However, with real-world datasets, we'll typically deal with collections that contain hundreds of thousands to many millions 
of documents. 

Consider the first step, which is finding all the `Orders` documents with the `company` field set to a value. If I have a few million 
documents to scan, that alone can be quite expensive, both in I/O (to read the data from disk) and computation (to check equality
so often). This is obviously something that we can optimize with an index. It's the next step that's hard to work with.

If a company has a _lot_ of orders, then the process of iterating over each of these orders can be extraordinarily expensive. 
Consider the case where you want to show the number of orders and the number of items purchased on the company page. This is a 
small amount of information that can tell you how important this particular customer is. 

However, if we need to repeat the second step each time we run the query, that can get expensive very quickly. What's 
worse is that the more important the company, the more orders and items this particular customer purchased from us and the slower 
things will become. This method won't work. It's punishing success, and that isn't something we want.

Typically, you won't run such queries directly because of their cost and time to run.
Instead, you'll write a job that will run these queries at idle times and cache the results. Then you only have to deal with cache invalidation, making sure that this job is never run under load, explaining to users that the results are delayed and...

It's complex. All you wanted was to put a couple numbers on a page, and suddenly you need to deploy a background job, monitor
its cost and execution and wait until the daily or weekly run to get updated numbers. Such a high cost of aggregation will usually
cause such features to be dropped.

And even if you don't have enough data to require such measures, aggregation queries are still typically costly for the 
database&mdash;enough so that they're used sparingly in most systems.

RavenDB's aggregation doesn't work like this. 

#### The gory details of aggregation in RavenDB

Instead of gathering all the data and then aggregating it all in place, RavenDB uses MapReduce to break apart the aggregation 
computation into discrete steps: the "map" and the "reduce." 

Let's look at how RavenDB _actually_ processes the query in Listing 11.1. Run the query in the listing and then click on the index button in the results, as shown in Figure 11.1.

We're going to go deep into how RavenDB is handling aggregations. You'll not typically need to know this level of detail, 
and you can feel free to just skip this section if you don't think you don't need such knowledge. I'm including it here because it's important to understand the implications of
how things work and that aggregation queries in RavenDB are _very_ cheap.

![Getting to the index details on the query page](./Ch11/img01.png)

You can already see that an aggregation query in RavenDB is also using an index. In addition to the usual options, such as the 
index terms and performance statistics, there's also the MapReduce visualizer, which we'll look at in more detail later. For now,
click on the `View Index` option, which should open the index details page shown in Figure 11.2.

![A Map-Reduce index aggregation `Orders` by `company`](./Ch11/img02.png)

Figure 11.2 shows the structure of the index. Operating over `Orders`, it groups by the `company` and aggregates over them. One thing to note here is that there's no mention anywhere of `companies/1-A`. Even though the query in
Listing 11.1 mentioned it, the index isn't operating on that particular value but rather on the generic concept of aggregating by
the `company` field.

In other words, as usual, RavenDB looked at the query and generalized the operation to answer any question using `company`. But what about the aggregation?

Aggregation is actually handled via two separate actions, `map` and `reduce`. The first stage runs a `map` operation on
each of the documents, grabbing just `company`, `sum(Lines[].Quantity)` and `Count = 1` from each of the `Orders` documents. The 
second stage is to group them all by `company` and run the `reduce` operation to get the final result by each of the 
`company` field values.

If this doesn't make sense to you, don't worry, RavenDB contains a few tools specifically to help you understand how the 
`map-reduce` process work.  

In the `Indexes` tab, go to `Map-Reduce Visualizer` and then select the `Auto/Orders/ByCountAndLines[].QuantityReducedBycompany`
index from the drop down. Then search for the following document ids: `orders/396-A` and `orders/445-A`. The result should look
similar to Figure 11.3.

![The Map-Reduce Visualizer allows us to inspect the internal structure of the index](./Ch11/img03.png)

Clicking on the `{"company":"companies/1-A"}` rectangle in Figure 11.3 will give us more details about that particular value, as
you can see in Figure 11.4.

![A single reduce result and the total reduced value for a map-reduce index](./Ch11/img04.png)

With the details in Figures 11.3 and 11.4, we can now see exactly what we mean when we talk about `map` output and the resulting
aggregation. The documents we selected (`orders/396-A` and `orders/445-A`) both belong to `companies/1-A`, and we can see that 
for `orders/396-A` the `map` output was `{"company": "companies/1-A","Count": 1,"Lines[].Quantity": 38}`. Indeed, if we'll go 
and inspect the document, we'll see three line itmes, with quantities of 15, 21 and 2, totalling 38. For `orders/445-A` we can
see that the total quantity is 20, with a single line item. 

This is interesting, but what is even more interesting is the aggregated value. For `companies/1-A`, you can see the aggrated 
values of a total of 6 orders for this company with a final quantity of 174 items ordered. Clicking on the aggregation summary
will take us even further down, into the inidivual page entries, as you can see in Figure 11.5.

![Individual mapped entries for `companies/1-A`](./Ch11/img05.png)

In Figure 11.5 you can see all the details for each of the entries for `companies/1-A`. This is the lowest level we need to 
inspect as well as the reason why RavenDB implements aggregation in such a manner. I already spent several pages just explaining
what is going on, why is the aggregation implementation for RavenDB so _complex_?

The reason is quite simple, actually. The reason for the complexity is that we don't run aggregation queries once, instead, we
compute the aggregation result once, and then we store it. When we issued the query in Listing 11.1 we queried only for results
for `companies/1-A`, but the index that the query optimizer generated for us applies to all companies.

In fact, if we would now run the same query, but for `companies/2-A`, we'll be able to reuse the same index, and in fact, we'll 
have to do very little work. The query will use the index and fetch the already pre-computed results for `companies/2-A` and
won't have to actually perform any aggregation whatsoever. All the work has already been done. 

As great as that is, you might be asking yourself why this level of complexity. After all, surely we could have done the same
without so many moving parts, right? This is correct, but there is one additional item that we need to consider. How are we 
going to handle updates?

The map-reduce indexes in RavenDB aren't simply a cache of the already computed results, instead, we store the data in such a 
way that make it cheap to also update the results. Consider what will happen inside RavenDB when a new order comes in. We'll run
the `map` portion of the index, getting the `company`, `sum(Lines[].Quantity)` and `Count = 1` from the newly created document.

The easiest way to visualize that is to just add another row to Figure 11.5. At this point, RavenDB can then just aggregate the
new results alongside the already existing result and get to the final tally. In other words, the complexity here exists in order
to allow RavenDB to efficently update map-reduce results when documents are created or updated.

This works great when we have a small number of items to aggregate, such in the case with `companies/1-A`, but what happens when
the number of items grow? Let's increase the number of documents that we are aggregating by a hundred fold and see where that 
takes us. Go do `Documents` and then `Patch` and run the update script in Listing 11.2.

```{caption="Increase the number of Orders documents by a hundred" .sql}
from Orders 
update {
    for(var i = 0; i < 100; i++ ){
        put("orders/", this);
    }
}
```

After running this script, we should have about 83,000 Orders documents in the database. Where previously we had 6 entries for
`companies/1-A`, we now have 600. Let's look at how that work. Go back to the Map-Reduce Visualizer and select the
`Auto/Orders/ByCountAndLines[].QuantityReducedBycompany` index and then add the `orders/396-A` and `orders/445-A` documents.
The result is shown in Figure 11.6

![Showing 600 mapped entries for `companies/1-A`](./Ch11/img06.png)

This is very similar to how it looked before, and indeed, the structure of the data is exactly the same. This is because the
amount of entires for `companies/1-A` is still quite small. Let's select another pair of documents, this time belonging to
`companies/77-A` and see what kind of structure we have there. Add `orders/77-A` and `orders/146-A` to the visualizer and 
see what the result looks like. You can see the results in Figure 11.7.

![Map-reduce works as a tree once we have enough entries for a particular value](./Ch11/img07.png)

On the left in Figure 11.7 you can see that unlike for `companies/1-A` in Figure 11.6, were we had a flat list, Figure 11.7
shows a tree structure. Indeed, once we are past a certain size, RavenDB will start processing map-reduce entries in a tree
like fashion.

Consider an update to `orders/77-A` and how RavenDB will apply it. First, we'll run the `map` on the updated document, giving
us the map entry to write to `#1028`. Then, we'll run the reduce on that page, giving us the final tally for this page. We'll
then recurse upward, toward `#1437`, where we'll also run the reduce.

> **Using aggregation in RavenDB**
>
> Aggregation operations in RavenDB are cheap, both to compute and to query. This is in stark contrast to the usual behavior of 
> aggregation queries in other databases. The more data you have to go through, the more efficently will RavenDB be able to 
> actually process and aggregate it. 
>
> While typically you'll run aggregation as a daily or weekly report (usually during off hours) and store these results for use 
> later, RavenDB allow you to just query the aggregated data and RavenDB will provide the answers you need as well as keep 
> everything up to date. 

The end result is that an update operation will take us a single `map` invocation and two `reduce` calls to update the final
result for `companies/77-A`. In other words, if we have a lot of data we have for a particular key, RavenDB will start segmenting
this data and apply aggregation operations in such a way to reduce the number of operations required to a minimum. This ensures
that not only are we able to answer queired efficently, but we are also able to update the map-reduce results with very little 
costs.

### Defining your own map/reduce indexes

In Listing 11.1 we have seen how we can query using `group by` and RavenDB will generate a map-reduce index for us behind the 
scenes. This works quite nicely, but there is only so much that you can do with a dynamic query before the query optimizer will
give up.

The query optimizer can do quite a lot, but in order to make sure that it is predictable, it is currently limited to recognizing 
and being able to generate map-reduce indexes from a fairly small list of predictable patterns. For more complex things, you'll 
need to create your own map-reduce index. Luckily, this is quite easy to do.

Let's say that we want to get the total number of sales and the total amount we made on each product. We cannot express this as 
a simple query, so we'll need to create an map-reduce index of our own for that. Go to the `Indexes` page and click on 
`New Index`. Name the index `Products/Sales` then click on `Add Reduction` button. Listing 11.3 has the contents of the `Map` 
and `Reduce` fields. After you have done filling these in, click on `Save` to create the new index.

```{caption="Compute total number of sales and revenue per product" .cs}
// map 
from o in docs.Orders
from l in o.Lines
select new
{
    l.Product,
    l.Quantity,
    Total = (l.Quantity * l.PricePerUnit) * (1 - l.Discount)
}


// reduce
from r in results
group r by r.Product into g
select new
{
    Product = g.Key,
    Quantity = g.Sum(x=>x.Quantity),
    Total = g.Sum(x=>x.Total)
}
```

Before we get to investigating how this index work, let's talk about what it does. The map portion runs over all the orders and 
all the line items for each order. Then, for each of the line items, we output an entry with the quantity sold and the amount
of money we made on this product. The reduce will group all these results based on the `Product` field and then sum up all the 
final numbers for the `Total` and `Quantity` sold. 

Now we can really see the separate operations of `map` and `reduce`, it might make more sense how they are truly two separate
operations. It is also important to understand that we aren't actually running the `map` or the `reduce` on the full results 
all the time. Instead, we break it apart internally and apply them to portions of the data each time. This leads to several 
important restrictions on map-reduce indexes:

* Both the `map` and `reduce` functions must be _pure_ functions. In other words, they should have no external input, and 
  calling them with the same input must always return the same output. In particular, usage of `Random` or `DateTime.Now` and
  similar calls is not allowed. 
* The output of the `map` is fed into the `reduce`, this is quite obvious, but what may not be obvious is that the output of 
  the `reduce` is _also_ fed into the `reduce`, recursively. In particular, you should make no assumptions in the `reduce` about
  the number of calls or the amount of data that you have to process in each invocation to `reduce`. 
* The output of the `reduce` must match the output of the `map` (because both are being fed back into the `reduce`, they must
  have the same structure). RavenDB will error if you have a different shape for each of the functions.

Because of these restrictions, RavenDB can apply the `map` and `reduce` functions in an incremental fashion and generate the 
results that we have already seen. This is the key for RavenDB's ability to compute aggregation cheaply over time.

We can now run the query in Listing 11.4 to find out the top grossing products:

```{caption="Finding the top grossing products across all orders" .sql}
from index 'Products/Sales'
order by Total as double desc
```

You'll note that the results of this query are effectively instantaneous, even though we have tens of thousands of records, 
RavenDB only need to look through 77 precomputed results. We can also sort by `Quantity` to find the most popular products. 

Go in to the `Map-Reduce Visualizer` and select the `Products/Sales` index and then enter `orders/6-A` and `orders/10-A` to
see the internal structure of the map-reduce index, as shown in Figure 11.8.

![Internal sturcutre of the `Products/Sales` index in the map-reduce visualizer](./Ch11/img08.png)

As you can see in Figure 11.8, all of the entries we have here are big enough to require a tree structure. If we'll look at
`products/39-A` you can see that this is holding just over 80,000 entries, with Figure 11.9 zooming into a single page inside
that tree.

![A single page holding over 25,000 map entries, due to high compression rate](./Ch11/img09.png)

Page `#578` holds over 25,000 entries for this product. How can a single page hold so many? The answer is that RavenDB will 
apply compression to the mapped entries. Since they are mostly very similar, they have a very high compression rate, allowing
us to pack a _lot_ of entries in a very small amount of space. This also adds to performance of the updating entries in the tree,
since we don't have to do as much work and the depth of the tree is much smaller.

This is about as low level as we'll get when discussing the map-reduce implementation. You know have sufficent information to 
have a good feel about the relative costs of using aggregation in general and map-reduce in particular in RavenDB. With that 
knowledge under your belt, let's explore some of the more interesting things that we can do with map-reduce indexes in RavenDB.

#### Common pitfalls with map-reduce indexes

Certain operations are not easy to perform using the limits on the `map` and `reduce` functions that RavenDB has. Probably the 
most obvious example here is the problem with calculating an average. Let's edit the `Products/Sales` index to add an average.
Listing 11.5 shows the updated definition with the most obvious (and wrong) way to do so:

```{caption="Computing average (wrongly) in map-reduce index" .cs}
// map 
from o in docs.Orders
from l in o.Lines
select new
{
    l.Product,
    l.Quantity,
    Count = 1,
    Total = (l.Quantity * l.PricePerUnit) * (1 - l.Discount),
    Average = 0, // so we'll have the same shape as the reduce
    Debug = new object[0] // same shape as reduce
}


// reduce
from r in results
group r by r.Product into g
select new
{
    Product = g.Key,
    Quantity = g.Sum(x=>x.Quantity),
    Count = g.Sum(x=>x.Count),
    Total = g.Sum(x=>x.Total),
    Average = g.Average(x=>x.Total),
    Debug = g.Select(x=>x.Total).ToArray()
}
```

Save the updated index and then run the following query: `from index 'Products/Sales' where Product = 'products/1-A'`, 
the result is showing in Figure 11.10.

![The wrong results shown for an average computation](./Ch11/img10.png)

The results shown in Figure 11.10 are wrong. The average price for a box of Chai (which is what `products/1-A` is) is not 
$184,514.014. So why are we getting this result? The answer is simple. The `Average` call is pretty simple, it simply sum
the elements that it got and then divide by the number of elements. But remember that `map` and `reduce` calls are not called
on each individual values, instead, they are called in batches. This means that by the the time the final call to `reduce` 
happened, it didn't get a flat list of all the results, but an aggregated list. Figure 11.11 shows the `Debug` field from
the index, which can allow us to better understand what is going on.

![The debug field from the `Products/Sales` index demonstrating the recursive nature of `reduce` operations](./Ch11/img11.png)

As you can see, the computation of the `Average` is correct, for the input that it got. Because we execute the `reduce` multiple
times, by the time the final round came, we fed it seven results, each of them aggregating about 500 documents.
In other words, when we write a map-reduce index, we need to take into account the fact that we never have access to the total
result set. This make sense, when you think about it, you don't want to have a single function invocation that has to go through
many tens of thousands of documents (or much more, on realistic data sets).

> **Do not assume that the order is fixed either**
>
> If your map-reduce index does work that rely on the order of operations or the order of items passed to the `map` or 
> `reduce` function, it is going to fail in a similar manner. This is common when you have calls to `First` or uses
> arrays in the map-reduce index. If you care about the order of operations, you'll need to ensure that as part of
> the map-reduce index and remember that the calls are made in a recursive manner. 

So how _can_ we compute an average? The answer is that we can do this quite simply. You can see it in Listing 11.6.

```{caption="Properly computing average in a map-reduce index" .cs}
// reduce
from r in results
group r by r.Product into g
let qty = g.Sum(x=>x.Quantity)
let total = g.Sum(x=>x.Total)
select new
{
    Product = g.Key,
    Quantity = qty,
    Count = g.Sum(x=>x.Count),
    Total = total,
    Average = total / qty
}
```

All we need to do for the proper computation is to make sure that we aren't assuming that the total dataset is passed to the
`reduce` call. In this case, we already have all the data that we need and we can just compute the final average. Note that we'll
still need to have a field called `Average` in the `map` function, because the shape must match. But because we are never reading
this field, we don't have to give it any special value and can just initialize it to any value we want.

Similarly to `Average`, we cannot use `Count` either in the `reduce`, for the exact same reason. That is why we use `Count = 1` 
in the `map`, to allow us the start the recursive computation of the total amount of items.

#### Complex map-reduce indexes

So far the map-reduce indexes we build were pretty simple, mostly doing trivial aggregation on numbers. To be fair, that is 
usually what you'll do with map-reduce indexes, but you can also do more with them. 
A good example would be to compute the total sales per company. This sounds like just taking the `Products/Sales` index and 
changing what we are grouping on, so let's make this more interesting. What we want to do is to get, per company, the total 
sales per product. 

Listing 11.7 shows the `Companies/Purchases` index, which does just that.

```{caption="Computing the total products sold for each company" .cs}
//map
from o in docs.Orders
from l in o.Lines
select new
{
    o.company,
    Products = new []{ new { l.Product, l.Quantity } },
    Total = l.Quantity
}

//reduce
from r in results
group r by r.company into g
select new
{
    company = g.Key,
    Products = g.SelectMany(x=>x.Products)
        .GroupBy(x=>x.Product)
        .Select(p => new
        {
            Product = p.Key,
            Quantity = p.Sum(x=>x.Quantity)
        }),
    Total = g.Sum(x=>x.Total)
}
```

Taking a look at Listing 11.7, we can see that the `map` function looks pretty normal, but there is considerable amount of 
work that is going on in the `reduce`. Let's break it apart into its component parts and examine each independently.

First, the `map` generate an entry per line item, and the entry contains a single element `Products` array with the product's 
id and the amount sold. The reason we create the array in the map is that we need to match the shape of the `reduce`. In the
`reduce` function, we group the results by the `company` and then generate the results for that particular company. The most
interesting tidbit happens when building the `Products` field. 

There, we apply _another_ `GroupBy` call to aggregate the data inside the `company` once again. We use this to get the total
numbers of items purchased for each product, and this mechanism allow us to aggregate the data across mutliple invocations to
`reduce` safely and in a very natural manner. 

> **Reduce should... _reduce_ the size of the data**
>
> A somewhat overlooked requirement for the `reduce` function is that the amount of data going out of the `reduce` function
> should be smaller than the amount of data going in. All the examples we have seen so far had this properly, mostly because
> they aggregated information
>
> Let's look at an example where the amount of information that passed through `reduce` does not go down and discuss its 
> implications. In the `Companies/Puchases` index, if the `Products` field was defined just as 
> `Products = g.SelectMany(x=>x.Products)` we'll have a problem. Calling `reduce` will not actually reduce the amount of data
> that we are working and as time goes by the `reduce` function will have to operate on larger and larger values.
>
> This is no an efficent way to do things, and since we need to keep track of intermediate values, it will lead to a marked 
> increased in memory utilization, disk space usage and overall be quite expensive. 

Now, let's see what this index actually generates. Execute the following query: 
`from index 'Companies/Purchases' where company ='companies/13-A'` and observe its results. You can also see that in 
Figure 11.12.

![Complex output form the index, showing aggregation inside the reduced result](./Ch11/img12.png)

This kind of index can be used to prepare complex computations ahead of time, leaning on RavenDB's ability to compute this 
ahead of time. RavenDB will then be in charge of keeping such details up to date, and your client code can simply query the 
output of the index. 

### Querying map-reduce indexes

When talking about map-reduce indexes we typically focus on the map-reduce portion of them, the aggregation and computation that
is being done. This, however, is only part of the story. While a lot of stuff is going on in the map-reduce portion of the index,
in the end, the data is still being written to the index in exactly the same way as we have seen in the previous chapter. 

This means that you can execute any of the usual operations that you would normally run. As a good example, let's look at 
Figure 11.12, the output of the query is pretty bare bone, what _is_ `products/21-A`, for example. Listing 11.8 shows a nice 
way to get more information, at very little cost.

```{caption="Using functions to enrich the results from a map-reduce index" .sql}
declare function addProductName(result){
    for(var i = 0; i < result.Products.length; i++){
        var p = load(result.Products[i].Product);
        result.Products[i].ProductName = p.Name;
    }
    return result;
}
from index 'Companies/Purchases' as result
where result.company ='companies/13-A'
select addProductName(result)
```

The result of the query in Listing 11.8 is shown in Listing 11.9.

```{caption="Output of the query in Listing 11.8, showing enriched result set of map-reduce index" .js}
{
    "company": "companies/13-A",
    "Products": [
        {
            "Product": "products/21-A",
            "Quantity": 1010,
            "ProductName": "Sir Rodney's Scones"
        },
        {
            "Product": "products/37-A",
            "Quantity": 101,
            "ProductName": "Gravad lax"
        }
    ],
    "Total": 1111
}
```

The query in Listing 11.8 should serve as a nice example for the kind of things we can do, but it just the first taste. In 
addition to using projections in this manner, you can also define map-reduce fields to support Full Text Search, apply 
suggestions and even More Like This. 

You can even query using facets on map-reduce indexes, an example of which you can see in Listing 11.10.

```{caption="Facets also apply to map-reduce indexes" .sql}
from index 'Companies/Purchases'
select facet(
    Total < 5000, 
    Total between 5_000 and 9_999,
    Total between 10_000 and 24_999,
    Total between 25_000 and 49_999,
    Total between 50_000 and 74_999,
    Total > 75_000,
    avg(Total)
    )
```

Applying facets, which can do their own aggregation, to map-reduce is an interesting experience. It allow us to do some 
interesting things, such as building a map-reduce operation to aggregate some of the data in a static fashion, then allow
faceted queries to slice and dice it further. If you'll look at the results of the query in Listing 11.10 you'll note that
the faceted results also include an average value for each facet of the results, and we have support for the usual (`min`,
`max`, `sum`, `avg`) aggregation methods. 

In fact, that is not the only way to aggregate the data in multiple ways, there is also recursive map-reduce operations...

### Recursive map-reduce

In our Northwind example, we have orders representing sales to companies. This is the typical ECommernce model and should be
pretty familiar to you. A common requirement in such a system is reporting, in particular, pulling sales data on daily, monthly
and yearly basis. 

Listing 11.11 shows the `Products/DailySales` index, which gives us the details of sales by product by day. 

```{caption="Computing total daily sales by product for all orders" .cs}
//map 
from o in docs.Orders
from l in o.Lines
select new 
{
    o.OrderedAt.Date,
    l.Product,
    Count = l.Quantity
}

//reduce
from r in results
group r by new { r.Date, r.Product } into g
select new
{
    g.Key.Date,
    g.Key.Product,
    Count = g.Sum(x=>x.Count)
}
```

The output of the `Products/DailySales` index can be seen in Figure 11.13. The only new thing we have in Listing 11.11 is the
`group by` on multiple fields, the `Date` and `Product`.

![Showing the daily sales for each product on the May 6th 1998](./Ch11/img13.png)

Next, we need to compute the same values per month, and then per year. We can define the same index again and run the exact
same computation, but this time group by the product, year and month and then just by the product and year. But we currently
have over 83,000 orders in the database, and we'll likely have more. Doing the same operation on all these values again and
again seems... inefficient. 

RavenDB supports the notion of outputing a map-reduce output to the index and to a dedicated collection. Set the 
`Output reduce to collection` to `DailyProductSales`. Figure 11.14 shows how this can be done. 

![Configuring map-reduce index to write the output of the index to a dedicated collection](./Ch11/img14.png)

Make the modification to the `Products/DailySales` index and save it, then head to the `Documents` tab and look at the 
collections in the database. You can see the results in Figure 11.15.

![Artifical documents created as a result of a map-reduce's output collection](./Ch11/img15.png)

In many respect, these artifical documents will behave just like standard documents. You can load them,
query them and even save modifications to them. Note that modifying an artifical document by hand 
is _not_ recommended, the next update  to the index will overwrite any changes you make to the document,
after all). Artificial documents are _not_ replciated. Since they are being created by the index directly,
they will be created by the index in the remote node as well, so there is no point in sending them 
over the wire. 

Artifical documents are updated whenever the index completes a batch of documents, so there is very little
lag time between the index picking up changes and the artifical documents update. If this was the only
thing that artifical documents were good for, that wouldn't be of much use. After all, we already have
the results as the output of the map-reduce index, so why do we need artifical documents?

The primary reason that artifical documents exists is that you can setup indexes on top of them. And that
includes _additional map-reduce indexes_. Take a look at Listing 11.12, showing just such an example of
the `Products/MonthlySales` index, computing the monthly totals from the daily totals.


```{caption="Recursive map-reduce index using artifical documents" .cs}
//map 
from ds in docs.DailyProductSales
select new 
{
    ds.Product,
    Year =  ds.Date.Year,
    Month = ds.Date.Month,
    ds.Count
}

//reduce
from result in results
group result by new 
{ 
    result.Product, 
    result.Year, 
    result.Month 
}
into g
select new
{
    g.Key.Year,
    g.Key.Month,
    g.Key.Product,
    Count = g.Sum(x=>x.Count)
    
}
```

You can see that the `map` of the `Products/MonthlySales` index is using the `DailyProductSales` 
collection as it source, and the `reduce` aggregates the data by product on a montly basis. Let's examine
what is actually going on here in a little more depth.

Whenever an order is modified or created, the `Products/DailySales` index will run, computing the updated 
daily totals for the products in the new order. As a result of this index running, artificial documents will
be created (with ids such as `DailyProductSales/3183876884104958208`) with the result of the map-reduce index.

Because a `DailyProductSales` document was created, the `Products/MonthlySales` index will run on the changed daily
tallies to update its own numbers. We can even set things up so we'll have an output collection for the 
`MonthlyProductsSales` as well, and then define a `Products/YearlySales`. The recursive nature of the indexing naturally
extends in this manner. 

Artifical documents do have a few limitations that you should be aware of:

* RavenDB will detect and generate an error if you have a cycle of artifical documents. In other words, you can't
  define another index that will output artifical documents if that will trigger (directly or indirectly) the 
  same index. Otherwise, you might setup a loop where the indexes run in an infinite loop. 
* You must choose an empty collection, RavenDB will not allow you to output artifical documents into a pre-existing
  collection. This is done because RavenDB will overwrite any document in the collection, so it prevent the option
  of overwriting existing documents. 
* The document identifiers for the artificial documents are generated by RavenDB (based on the hash of the reduce 
  key) and you don't have any control over them.
* Artifical documents are not sent over replication and cannot use revisions. 

> **Artifical documents and subscriptions**
>
> You can use subscriptions and artifical documents together (in fact, along with recursive map-reduce, that is one
> of the primary reasons that they exists), but you need to be aware of a small wrinkle in this setup. Because
> artifical documents aren't sent over via replication, each node in the database group is going to have its own
> (independent) copy of the results. The _contents_ are the same, but the subscription has no way of knowing this.
>
> Because of this issue, it is recommended to use artifical documents with subscriptions only on a single node, because
> failover of the subscription to another node may cause the subscription to send artifical documents that the 
> subscription has already ackonwledged. You can configure this by _disabling_ the dynamic task distribution as part of
> the subscription configuration.

Artifical documents and recursive map-reduce are a good match, but before you turn your RavenDB instances into reporting 
powerhouse, there is another possible topolgoy to consider. Instead of defining these indexes and processes on the 
main database, setup external replication to a dedicated database (on the same cluster or on a separate one) and run
all that work there. 

This can simplify distribution of work as the application grow. If you have many such indexes and a high rate of 
changes, being able to isolate the work to particular database group (and thus, specific nodes) can be very helpful. 

### Multimap-reduce indexes

In the previous chapter, Figure 10.12 demonstrated the usage of multimap indexes to index several collections and 
union the results into a single index. This can be very nice when you want to search over several things at the same
time. The example we looked at in Chpater 10 was searching a person by name, where the person can be an `Emloyee`, a 
`Contact` on a `company` or a `Contant` for a `Supplier`. Regardless, we were able to search for that person easily
and in a convient manner.

Multimap-reduce indexes allow us to extend that behavior naturally to also include aggregation across multiple sources
in a very natural manner. Let's first look at Listing 11.13, showing off this feature, and then explore what this means.
Go to the `Indexes` page and create a new index, click on `Add map` twice and create the three maps then click on 
`Add Reduction` and add the reduce. Name the new index `Cities/Details` and click on `Save`.

```{caption="Multimap-reduce index that sums the points of interest in each city" .cs}
// map #1
from c in docs.Companies
select new 
{
    c.Address.City,
    Companies = 1,
    Suppliers = 0,
    Employees = 0
}


// map #2
from s in docs.Suppliers
select new 
{
    s.Address.City,
    Companies = 0,
    Suppliers = 1,
    Employees = 0
}

// map 3
from e in docs.Employees
select new{
    e.Address.City,
    Companies = 0,
    Suppliers = 0,
    Employees = 1
}

//reduce
from result in results
group result by result.City 
into g
select new 
{
    City = g.Key,
    Companies = g.Sum(x=>x.Companies),
    Suppliers = g.Sum(x=>x.Suppliers),
    Employees = g.Sum(x=>x.Employees),
}
```

Take a few minutes to look at Listing 11.13, there is a lot of stuff going on there. We define three maps, on the 
`Companies`, `Suppliers` and `Employees` and for each we output a count for the type of the document we are mapping 
as well as the relevant `City`. Finally, on the reduce, we simple group by the `City` and then sum up all the results 
from all the intermediate steps to get the final tally. Listing 11.14 shows the output from this index for London.

```{caption="Output of the `Cities/Details` index for London"}
{
    "City": "London",
    "Companies": 6,
    "Suppliers": 1,
    "Employees": 4
}
```

The `Cities/Details` index is interesting, in the sense that it shows off the capabilites, but it isn't really that
exciting. It is also operates on quite a small dataset, even if it touches multiple collections. Let's modify the index
by also have it operate on orders. 

> **Map-reduce indexes are also just indexes**
>
> It bears repeating that a map-reduce index is also just an index. We focused heavily in this chapter on the map-reduce
> portion of such indexes, but it is also important to remember that they are also capable of doing everything that a 
> map only index can do. 
>
> In other words, if we had a `Cities` collection that also had the coordinates of each city, we
> would be able to modify the `Cities/Details` index to be a multimap-reduce index that also provides spatial
> queries.
> You can use facets on a map-reduce index, apply suggestions, run full text queries, etc. All too often I saw users
> simply assume that the map-reduce part of the index was were the functionality stopped, where in fact this is where
> it _began_.

We'll first add another entry for each of the existing maps `OrderTotal = 0`. And we'll add the same logic for the reduce
function: `OrderTotal = g.Sum(x=>x.OrderTotal)`. The first step is required because all the maps and reduce in an index
must have the same output. The second is require to actually sum up the information we'll shortly add. Now click on the 
`Add map` button on the index edit page and add the map shown in Listing 11.15.

```{caption="Adding total order per city to the `Cities/Details` index" .cs}
from o in docs.Orders
select new
{
    o.ShipTo.City,
    Companies = 0,
    Suppliers = 0,
    Employees = 0,
    OrderTotal =  o.Lines.Sum( 
        x => x.Quantity * x.PricePerUnit 
    )
}
```

We note have an index that can give us a lot of interesting details about each city. Listing 11.16 shows the updated 
output for London in the database. 

```{caption="London's information now include the total revenue for orders shipped to it"}
{
    "City": "London",
    "Companies": 6,
    "Suppliers": 1,
    "Employees": 4,
    "OrderTotal": 4107034.71
}
```

As you can imagine, this kind of behavior is powerful, because it allows you to pull data from disparate parts of your 
system and aggregate it with very little work. The fact that querying this information is also effectively free also
make it much easier to consume and work with as well. 

### Dynamic aggregation with map-reduce indexes

Map-reduce indexes are wonderful way to handle aggregation. RavenDB's ability to precompute and answer complex aggregation
cheaply can make a world of difference in your ability to deliver features in your applications. The fact that showing an
aggregated value doesn't require you to setup an off hours job, monitor it, clear caches, etc is a force multiplier for your
application's capabilities.

However, while map-reduce operation can seem magical at times, it is a tool that is fit for purpose, and trying to use it for
a different purpose than it is intended for will produce sub-optimal results. Map-reduce indexes are great when:

* The fields that you are aggregating by are known in advanced.
* The source data that you are aggregating is known in advanced. 

In other words, map-reduce is great for static aggregation. If you want to get the daily totals by product, map-reduce is the 
perfect solution. The data source (all orders) is known, what you are aggregating by (the date and the product) is known in
advanced. RavenDB is able to generate the appropraite map-reduce operation, compute the result and allow you to query them
easily and cheaply.

But what happens if we want to aggregate daily sales by product only for London? Well, that is easy, we can define another
map-reduce index that aggregate the results by (date, product and city). We get the same benefits, and everyone is happy, but
the next request is to get the daily sales by city based on the supplier, not the product. And the _next_ request after that
is to aggregate the sales by the employee on a monthly basis and the one after that is to see the yearly sales by product 
only for specific customers and then... .

I think you get the picture. Map-reduce indexes are great when the type of aggregation is known in advanced. In this case, 
RavenDB is able to prepare everything and and have an answer ready for you by the time you query. But if your queries are 
dynamic and fluid, changing what you are querying on and how you are doing the aggregation, this is much more complex.

One option would be to define a map-reduce index for each type of aggregation you need. This works, but you might end up with
a _lot_ of map-reduce indexes. That can be fine, RavenDB is _very_ efficent in handling of map-reduce indexes (they are actually
typically _cheaper_ to run than a map only index, actually), but a large number of indexes still means that we need to excecute
some amount of work times the number of indexes for each change. 

A better approach for dynamic aggregation is to use facets. To see the total sales for egg noddles and cheese in the first week
of July 1996, we can run the query in Listing 11.17.

```{caption="Using facets on top of a map-reduce index to achieve dynamic aggregation" .sql}
from index 'Products/DailySales'
where Date between '1996-07-01' and '1996-07-07' 
and   Product in ('products/11-A', 'products/42-A')
select facet(Product, sum(Count))
```

Let's break the query in Listing 11.17 apart, one clause at a time. First, we select the map-reduce index `Products/DailySales`,
which we already seen in Listing 11.11. The `where` clause specify the date range we want to query, and the particular products
that we are interested in. Note that the `Date` query is using `between` and rely on the fact that we do lexical comparisons to
get a clear sytnax for the date range. 

The `select` clause is using facets, but unlike the facets queries that we looked at in the previous chapter, we are now adding
a new wrinkle `sum(Count)` actually allows us to do an aggregation operation over the query results. In this way, we can control
dynamically what will be aggregated.

> **Cost analysis for dynamic aggregation (facets) vs. static aggregation (map-reduce)**
>
> An important distinction needs to be made about the cost structures of using aggregation in map-reduce vs. facets. For
> map-reduce, by the time that you are querying the information, it has already been aggregated. You are actually doing a 
> search on top of the already pre-computed results. 
>
> Aggregation with facets (which is sometimes also called dynamic aggregation) require us to run over the results of the 
> query and compute the final aggregated values. Note that the amount of results the faceted query needs to aggregated is
> just the _query_ results, not the total size of the source data.
> 
> This can be confusing, so we'll use the example of `Products/DailySales` index and the faceted query in Listing 11.17 
> to clear things up. We have the following values:
> 
> * `D` - the total number of orders (in this database, after running the patch operation in Listing 11.2) is 83,830 documents.
> * `R` - the unique reduce keys after the map-reduce aggregation. In the case of the `Products/DailySales` index, that is the 
>   unique (date, product) pairs that we are grouping by. This value is 2,106 unique reduce keys for the dataset we have.
> * `Q` - the number of results matched in the query. For the query in Listing 11.7 (sans facets) that number is 2. 
>
> With these values, we can now give proper estimates of the costs of making various queries. A typical cost of a map-reduce 
> query is `O(log R)`. So the cost for a query on `Products/DailySales` would be about 11 operations. 
> 
> A faceted query on all orders will have a cost of `O(D)`, with `D` equals to 83,830. This computation is done on each time
> the query is run. However, the only reason that the cost is over `D` is that we queried over all orders. Because the number
> of results for the query in LIting 11.7 is 2, the cost of actually aggregating them is effectively nil. 
> 
> It is a great way to handle such scenarios, use map-reduce to do the first level of the aggregation, and then use dynamic
> aggregation using facets to further slice and dice the data as you need.

The result of the faceted query is affected by the matches for the query, and that can change dynamically with very little 
cost, opening the way to more dynamic aggregation queries. Dynamic aggregation also has a pretty straightforward cost, since
it is linear to the amount of matches for the query. 

That is a great thing, since it simplify dynamic queries, but it also means that if you want to run a faceted query with 
aggregation on a very large result set, it is going to take time to process. A map-reduce on the same (or much larger) amount
of data will be much faster at querying time, but is limited in the amount of flexiblity it allows for each query.

Combining map-reduce and facets to handle this is a great way to reduce^[Pun very much intended here.] the amount of data the
facets needs to go through. It is easiest to consider this kind of approach as feeding the facets baby food, already pre-chewed. 
That cuts down dramatically in the amount of effort required to get the final result.

### Multi step aggregation processes

We have looked at all sorts of map-reduce indexes in this chapter, and also looked at how we can use dynamic aggregation to 
build dynamic aggregated queries. Now I want to apply all that knowledge into a more complex example, utilizing many of the 
features that we explored in this chapter. 

The requirement, we want to be able to get a monthly report of sales by supplier per city. It might be easier to understand
if we start from the point of view of the user interface. Take a look at Figure 11.16, which shows a mockup of the search
interface and the results we want to see. 

![Mockup of the user interface for the kind of data we want to get out](./Ch11/img16.png)

The simplest way to be able to answer this query is to use `LoadDocument` in the map phase of the index. You can see how 
this is done in Listing 11.18.

```{caption="Map-reduce index using `LoadDocument` can pull data from related documents" .cs}
//map
from o in docs.Orders
from l in o.Lines
let product = LoadDocument(l.Product, "Products")
select new
{
    o.ShipTo.City,
    o.ShippedAt.Month,
    o.ShippedAt.Year,
    Supplier =  product.Supplier,
    Total = l.PricePerUnit * l.Quantity
}
// reduce
from result in results
group result by new 
{ 
    result.City, 
    result.Supplier,
    result.Month,
    result.Year
} 
into g
select new 
{
    g.Key.City,
    g.Key.Supplier,
    g.Key.Month,
    g.Key.Year,
    Total = g.Sum(x=>x.Total)
}
```

The interesting bits in Listing 11.18 is the `LoadDocument` call in the map. Everything else is pretty much the same as we have
done throughout this chapter. We looked at `LoadDocument` in the previous chapter, and it serve much of the same role in 
map-reduce indexes as well. If the product document has changed, we'll have to re-index all the documents that referenced it. 
Because of this, the `LoadDocument` option is only available during the map phase of a map-reduce index, you cannot call 
`LoadDocument` from the reduce.

A sample query for this index can be seen in Listing 11.19.

```{caption="This query can be used to render the results in Figure 11.16." .sql}
from index 'Sales/ByCityAndSupplier' as t
where t.City= 'London' and t.Month = 2 and t.Year = 1997
order by t.Total as double desc
load t.Supplier as s
select s.Name, t.City, t.Supplier , t.Month, t.Year, t.Total
```

This works and generate the results you could see in Figure 11.16. However, there is a problem here. `LoadDocument` requires
that RavenDB will update the index if the referenced document (the product's document, in this case) is updated. This means that
an update to a product can force RavenDB to re-index all the orders that have this product. If this is a popular product, this
may require RavenDB to reindex large number of orders.

Ideally, we want to have `LoadDocument` where the number of referencing documents is bounded and small. Is there a better way
to handle this? We can change the index in Listing 11.18 so it will not include the `Supplier`, and instead will group things
only by the (product, city, month, year). We'll also define an output collection for the results.
The output collection is then used in another index where we'll use a `LoadDocument` to achieve the same output. 

Why all of this complexity? Aren't we in exactly the same position as we were before? An update to a product document will 
force us to reindex, after all. And the answer is yet, it will force us to re-index, but the question is _what_. In the case
of Listing 11.18, any change to a product will force re-indexing of all the `Orders` documents that referenced it. But in
the case where we had an intermediate artifical documents collection, we'll only need to re-index those that referenced the
modified product. 

Those documents have already gone through a map-reduce process, and there are likely to be far fewer of them than there are 
orders, so this is a net win in terms of the total amount of work that has to be done.

### Summary

This chapter covered a _lot_ of ground. I tried to strike the right balance between giving you enough information about what is
actually going on under the covers without drowning everything in implmentation details. 

The key takeaway I wish for you to take from this chapter is the notion that aggregation in RavenDB is cheap and plentiful, so
have at it. This is in direct contrast to the way things usually work with other databases. Aggregation is often expensive and 
hard, so it gets pushed to dedicated solutions (nightly runs, reporting databases, cached queries, etc). None of this is 
needed with RavenDB.

We started this chapter by writing some simple RQL queries using `group by`, and it just worked. The query optimizer recognized
the query, generated the appropriate map-reduce index and we were off to the races. We then took a deep, hard look at what was
actually going on there, analyzing what the index was doing and how map-reduce works inside RavenDB.

In order to better understand that, RavenDB has a few tools builtin to help you. Chief among them is the map-reduce visualizer,
which let you peek deep into the heart of how RavenDB execute map-reduce operations. We looked at the different behavior that
happens when we have a small amount of items to aggregate for a particular reduce key (all items are reduced in a single group)
and when we have a very large amount of items (reduce is handle in a recursive tree fashion). 

This means that updates to map-reduce indexes are very fast, because we usually need to do a minimal amount of work to get the
updated results.
After learning how RavenDB processes map-reduce indexes, we learned how we can define our own map-reduce indexes, with complete
freedom of how we want to structure the data and aggregate it. 
There are also some things to remember when you build your indexes, such as making sure that the `map` and `reduce` functions are
pure, that your `reduce` function can handle being called recursively and that the `reduce` outputs less data than we put in.

We then upped the ante and looked at more complex map-reduce indexes, grouping all the results by company and inside each company
grouping things again by product to get detailed summary of the results. These kind of indexes can allow you to do sophisticated
aggregation, rollups and computation during the indexing, then expose the whole thing, readily consumable, to your application.

Map-reduce indexes can do amazing things, but they are also indexes in RavenDB, which means that in additional to whatever you 
are doing in the map-reduce operation, all the usual stuff you can do with indexes is also there. We looked at how we can use
JavaScript projections to enrich the map-reduce output during query, use facets on the data (including additional aggregation)
and there are many more options (full text search, spatial, etc).

If a single map-reduce index isn't enough, you can always try things recursively. Indeed, RavenDB allows map-reduce indexes to
output artifical documents back to the database, and these documents are a key part of allowing RavenDB to handle recursive
map-reduce indexes. We looked at how we can do a daily and monthly rollups of sales per products by creating a map-reduce index
that is fed off the artifical documents collection that is created by another map-reduce index. This opens us the option to do
some really cool things, because you can pipeline the work and take advantage of work that has already been done.

Another cool option we looked at was the ability to use multimap-reduce indexes. Instead of just aggregating data from a single
collection, we can aggregate the data from several of them at once. That can allow you to paint a picture of what is actually
going on in your database with very little effort and gain really good insights into what you are doing. We saw a good example
of that when we looked at the what is going on in each city and were able to tell how many companies, employees, suppliers and
sales we had in each location. 

Map-reduce are great when you have a static aggregation scenario. When you know what you are aggregating on ahead of time.
Otherwise, you'll need (either directly or via the query optimizer) to generate map-reduce indexes for each of the premutation
that you want to query. Another alternative to that is the notion of dynamic aggregation, using facets to slice & dice the 
information dynamically. This tends to be less efficent than a map-reduce index, but it has less upfront costs.

It is common to do that in two stages, first define the rough shape of the aggregation in a map-reduce index, efficently 
doing the aggregation and then using facets and dynamic aggregation on that much smaller result set to narrow down things
more accurately. This gives you both fast aggregations and more flexiblity in the queries.

We finished the chapter by looking at aggregating documents by a _related_ document, using `LoadDocument` in a map-reduce
index. This works and can be quite an elegant solution for some scenarios, but it is also possible to get into trouble with
this approach because an update to a referenced document requires re-indexing of all the referencing documents. Instead, we
can utilize artifical documents and two map-reduce indexes to reduce the amount of work that is required when we need to
re-index.

The map-reduce engine inside RavenDB is very flexible and it has been used to great effect over the years, including many 
scenario that the development team was quite surprised about. Aggregation of results is only the most obvious of the options
that are available to you.

In the next chapter, we'll switch gears a bit and move to a more practical mindset, talking about how to utilize and work with
indexes in your applications.