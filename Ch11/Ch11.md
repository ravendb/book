
## Map-Reduce in RavenDB

Map-Reduce is an old term. It came from Lisp and was used as early as the 1960s. For a long time, it was primarily known only for
functional language afficandos, and it was rarely if ever seen outside their circles. In 2004, the Google paper
[MapReduce: Simplified Data Processing on Large Clusters](https://research.google.com/archive/mapreduce-osdi04.pdf) was released 
and map-reduce was instantly a big hit in the distributed programming circles. Everyone had to have a distributed map-reduce
implemention.

RavenDB is a distributed database, and it uses map-reduce. However, it doesn't do so in the context of distributed computing. 
Instead, RavenDB uses map-reduce for aggregation of data on each node independently. If you're used to map-reduce jobs that
are running on large clusters and processing terrabytes of data, that might look very strange. 
But RavenDB isn't using Map-Reduce to break apart large computation across different machines, instead, it uses Map-Reduce to 
break apart computation across _time_.

It is usually easier to explain with an example, so let's jump into that.

### Executing simple aggregations

Listing 11.1 shows a simple aggregation query, giving us the total number of orders and items purchased by a particular company.

```{caption="A simple aggregation query in RavenDB" .sql}
from Orders as o
group by o.Company
where o.Company  = 'companies/1-A'
select count() as NumberOfOrders, 
	   sum(o.Lines[].Quantity) as ItemsPurchased, 
	   o.Company
```

The query in Listing 11.1 should be very familiar to anyone who has used SQL before. Now, let us analyze what RavenDB must do in
order to answer this kind of query:

1. Find all the `Orders` documents where the `Company` field is set to `'companies/1-A'`.
2. Iterate over all of those, count their number and sum the number of line items in each.
3. Return the results to the client.

This seems quite straightforward, but it has a couple of issues. In particular, the first two steps. The sample data we are using
has just over a thousand documents in the database. There isn't much that we can do to make any query expensive over that dataset.
However, with real world datasets, we will typically deal with collections that contain hundreds of thousands to many milliions 
of documents. 

Consider the first step, finding all the `Orders` documents with the `Company` field set to a value. If I have a few millions 
documents to scan, that alone can be quite expensive, both in I/O (to read the data from disk) and computation (to check equality
so often). This protion is quite obviously something that we can optimize with an index. It is the next portion that is much 
harder to work with.

If a company has a _lot_ of orders, then the process of iterating over each of these orders can be extraordinarily expensive. 
Consider the case where you want to show the number of orders and the number of items purchased on the company page. This is a 
small amount of information which can tell how important this particular customer is. 

However, if we would need to repeat the second step each time that we run the query, that can get expensive very quickly. What is 
worse is that the more important the company, the more orders and items this particular customer purchased from us, the slower 
things will become. This method won't work, it is punishing success, and that isn't something we want.

Typically you will not run such queries directly, because of their cost and time to run.
Instead, you'll write a job that will run these queries at idle times and cache the results. Then you only have to deal with cache invalidation, making sure that this job is never run under load, explaining to users that the results are delayed and...

It's complex. All we wanted was to put a couple of numbers on a page, and suddenly you need to deploy a background job, monitor
its cost and execution and wait until the daily or weekly run to get updated numbers. Such high cost of aggregation will usually
cause such features to be dropped.

And even if you don't have enough data to require such measures, aggregation queries are still typically very costly for the 
database. Enough so that they are used sparingly in most systems.

RavenDB's aggregation doesn't work like this. 

#### Aggregation in RavenDB

Instead of gather all the data and then aggregating it all in place, RavenDB uses Map-Reduce to break apart the aggregation 
computation into discrete steps, the Map and the Reduce. Let's look at how RavenDB is _actually_ processing the query in 
Listing 11.1. Run the query in Listing 11.1 and then click on the index button in the results, as shown in Figure 11.1. 

![Getting to the index details on the query page](./Ch11/img01.png)

You can already see that an aggregation query in RavenDB is also using an index. In addition to the usual options, such as the 
index terms and performance statistics, there is also the Map-Reduce Visualizer, which we'll look in more detail later. For now
click on the `View Index` option, which should open the index details page, shown in Figure 11.2.

![A Map-Reduce index aggregation `Orders` by `Company`](./Ch11/img02.png)

Figure 11.2 shows the structure of the index. Operating over `Orders`, it is grouping by the `Company` and getting then 
aggregating over them. One thing to note here is that there is no mention anywhere of `companies/1-A`. Even though the query in
Listing 11.1 mentioned it, the index is not operating on that particular value, but on the generic concept of aggregating by
the `Company` field.

In other words, as usual, RavenDB has looked at the query and generalized the operation to answer any such question using any 
`Company`. But what about the aggregation?




#### Recursive map-reduce

### Disadvantages of Map-Reduce indexes in RavenDB